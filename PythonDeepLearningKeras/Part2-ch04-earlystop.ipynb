{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 조기종료 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[50000:]\n",
    "y_val = y_train[50000:]\n",
    "x_train = x_train[:50000]\n",
    "y_train = y_train[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(50000, 784).astype('float')/255.0\n",
    "x_val = x_val.reshape(10000, 784).astype('float')/255.0\n",
    "x_test = x_test.reshape(10000, 784).astype('float')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rand_indxs = np.random.choice(50000, 700)\n",
    "val_rand_indxs = np.random.choice(10000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[train_rand_indxs]\n",
    "y_train = y_train[train_rand_indxs]\n",
    "x_val = x_val[val_rand_indxs]\n",
    "y_val = y_val[val_rand_indxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "y_val = np_utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_dim=28*28, units=2, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/3000\n",
      "700/700 [==============================] - 0s 343us/step - loss: 2.3045 - acc: 0.0771 - val_loss: 2.2951 - val_acc: 0.1167\n",
      "Epoch 2/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 2.2732 - acc: 0.1000 - val_loss: 2.2551 - val_acc: 0.1367\n",
      "Epoch 3/3000\n",
      "700/700 [==============================] - 0s 140us/step - loss: 2.2381 - acc: 0.1300 - val_loss: 2.2240 - val_acc: 0.1567\n",
      "Epoch 4/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 2.2066 - acc: 0.1371 - val_loss: 2.1963 - val_acc: 0.1733\n",
      "Epoch 5/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 2.1773 - acc: 0.1614 - val_loss: 2.1709 - val_acc: 0.1633\n",
      "Epoch 6/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 2.1475 - acc: 0.1886 - val_loss: 2.1265 - val_acc: 0.2000\n",
      "Epoch 7/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 2.1094 - acc: 0.2214 - val_loss: 2.0847 - val_acc: 0.2233\n",
      "Epoch 8/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 2.0690 - acc: 0.2500 - val_loss: 2.0422 - val_acc: 0.2233\n",
      "Epoch 9/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 2.0286 - acc: 0.2643 - val_loss: 1.9978 - val_acc: 0.2233\n",
      "Epoch 10/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 1.9854 - acc: 0.2829 - val_loss: 1.9499 - val_acc: 0.2500\n",
      "Epoch 11/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.9402 - acc: 0.2857 - val_loss: 1.9019 - val_acc: 0.3233\n",
      "Epoch 12/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 1.8957 - acc: 0.3200 - val_loss: 1.8565 - val_acc: 0.3567\n",
      "Epoch 13/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 1.8543 - acc: 0.3371 - val_loss: 1.8162 - val_acc: 0.3767\n",
      "Epoch 14/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 1.8155 - acc: 0.3571 - val_loss: 1.7811 - val_acc: 0.3833\n",
      "Epoch 15/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.7826 - acc: 0.3700 - val_loss: 1.7471 - val_acc: 0.3767\n",
      "Epoch 16/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 1.7531 - acc: 0.3786 - val_loss: 1.7204 - val_acc: 0.3867\n",
      "Epoch 17/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 1.7252 - acc: 0.3686 - val_loss: 1.6939 - val_acc: 0.3967\n",
      "Epoch 18/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 1.6998 - acc: 0.3814 - val_loss: 1.6703 - val_acc: 0.4100\n",
      "Epoch 19/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 1.6774 - acc: 0.3957 - val_loss: 1.6446 - val_acc: 0.4133\n",
      "Epoch 20/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 1.6561 - acc: 0.4043 - val_loss: 1.6215 - val_acc: 0.4267\n",
      "Epoch 21/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 1.6349 - acc: 0.4157 - val_loss: 1.6026 - val_acc: 0.4367\n",
      "Epoch 22/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 1.6159 - acc: 0.4200 - val_loss: 1.5838 - val_acc: 0.4533\n",
      "Epoch 23/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 1.5987 - acc: 0.4286 - val_loss: 1.5653 - val_acc: 0.4567\n",
      "Epoch 24/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 1.5825 - acc: 0.4271 - val_loss: 1.5475 - val_acc: 0.4567\n",
      "Epoch 25/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 1.5660 - acc: 0.4486 - val_loss: 1.5315 - val_acc: 0.4633\n",
      "Epoch 26/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 1.5509 - acc: 0.4400 - val_loss: 1.5173 - val_acc: 0.4433\n",
      "Epoch 27/3000\n",
      "700/700 [==============================] - 0s 130us/step - loss: 1.5366 - acc: 0.4443 - val_loss: 1.5080 - val_acc: 0.4800\n",
      "Epoch 28/3000\n",
      "700/700 [==============================] - 0s 130us/step - loss: 1.5238 - acc: 0.4486 - val_loss: 1.4924 - val_acc: 0.4733\n",
      "Epoch 29/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 1.5095 - acc: 0.4600 - val_loss: 1.4786 - val_acc: 0.4567\n",
      "Epoch 30/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 1.4985 - acc: 0.4529 - val_loss: 1.4695 - val_acc: 0.4767\n",
      "Epoch 31/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 1.4864 - acc: 0.4571 - val_loss: 1.4585 - val_acc: 0.4800\n",
      "Epoch 32/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 1.4763 - acc: 0.4514 - val_loss: 1.4460 - val_acc: 0.4800\n",
      "Epoch 33/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.4651 - acc: 0.4600 - val_loss: 1.4394 - val_acc: 0.4900\n",
      "Epoch 34/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 1.4548 - acc: 0.4657 - val_loss: 1.4260 - val_acc: 0.4800\n",
      "Epoch 35/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 1.4456 - acc: 0.4743 - val_loss: 1.4234 - val_acc: 0.5000\n",
      "Epoch 36/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 1.4357 - acc: 0.4714 - val_loss: 1.4078 - val_acc: 0.4800\n",
      "Epoch 37/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 1.4284 - acc: 0.4686 - val_loss: 1.4033 - val_acc: 0.4833\n",
      "Epoch 38/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 1.4197 - acc: 0.4843 - val_loss: 1.3987 - val_acc: 0.5000\n",
      "Epoch 39/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 1.4112 - acc: 0.4657 - val_loss: 1.3917 - val_acc: 0.4900\n",
      "Epoch 40/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 1.4019 - acc: 0.4857 - val_loss: 1.3829 - val_acc: 0.4900\n",
      "Epoch 41/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 1.3958 - acc: 0.4800 - val_loss: 1.3731 - val_acc: 0.4800\n",
      "Epoch 42/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 1.3898 - acc: 0.4786 - val_loss: 1.3700 - val_acc: 0.4967\n",
      "Epoch 43/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 1.3808 - acc: 0.4914 - val_loss: 1.3628 - val_acc: 0.4933\n",
      "Epoch 44/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 1.3730 - acc: 0.4943 - val_loss: 1.3619 - val_acc: 0.4967\n",
      "Epoch 45/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 1.3670 - acc: 0.4971 - val_loss: 1.3524 - val_acc: 0.5367\n",
      "Epoch 46/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 1.3604 - acc: 0.5100 - val_loss: 1.3499 - val_acc: 0.5433\n",
      "Epoch 47/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 1.3534 - acc: 0.5157 - val_loss: 1.3403 - val_acc: 0.5167\n",
      "Epoch 48/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 1.3477 - acc: 0.5157 - val_loss: 1.3313 - val_acc: 0.5267\n",
      "Epoch 49/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 1.3426 - acc: 0.5171 - val_loss: 1.3306 - val_acc: 0.5300\n",
      "Epoch 50/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 1.3357 - acc: 0.5214 - val_loss: 1.3311 - val_acc: 0.5367\n",
      "Epoch 51/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 1.3309 - acc: 0.5200 - val_loss: 1.3232 - val_acc: 0.5367\n",
      "Epoch 52/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 1.3246 - acc: 0.5214 - val_loss: 1.3142 - val_acc: 0.5267\n",
      "Epoch 53/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 1.3180 - acc: 0.5200 - val_loss: 1.3148 - val_acc: 0.5300\n",
      "Epoch 54/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 1.3129 - acc: 0.5257 - val_loss: 1.3167 - val_acc: 0.5400\n",
      "Epoch 55/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 1.3093 - acc: 0.5329 - val_loss: 1.3086 - val_acc: 0.5467\n",
      "Epoch 56/3000\n",
      "700/700 [==============================] - 0s 142us/step - loss: 1.3043 - acc: 0.5271 - val_loss: 1.3066 - val_acc: 0.5367\n",
      "Epoch 57/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 1.2988 - acc: 0.5371 - val_loss: 1.2949 - val_acc: 0.5367\n",
      "Epoch 58/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 1.2922 - acc: 0.5286 - val_loss: 1.3024 - val_acc: 0.5533\n",
      "Epoch 59/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 1.2901 - acc: 0.5329 - val_loss: 1.2925 - val_acc: 0.5400\n",
      "Epoch 60/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 133us/step - loss: 1.2851 - acc: 0.5371 - val_loss: 1.2890 - val_acc: 0.5300\n",
      "Epoch 61/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 1.2785 - acc: 0.5429 - val_loss: 1.2854 - val_acc: 0.5233\n",
      "Epoch 62/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 1.2755 - acc: 0.5414 - val_loss: 1.2842 - val_acc: 0.5433\n",
      "Epoch 63/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.2707 - acc: 0.5314 - val_loss: 1.2796 - val_acc: 0.5400\n",
      "Epoch 64/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.2665 - acc: 0.5443 - val_loss: 1.2725 - val_acc: 0.5367\n",
      "Epoch 65/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.2624 - acc: 0.5443 - val_loss: 1.2749 - val_acc: 0.5400\n",
      "Epoch 66/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.2570 - acc: 0.5557 - val_loss: 1.2688 - val_acc: 0.5400\n",
      "Epoch 67/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.2541 - acc: 0.5557 - val_loss: 1.2674 - val_acc: 0.5500\n",
      "Epoch 68/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 1.2506 - acc: 0.5514 - val_loss: 1.2657 - val_acc: 0.5333\n",
      "Epoch 69/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.2447 - acc: 0.5471 - val_loss: 1.2560 - val_acc: 0.5300\n",
      "Epoch 70/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.2393 - acc: 0.5457 - val_loss: 1.2638 - val_acc: 0.5533\n",
      "Epoch 71/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.2374 - acc: 0.5543 - val_loss: 1.2589 - val_acc: 0.5333\n",
      "Epoch 72/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.2327 - acc: 0.5586 - val_loss: 1.2609 - val_acc: 0.5467\n",
      "Epoch 73/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.2297 - acc: 0.5514 - val_loss: 1.2561 - val_acc: 0.5533\n",
      "Epoch 74/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.2262 - acc: 0.5586 - val_loss: 1.2512 - val_acc: 0.5433\n",
      "Epoch 75/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 1.2219 - acc: 0.5671 - val_loss: 1.2455 - val_acc: 0.5433\n",
      "Epoch 76/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.2187 - acc: 0.5543 - val_loss: 1.2512 - val_acc: 0.5500\n",
      "Epoch 77/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.2162 - acc: 0.5686 - val_loss: 1.2438 - val_acc: 0.5467\n",
      "Epoch 78/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.2114 - acc: 0.5671 - val_loss: 1.2394 - val_acc: 0.5333\n",
      "Epoch 79/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 1.2068 - acc: 0.5700 - val_loss: 1.2438 - val_acc: 0.5467\n",
      "Epoch 80/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.2050 - acc: 0.5657 - val_loss: 1.2367 - val_acc: 0.5300\n",
      "Epoch 81/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.2013 - acc: 0.5686 - val_loss: 1.2381 - val_acc: 0.5467\n",
      "Epoch 82/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.1975 - acc: 0.5714 - val_loss: 1.2402 - val_acc: 0.5467\n",
      "Epoch 83/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.1944 - acc: 0.5757 - val_loss: 1.2326 - val_acc: 0.5500\n",
      "Epoch 84/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.1928 - acc: 0.5700 - val_loss: 1.2298 - val_acc: 0.5467\n",
      "Epoch 85/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.1896 - acc: 0.5700 - val_loss: 1.2291 - val_acc: 0.5533\n",
      "Epoch 86/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.1850 - acc: 0.5700 - val_loss: 1.2345 - val_acc: 0.5600\n",
      "Epoch 87/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.1822 - acc: 0.5814 - val_loss: 1.2387 - val_acc: 0.5633\n",
      "Epoch 88/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.1775 - acc: 0.5800 - val_loss: 1.2288 - val_acc: 0.5600\n",
      "Epoch 89/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.1769 - acc: 0.5800 - val_loss: 1.2256 - val_acc: 0.5600\n",
      "Epoch 90/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.1731 - acc: 0.5829 - val_loss: 1.2272 - val_acc: 0.5667\n",
      "Epoch 91/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.1714 - acc: 0.5829 - val_loss: 1.2220 - val_acc: 0.5533\n",
      "Epoch 92/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 1.1683 - acc: 0.5829 - val_loss: 1.2204 - val_acc: 0.5600\n",
      "Epoch 93/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 1.1647 - acc: 0.5914 - val_loss: 1.2207 - val_acc: 0.5533\n",
      "Epoch 94/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 1.1616 - acc: 0.5829 - val_loss: 1.2215 - val_acc: 0.5667\n",
      "Epoch 95/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.1597 - acc: 0.5814 - val_loss: 1.2187 - val_acc: 0.5700\n",
      "Epoch 96/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.1566 - acc: 0.5886 - val_loss: 1.2170 - val_acc: 0.5533\n",
      "Epoch 97/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.1532 - acc: 0.5914 - val_loss: 1.2278 - val_acc: 0.5633\n",
      "Epoch 98/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.1512 - acc: 0.5943 - val_loss: 1.2213 - val_acc: 0.5633\n",
      "Epoch 99/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.1489 - acc: 0.5957 - val_loss: 1.2138 - val_acc: 0.5600\n",
      "Epoch 100/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.1457 - acc: 0.5886 - val_loss: 1.2187 - val_acc: 0.5567\n",
      "Epoch 101/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.1432 - acc: 0.5900 - val_loss: 1.2151 - val_acc: 0.5533\n",
      "Epoch 102/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.1412 - acc: 0.5829 - val_loss: 1.2128 - val_acc: 0.5600\n",
      "Epoch 103/3000\n",
      "700/700 [==============================] - 0s 110us/step - loss: 1.1382 - acc: 0.5900 - val_loss: 1.2170 - val_acc: 0.5700\n",
      "Epoch 104/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.1355 - acc: 0.5871 - val_loss: 1.2109 - val_acc: 0.5700\n",
      "Epoch 105/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.1319 - acc: 0.5971 - val_loss: 1.2188 - val_acc: 0.5733\n",
      "Epoch 106/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.1320 - acc: 0.5929 - val_loss: 1.2103 - val_acc: 0.5467\n",
      "Epoch 107/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.1281 - acc: 0.5986 - val_loss: 1.2103 - val_acc: 0.5600\n",
      "Epoch 108/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.1260 - acc: 0.6014 - val_loss: 1.2130 - val_acc: 0.5533\n",
      "Epoch 109/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.1241 - acc: 0.5957 - val_loss: 1.2088 - val_acc: 0.5600\n",
      "Epoch 110/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.1225 - acc: 0.6029 - val_loss: 1.2078 - val_acc: 0.5733\n",
      "Epoch 111/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.1187 - acc: 0.5957 - val_loss: 1.2064 - val_acc: 0.5567\n",
      "Epoch 112/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.1161 - acc: 0.6086 - val_loss: 1.2076 - val_acc: 0.5733\n",
      "Epoch 113/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 1.1138 - acc: 0.6029 - val_loss: 1.2051 - val_acc: 0.5633\n",
      "Epoch 114/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.1103 - acc: 0.6100 - val_loss: 1.2004 - val_acc: 0.5600\n",
      "Epoch 115/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.1099 - acc: 0.6014 - val_loss: 1.2085 - val_acc: 0.5733\n",
      "Epoch 116/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 1.1076 - acc: 0.6143 - val_loss: 1.2000 - val_acc: 0.5767\n",
      "Epoch 117/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.1052 - acc: 0.5971 - val_loss: 1.2049 - val_acc: 0.5600\n",
      "Epoch 118/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 1.1022 - acc: 0.6071 - val_loss: 1.2069 - val_acc: 0.5767\n",
      "Epoch 119/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.1014 - acc: 0.6057 - val_loss: 1.2040 - val_acc: 0.5633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.1000 - acc: 0.6057 - val_loss: 1.2090 - val_acc: 0.5733\n",
      "Epoch 121/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.0981 - acc: 0.6114 - val_loss: 1.2093 - val_acc: 0.5733\n",
      "Epoch 122/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 1.0964 - acc: 0.6086 - val_loss: 1.2051 - val_acc: 0.5667\n",
      "Epoch 123/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.0926 - acc: 0.6157 - val_loss: 1.2005 - val_acc: 0.5733\n",
      "Epoch 124/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.0908 - acc: 0.6114 - val_loss: 1.2008 - val_acc: 0.5767\n",
      "Epoch 125/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 1.0882 - acc: 0.6200 - val_loss: 1.2046 - val_acc: 0.5733\n",
      "Epoch 126/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 1.0876 - acc: 0.6129 - val_loss: 1.2027 - val_acc: 0.5733\n",
      "Epoch 127/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.0853 - acc: 0.6286 - val_loss: 1.1999 - val_acc: 0.5800\n",
      "Epoch 128/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.0831 - acc: 0.6143 - val_loss: 1.2075 - val_acc: 0.5667\n",
      "Epoch 129/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 1.0808 - acc: 0.6186 - val_loss: 1.2017 - val_acc: 0.5767\n",
      "Epoch 130/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.0790 - acc: 0.6214 - val_loss: 1.2072 - val_acc: 0.5667\n",
      "Epoch 131/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.0765 - acc: 0.6129 - val_loss: 1.2000 - val_acc: 0.5700\n",
      "Epoch 132/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.0757 - acc: 0.6157 - val_loss: 1.2014 - val_acc: 0.5800\n",
      "Epoch 133/3000\n",
      "700/700 [==============================] - 0s 110us/step - loss: 1.0754 - acc: 0.6057 - val_loss: 1.1983 - val_acc: 0.5700\n",
      "Epoch 134/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.0715 - acc: 0.6086 - val_loss: 1.2053 - val_acc: 0.5667\n",
      "Epoch 135/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.0696 - acc: 0.6171 - val_loss: 1.2026 - val_acc: 0.5633\n",
      "Epoch 136/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.0686 - acc: 0.6214 - val_loss: 1.1965 - val_acc: 0.5667\n",
      "Epoch 137/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.0665 - acc: 0.6114 - val_loss: 1.2026 - val_acc: 0.5667\n",
      "Epoch 138/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.0654 - acc: 0.6171 - val_loss: 1.1974 - val_acc: 0.5733\n",
      "Epoch 139/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.0631 - acc: 0.6229 - val_loss: 1.1976 - val_acc: 0.5767\n",
      "Epoch 140/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.0624 - acc: 0.6171 - val_loss: 1.1981 - val_acc: 0.5800\n",
      "Epoch 141/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.0606 - acc: 0.6143 - val_loss: 1.1980 - val_acc: 0.5733\n",
      "Epoch 142/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.0577 - acc: 0.6243 - val_loss: 1.1957 - val_acc: 0.5700\n",
      "Epoch 143/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.0583 - acc: 0.6329 - val_loss: 1.1957 - val_acc: 0.5767\n",
      "Epoch 144/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.0565 - acc: 0.6214 - val_loss: 1.1955 - val_acc: 0.5733\n",
      "Epoch 145/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.0525 - acc: 0.6314 - val_loss: 1.1970 - val_acc: 0.5700\n",
      "Epoch 146/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.0524 - acc: 0.6257 - val_loss: 1.1997 - val_acc: 0.5767\n",
      "Epoch 147/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.0498 - acc: 0.6357 - val_loss: 1.1958 - val_acc: 0.5733\n",
      "Epoch 148/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.0486 - acc: 0.6271 - val_loss: 1.1935 - val_acc: 0.5700\n",
      "Epoch 149/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.0467 - acc: 0.6357 - val_loss: 1.1986 - val_acc: 0.5633\n",
      "Epoch 150/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 1.0455 - acc: 0.6343 - val_loss: 1.1994 - val_acc: 0.5700\n",
      "Epoch 151/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 1.0434 - acc: 0.6343 - val_loss: 1.1963 - val_acc: 0.5767\n",
      "Epoch 152/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.0412 - acc: 0.6400 - val_loss: 1.1987 - val_acc: 0.5700\n",
      "Epoch 153/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.0410 - acc: 0.6443 - val_loss: 1.1982 - val_acc: 0.5733\n",
      "Epoch 154/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 1.0386 - acc: 0.6357 - val_loss: 1.1960 - val_acc: 0.5833\n",
      "Epoch 155/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.0356 - acc: 0.6357 - val_loss: 1.1947 - val_acc: 0.5800\n",
      "Epoch 156/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 1.0364 - acc: 0.6300 - val_loss: 1.1968 - val_acc: 0.5800\n",
      "Epoch 157/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.0336 - acc: 0.6371 - val_loss: 1.2020 - val_acc: 0.5667\n",
      "Epoch 158/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 1.0329 - acc: 0.6400 - val_loss: 1.1971 - val_acc: 0.5633\n",
      "Epoch 159/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.0314 - acc: 0.6429 - val_loss: 1.2013 - val_acc: 0.5567\n",
      "Epoch 160/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.0308 - acc: 0.6457 - val_loss: 1.2024 - val_acc: 0.5700\n",
      "Epoch 161/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.0270 - acc: 0.6400 - val_loss: 1.1941 - val_acc: 0.5767\n",
      "Epoch 162/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.0263 - acc: 0.6500 - val_loss: 1.1969 - val_acc: 0.5733\n",
      "Epoch 163/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.0271 - acc: 0.6329 - val_loss: 1.2009 - val_acc: 0.5700\n",
      "Epoch 164/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.0253 - acc: 0.6400 - val_loss: 1.2019 - val_acc: 0.5633\n",
      "Epoch 165/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.0202 - acc: 0.6471 - val_loss: 1.1987 - val_acc: 0.5700\n",
      "Epoch 166/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.0229 - acc: 0.6457 - val_loss: 1.1998 - val_acc: 0.5700\n",
      "Epoch 167/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.0194 - acc: 0.6486 - val_loss: 1.2001 - val_acc: 0.5800\n",
      "Epoch 168/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 1.0204 - acc: 0.6400 - val_loss: 1.2013 - val_acc: 0.5800\n",
      "Epoch 169/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.0188 - acc: 0.6457 - val_loss: 1.1971 - val_acc: 0.5800\n",
      "Epoch 170/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 1.0161 - acc: 0.6486 - val_loss: 1.2008 - val_acc: 0.5733\n",
      "Epoch 171/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.0163 - acc: 0.6471 - val_loss: 1.1978 - val_acc: 0.5767\n",
      "Epoch 172/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.0134 - acc: 0.6471 - val_loss: 1.2000 - val_acc: 0.5800\n",
      "Epoch 173/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.0121 - acc: 0.6500 - val_loss: 1.2033 - val_acc: 0.5700\n",
      "Epoch 174/3000\n",
      "700/700 [==============================] - 0s 110us/step - loss: 1.0114 - acc: 0.6457 - val_loss: 1.1984 - val_acc: 0.5667\n",
      "Epoch 175/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.0108 - acc: 0.6529 - val_loss: 1.1996 - val_acc: 0.5700\n",
      "Epoch 176/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.0088 - acc: 0.6486 - val_loss: 1.1983 - val_acc: 0.5700\n",
      "Epoch 177/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.0051 - acc: 0.6543 - val_loss: 1.2073 - val_acc: 0.5667\n",
      "Epoch 178/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.0063 - acc: 0.6543 - val_loss: 1.1964 - val_acc: 0.5800\n",
      "Epoch 179/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 117us/step - loss: 1.0009 - acc: 0.6586 - val_loss: 1.1914 - val_acc: 0.5633\n",
      "Epoch 180/3000\n",
      "700/700 [==============================] - 0s 109us/step - loss: 1.0047 - acc: 0.6514 - val_loss: 1.1948 - val_acc: 0.5700\n",
      "Epoch 181/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.0016 - acc: 0.6586 - val_loss: 1.1978 - val_acc: 0.5767\n",
      "Epoch 182/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.9989 - acc: 0.6586 - val_loss: 1.2007 - val_acc: 0.5800\n",
      "Epoch 183/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.9967 - acc: 0.6557 - val_loss: 1.1942 - val_acc: 0.5767\n",
      "Epoch 184/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.9942 - acc: 0.6557 - val_loss: 1.2081 - val_acc: 0.5733\n",
      "Epoch 185/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.9964 - acc: 0.6557 - val_loss: 1.1993 - val_acc: 0.5667\n",
      "Epoch 186/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.9920 - acc: 0.6486 - val_loss: 1.1923 - val_acc: 0.5700\n",
      "Epoch 187/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.9917 - acc: 0.6600 - val_loss: 1.1958 - val_acc: 0.5800\n",
      "Epoch 188/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.9911 - acc: 0.6671 - val_loss: 1.1975 - val_acc: 0.5767\n",
      "Epoch 189/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.9886 - acc: 0.6643 - val_loss: 1.1981 - val_acc: 0.5767\n",
      "Epoch 190/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.9900 - acc: 0.6586 - val_loss: 1.1974 - val_acc: 0.5733\n",
      "Epoch 191/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.9869 - acc: 0.6700 - val_loss: 1.1994 - val_acc: 0.5733\n",
      "Epoch 192/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 0.9856 - acc: 0.6600 - val_loss: 1.1969 - val_acc: 0.5900\n",
      "Epoch 193/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.9847 - acc: 0.6629 - val_loss: 1.1984 - val_acc: 0.5767\n",
      "Epoch 194/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.9827 - acc: 0.6586 - val_loss: 1.1962 - val_acc: 0.5833\n",
      "Epoch 195/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.9813 - acc: 0.6714 - val_loss: 1.1972 - val_acc: 0.5733\n",
      "Epoch 196/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.9790 - acc: 0.6657 - val_loss: 1.2018 - val_acc: 0.5733\n",
      "Epoch 197/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.9778 - acc: 0.6614 - val_loss: 1.2060 - val_acc: 0.5633\n",
      "Epoch 198/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.9783 - acc: 0.6657 - val_loss: 1.2044 - val_acc: 0.5867\n",
      "Epoch 199/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.9752 - acc: 0.6686 - val_loss: 1.1967 - val_acc: 0.5800\n",
      "Epoch 200/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.9753 - acc: 0.6700 - val_loss: 1.1982 - val_acc: 0.5867\n",
      "Epoch 201/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.9729 - acc: 0.6686 - val_loss: 1.1965 - val_acc: 0.5733\n",
      "Epoch 202/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.9731 - acc: 0.6743 - val_loss: 1.1971 - val_acc: 0.5733\n",
      "Epoch 203/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.9693 - acc: 0.6757 - val_loss: 1.2067 - val_acc: 0.5833\n",
      "Epoch 204/3000\n",
      "700/700 [==============================] - 0s 187us/step - loss: 0.9686 - acc: 0.6629 - val_loss: 1.2009 - val_acc: 0.5800\n",
      "Epoch 205/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.9675 - acc: 0.6686 - val_loss: 1.2008 - val_acc: 0.5967\n",
      "Epoch 206/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.9667 - acc: 0.6886 - val_loss: 1.1961 - val_acc: 0.5800\n",
      "Epoch 207/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.9645 - acc: 0.6757 - val_loss: 1.2008 - val_acc: 0.5800\n",
      "Epoch 208/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 0.9653 - acc: 0.6771 - val_loss: 1.2035 - val_acc: 0.5867\n",
      "Epoch 209/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 0.9620 - acc: 0.6757 - val_loss: 1.2007 - val_acc: 0.5767\n",
      "Epoch 210/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.9609 - acc: 0.6743 - val_loss: 1.2018 - val_acc: 0.5667\n",
      "Epoch 211/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.9598 - acc: 0.6843 - val_loss: 1.1971 - val_acc: 0.5867\n",
      "Epoch 212/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.9593 - acc: 0.6786 - val_loss: 1.2063 - val_acc: 0.5633\n",
      "Epoch 213/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.9583 - acc: 0.6771 - val_loss: 1.2034 - val_acc: 0.5833\n",
      "Epoch 214/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.9574 - acc: 0.6786 - val_loss: 1.2006 - val_acc: 0.5800\n",
      "Epoch 215/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 0.9555 - acc: 0.6814 - val_loss: 1.1998 - val_acc: 0.5867\n",
      "Epoch 216/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 0.9560 - acc: 0.6857 - val_loss: 1.2033 - val_acc: 0.5833\n",
      "Epoch 217/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.9535 - acc: 0.6771 - val_loss: 1.2006 - val_acc: 0.5800\n",
      "Epoch 218/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.9502 - acc: 0.6829 - val_loss: 1.2065 - val_acc: 0.5700\n",
      "Epoch 219/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.9499 - acc: 0.6857 - val_loss: 1.2021 - val_acc: 0.5867\n",
      "Epoch 220/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.9492 - acc: 0.6829 - val_loss: 1.2059 - val_acc: 0.5800\n",
      "Epoch 221/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.9484 - acc: 0.6857 - val_loss: 1.2070 - val_acc: 0.5800\n",
      "Epoch 222/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 0.9482 - acc: 0.6886 - val_loss: 1.2067 - val_acc: 0.5767\n",
      "Epoch 223/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.9449 - acc: 0.6843 - val_loss: 1.2008 - val_acc: 0.5800\n",
      "Epoch 224/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.9439 - acc: 0.6757 - val_loss: 1.2023 - val_acc: 0.5733\n",
      "Epoch 225/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.9444 - acc: 0.6857 - val_loss: 1.2033 - val_acc: 0.5833\n",
      "Epoch 226/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.9433 - acc: 0.6800 - val_loss: 1.2029 - val_acc: 0.5800\n",
      "Epoch 227/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.9405 - acc: 0.6871 - val_loss: 1.2048 - val_acc: 0.5833\n",
      "Epoch 228/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.9399 - acc: 0.6829 - val_loss: 1.2048 - val_acc: 0.5767\n",
      "Epoch 229/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.9407 - acc: 0.6929 - val_loss: 1.2083 - val_acc: 0.5867\n",
      "Epoch 230/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.9379 - acc: 0.6871 - val_loss: 1.2025 - val_acc: 0.5833\n",
      "Epoch 231/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.9381 - acc: 0.6943 - val_loss: 1.2002 - val_acc: 0.5800\n",
      "Epoch 232/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.9364 - acc: 0.6900 - val_loss: 1.2042 - val_acc: 0.5833\n",
      "Epoch 233/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.9344 - acc: 0.6886 - val_loss: 1.2060 - val_acc: 0.5933\n",
      "Epoch 234/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.9332 - acc: 0.6914 - val_loss: 1.2053 - val_acc: 0.5867\n",
      "Epoch 235/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.9341 - acc: 0.6900 - val_loss: 1.2082 - val_acc: 0.5767\n",
      "Epoch 236/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.9316 - acc: 0.6886 - val_loss: 1.2089 - val_acc: 0.5767\n",
      "Epoch 237/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.9312 - acc: 0.6871 - val_loss: 1.2086 - val_acc: 0.5800\n",
      "Epoch 238/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 125us/step - loss: 0.9268 - acc: 0.6986 - val_loss: 1.2054 - val_acc: 0.5767\n",
      "Epoch 239/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.9296 - acc: 0.6886 - val_loss: 1.2043 - val_acc: 0.5767\n",
      "Epoch 240/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.9273 - acc: 0.6957 - val_loss: 1.2046 - val_acc: 0.5767\n",
      "Epoch 241/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.9246 - acc: 0.6957 - val_loss: 1.2054 - val_acc: 0.5733\n",
      "Epoch 242/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.9231 - acc: 0.7000 - val_loss: 1.2091 - val_acc: 0.5767\n",
      "Epoch 243/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.9232 - acc: 0.6957 - val_loss: 1.2085 - val_acc: 0.5733\n",
      "Epoch 244/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.9236 - acc: 0.6886 - val_loss: 1.2057 - val_acc: 0.5833\n",
      "Epoch 245/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.9198 - acc: 0.6943 - val_loss: 1.2063 - val_acc: 0.5967\n",
      "Epoch 246/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.9202 - acc: 0.7014 - val_loss: 1.2036 - val_acc: 0.5833\n",
      "Epoch 247/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.9188 - acc: 0.6957 - val_loss: 1.2119 - val_acc: 0.5667\n",
      "Epoch 248/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.9186 - acc: 0.6886 - val_loss: 1.2087 - val_acc: 0.5767\n",
      "Epoch 249/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.9165 - acc: 0.6986 - val_loss: 1.2132 - val_acc: 0.5667\n",
      "Epoch 250/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.9155 - acc: 0.6943 - val_loss: 1.2039 - val_acc: 0.5800\n",
      "Epoch 251/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.9131 - acc: 0.7029 - val_loss: 1.2117 - val_acc: 0.5833\n",
      "Epoch 252/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.9129 - acc: 0.7057 - val_loss: 1.2100 - val_acc: 0.5767\n",
      "Epoch 253/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 0.9118 - acc: 0.7029 - val_loss: 1.2106 - val_acc: 0.5733\n",
      "Epoch 254/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 0.9102 - acc: 0.7143 - val_loss: 1.2102 - val_acc: 0.5833\n",
      "Epoch 255/3000\n",
      "700/700 [==============================] - 0s 110us/step - loss: 0.9103 - acc: 0.7014 - val_loss: 1.2106 - val_acc: 0.5833\n",
      "Epoch 256/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.9101 - acc: 0.7086 - val_loss: 1.2086 - val_acc: 0.5800\n",
      "Epoch 257/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.9083 - acc: 0.7014 - val_loss: 1.2168 - val_acc: 0.5800\n",
      "Epoch 258/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.9076 - acc: 0.7029 - val_loss: 1.2133 - val_acc: 0.5900\n",
      "Epoch 259/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.9065 - acc: 0.7029 - val_loss: 1.2102 - val_acc: 0.5833\n",
      "Epoch 260/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.9054 - acc: 0.6957 - val_loss: 1.2119 - val_acc: 0.5900\n",
      "Epoch 261/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.9041 - acc: 0.7043 - val_loss: 1.2114 - val_acc: 0.5900\n",
      "Epoch 262/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.9045 - acc: 0.7014 - val_loss: 1.2114 - val_acc: 0.5867\n",
      "Epoch 263/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.9024 - acc: 0.7086 - val_loss: 1.2156 - val_acc: 0.5833\n",
      "Epoch 264/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.9025 - acc: 0.7057 - val_loss: 1.2127 - val_acc: 0.5700\n",
      "Epoch 265/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.9016 - acc: 0.6986 - val_loss: 1.2153 - val_acc: 0.5867\n",
      "Epoch 266/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.9000 - acc: 0.7143 - val_loss: 1.2154 - val_acc: 0.5800\n",
      "Epoch 267/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.8991 - acc: 0.7100 - val_loss: 1.2191 - val_acc: 0.5733\n",
      "Epoch 268/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.8967 - acc: 0.7029 - val_loss: 1.2182 - val_acc: 0.5833\n",
      "Epoch 269/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.8973 - acc: 0.7086 - val_loss: 1.2182 - val_acc: 0.5800\n",
      "Epoch 270/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.8977 - acc: 0.7057 - val_loss: 1.2163 - val_acc: 0.5800\n",
      "Epoch 271/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.8937 - acc: 0.7043 - val_loss: 1.2159 - val_acc: 0.5900\n",
      "Epoch 272/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.8959 - acc: 0.7157 - val_loss: 1.2194 - val_acc: 0.5733\n",
      "Epoch 273/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.8940 - acc: 0.7114 - val_loss: 1.2182 - val_acc: 0.5833\n",
      "Epoch 274/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.8914 - acc: 0.7143 - val_loss: 1.2194 - val_acc: 0.5833\n",
      "Epoch 275/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.8909 - acc: 0.7086 - val_loss: 1.2176 - val_acc: 0.5833\n",
      "Epoch 276/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.8913 - acc: 0.7157 - val_loss: 1.2166 - val_acc: 0.5900\n",
      "Epoch 277/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.8897 - acc: 0.7114 - val_loss: 1.2160 - val_acc: 0.5767\n",
      "Epoch 278/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.8878 - acc: 0.7157 - val_loss: 1.2172 - val_acc: 0.5900\n",
      "Epoch 279/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.8869 - acc: 0.7271 - val_loss: 1.2142 - val_acc: 0.5900\n",
      "Epoch 280/3000\n",
      "700/700 [==============================] - 0s 106us/step - loss: 0.8854 - acc: 0.7200 - val_loss: 1.2213 - val_acc: 0.5733\n",
      "Epoch 281/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 0.8867 - acc: 0.7143 - val_loss: 1.2230 - val_acc: 0.5867\n",
      "Epoch 282/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.8843 - acc: 0.7143 - val_loss: 1.2231 - val_acc: 0.5900\n",
      "Epoch 283/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.8847 - acc: 0.7143 - val_loss: 1.2171 - val_acc: 0.5867\n",
      "Epoch 284/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.8831 - acc: 0.7157 - val_loss: 1.2201 - val_acc: 0.5933\n",
      "Epoch 285/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.8820 - acc: 0.7171 - val_loss: 1.2239 - val_acc: 0.5900\n",
      "Epoch 286/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.8819 - acc: 0.7157 - val_loss: 1.2248 - val_acc: 0.5800\n",
      "Epoch 287/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.8812 - acc: 0.7157 - val_loss: 1.2228 - val_acc: 0.5900\n",
      "Epoch 288/3000\n",
      "700/700 [==============================] - 0s 109us/step - loss: 0.8790 - acc: 0.7171 - val_loss: 1.2212 - val_acc: 0.5833\n",
      "Epoch 289/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.8781 - acc: 0.7143 - val_loss: 1.2212 - val_acc: 0.5900\n",
      "Epoch 290/3000\n",
      "700/700 [==============================] - 0s 107us/step - loss: 0.8787 - acc: 0.7257 - val_loss: 1.2216 - val_acc: 0.5867\n",
      "Epoch 291/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.8769 - acc: 0.7143 - val_loss: 1.2325 - val_acc: 0.5800\n",
      "Epoch 292/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.8756 - acc: 0.7186 - val_loss: 1.2250 - val_acc: 0.5933\n",
      "Epoch 293/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.8748 - acc: 0.7257 - val_loss: 1.2216 - val_acc: 0.5833\n",
      "Epoch 294/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.8750 - acc: 0.7214 - val_loss: 1.2221 - val_acc: 0.5967\n",
      "Epoch 295/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.8741 - acc: 0.7229 - val_loss: 1.2252 - val_acc: 0.5933\n",
      "Epoch 296/3000\n",
      "700/700 [==============================] - 0s 110us/step - loss: 0.8736 - acc: 0.7171 - val_loss: 1.2242 - val_acc: 0.5867\n",
      "Epoch 297/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 116us/step - loss: 0.8724 - acc: 0.7214 - val_loss: 1.2244 - val_acc: 0.5867\n",
      "Epoch 298/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.8707 - acc: 0.7271 - val_loss: 1.2327 - val_acc: 0.5833\n",
      "Epoch 299/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.8698 - acc: 0.7200 - val_loss: 1.2279 - val_acc: 0.5967\n",
      "Epoch 300/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.8697 - acc: 0.7257 - val_loss: 1.2243 - val_acc: 0.5967\n",
      "Epoch 301/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.8702 - acc: 0.7214 - val_loss: 1.2301 - val_acc: 0.5867\n",
      "Epoch 302/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.8680 - acc: 0.7243 - val_loss: 1.2272 - val_acc: 0.5867\n",
      "Epoch 303/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.8667 - acc: 0.7271 - val_loss: 1.2321 - val_acc: 0.5933\n",
      "Epoch 304/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.8667 - acc: 0.7286 - val_loss: 1.2351 - val_acc: 0.5833\n",
      "Epoch 305/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.8661 - acc: 0.7286 - val_loss: 1.2316 - val_acc: 0.5833\n",
      "Epoch 306/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.8638 - acc: 0.7314 - val_loss: 1.2307 - val_acc: 0.5933\n",
      "Epoch 307/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.8639 - acc: 0.7214 - val_loss: 1.2299 - val_acc: 0.5867\n",
      "Epoch 308/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.8627 - acc: 0.7243 - val_loss: 1.2332 - val_acc: 0.5933\n",
      "Epoch 309/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.8593 - acc: 0.7300 - val_loss: 1.2349 - val_acc: 0.5833\n",
      "Epoch 310/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.8634 - acc: 0.7257 - val_loss: 1.2305 - val_acc: 0.5967\n",
      "Epoch 311/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.8592 - acc: 0.7214 - val_loss: 1.2449 - val_acc: 0.5900\n",
      "Epoch 312/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 0.8608 - acc: 0.7243 - val_loss: 1.2348 - val_acc: 0.5867\n",
      "Epoch 313/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 0.8574 - acc: 0.7229 - val_loss: 1.2379 - val_acc: 0.5867\n",
      "Epoch 314/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.8588 - acc: 0.7243 - val_loss: 1.2422 - val_acc: 0.5833\n",
      "Epoch 315/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.8578 - acc: 0.7257 - val_loss: 1.2364 - val_acc: 0.5967\n",
      "Epoch 316/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.8566 - acc: 0.7271 - val_loss: 1.2326 - val_acc: 0.5967\n",
      "Epoch 317/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.8559 - acc: 0.7243 - val_loss: 1.2387 - val_acc: 0.5933\n",
      "Epoch 318/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.8546 - acc: 0.7286 - val_loss: 1.2382 - val_acc: 0.5900\n",
      "Epoch 319/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.8538 - acc: 0.7286 - val_loss: 1.2386 - val_acc: 0.5833\n",
      "Epoch 320/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 0.8522 - acc: 0.7343 - val_loss: 1.2341 - val_acc: 0.5867\n",
      "Epoch 321/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.8518 - acc: 0.7357 - val_loss: 1.2379 - val_acc: 0.5867\n",
      "Epoch 322/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.8521 - acc: 0.7271 - val_loss: 1.2399 - val_acc: 0.5933\n",
      "Epoch 323/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.8513 - acc: 0.7314 - val_loss: 1.2401 - val_acc: 0.5900\n",
      "Epoch 324/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.8498 - acc: 0.7300 - val_loss: 1.2466 - val_acc: 0.5800\n",
      "Epoch 325/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.8490 - acc: 0.7300 - val_loss: 1.2472 - val_acc: 0.5867\n",
      "Epoch 326/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.8493 - acc: 0.7300 - val_loss: 1.2397 - val_acc: 0.5967\n",
      "Epoch 327/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.8492 - acc: 0.7343 - val_loss: 1.2463 - val_acc: 0.5867\n",
      "Epoch 328/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.8469 - acc: 0.7357 - val_loss: 1.2436 - val_acc: 0.5833\n",
      "Epoch 329/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.8458 - acc: 0.7329 - val_loss: 1.2443 - val_acc: 0.5833\n",
      "Epoch 330/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.8470 - acc: 0.7243 - val_loss: 1.2448 - val_acc: 0.5867\n",
      "Epoch 331/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.8455 - acc: 0.7271 - val_loss: 1.2420 - val_acc: 0.5933\n",
      "Epoch 332/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.8436 - acc: 0.7371 - val_loss: 1.2454 - val_acc: 0.5933\n",
      "Epoch 333/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.8431 - acc: 0.7257 - val_loss: 1.2440 - val_acc: 0.5967\n",
      "Epoch 334/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.8433 - acc: 0.7300 - val_loss: 1.2426 - val_acc: 0.5933\n",
      "Epoch 335/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.8427 - acc: 0.7386 - val_loss: 1.2529 - val_acc: 0.5967\n",
      "Epoch 336/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.8415 - acc: 0.7314 - val_loss: 1.2566 - val_acc: 0.5867\n",
      "Epoch 337/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.8415 - acc: 0.7343 - val_loss: 1.2499 - val_acc: 0.5967\n",
      "Epoch 338/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.8397 - acc: 0.7371 - val_loss: 1.2488 - val_acc: 0.5900\n",
      "Epoch 339/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.8384 - acc: 0.7386 - val_loss: 1.2456 - val_acc: 0.5967\n",
      "Epoch 340/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.8373 - acc: 0.7343 - val_loss: 1.2483 - val_acc: 0.5900\n",
      "Epoch 341/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.8384 - acc: 0.7400 - val_loss: 1.2472 - val_acc: 0.5967\n",
      "Epoch 342/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.8357 - acc: 0.7414 - val_loss: 1.2528 - val_acc: 0.5933\n",
      "Epoch 343/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.8361 - acc: 0.7329 - val_loss: 1.2560 - val_acc: 0.5867\n",
      "Epoch 344/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.8341 - acc: 0.7443 - val_loss: 1.2549 - val_acc: 0.5967\n",
      "Epoch 345/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.8347 - acc: 0.7486 - val_loss: 1.2553 - val_acc: 0.5900\n",
      "Epoch 346/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.8331 - acc: 0.7486 - val_loss: 1.2488 - val_acc: 0.6000\n",
      "Epoch 347/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.8331 - acc: 0.7386 - val_loss: 1.2469 - val_acc: 0.6000\n",
      "Epoch 348/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.8298 - acc: 0.7471 - val_loss: 1.2489 - val_acc: 0.5967\n",
      "Epoch 349/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 0.8304 - acc: 0.7543 - val_loss: 1.2492 - val_acc: 0.6000\n",
      "Epoch 350/3000\n",
      "700/700 [==============================] - 0s 110us/step - loss: 0.8300 - acc: 0.7443 - val_loss: 1.2587 - val_acc: 0.5933\n",
      "Epoch 351/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.8286 - acc: 0.7486 - val_loss: 1.2520 - val_acc: 0.5900\n",
      "Epoch 352/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 0.8257 - acc: 0.7529 - val_loss: 1.2571 - val_acc: 0.5933\n",
      "Epoch 353/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 0.8252 - acc: 0.7486 - val_loss: 1.2681 - val_acc: 0.5933\n",
      "Epoch 354/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.8263 - acc: 0.7457 - val_loss: 1.2618 - val_acc: 0.5933\n",
      "Epoch 355/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.8247 - acc: 0.7514 - val_loss: 1.2483 - val_acc: 0.5900\n",
      "Epoch 356/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 116us/step - loss: 0.8266 - acc: 0.7514 - val_loss: 1.2542 - val_acc: 0.6033\n",
      "Epoch 357/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.8224 - acc: 0.7500 - val_loss: 1.2545 - val_acc: 0.6000\n",
      "Epoch 358/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.8226 - acc: 0.7543 - val_loss: 1.2685 - val_acc: 0.5967\n",
      "Epoch 359/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.8205 - acc: 0.7514 - val_loss: 1.2561 - val_acc: 0.5933\n",
      "Epoch 360/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.8202 - acc: 0.7500 - val_loss: 1.2593 - val_acc: 0.6000\n",
      "Epoch 361/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.8209 - acc: 0.7586 - val_loss: 1.2567 - val_acc: 0.5967\n",
      "Epoch 362/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.8186 - acc: 0.7586 - val_loss: 1.2682 - val_acc: 0.5933\n",
      "Epoch 363/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.8189 - acc: 0.7500 - val_loss: 1.2610 - val_acc: 0.5967\n",
      "Epoch 364/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.8182 - acc: 0.7471 - val_loss: 1.2590 - val_acc: 0.5967\n",
      "Epoch 365/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.8180 - acc: 0.7529 - val_loss: 1.2612 - val_acc: 0.6067\n",
      "Epoch 366/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.8165 - acc: 0.7571 - val_loss: 1.2630 - val_acc: 0.5967\n",
      "Epoch 367/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.8148 - acc: 0.7571 - val_loss: 1.2677 - val_acc: 0.5933\n",
      "Epoch 368/3000\n",
      "700/700 [==============================] - 0s 110us/step - loss: 0.8150 - acc: 0.7514 - val_loss: 1.2626 - val_acc: 0.5967\n",
      "Epoch 369/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 0.8142 - acc: 0.7557 - val_loss: 1.2625 - val_acc: 0.5967\n",
      "Epoch 370/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 0.8132 - acc: 0.7543 - val_loss: 1.2666 - val_acc: 0.5933\n",
      "Epoch 371/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.8112 - acc: 0.7514 - val_loss: 1.2646 - val_acc: 0.6000\n",
      "Epoch 372/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.8128 - acc: 0.7586 - val_loss: 1.2694 - val_acc: 0.6000\n",
      "Epoch 373/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.8118 - acc: 0.7571 - val_loss: 1.2732 - val_acc: 0.5933\n",
      "Epoch 374/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.8099 - acc: 0.7571 - val_loss: 1.2695 - val_acc: 0.5900\n",
      "Epoch 375/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.8111 - acc: 0.7514 - val_loss: 1.2717 - val_acc: 0.5933\n",
      "Epoch 376/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.8089 - acc: 0.7543 - val_loss: 1.2709 - val_acc: 0.6000\n",
      "Epoch 377/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.8095 - acc: 0.7543 - val_loss: 1.2679 - val_acc: 0.5967\n",
      "Epoch 378/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.8081 - acc: 0.7600 - val_loss: 1.2758 - val_acc: 0.5967\n",
      "Epoch 379/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.8071 - acc: 0.7557 - val_loss: 1.2707 - val_acc: 0.5900\n",
      "Epoch 380/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.8068 - acc: 0.7614 - val_loss: 1.2722 - val_acc: 0.5933\n",
      "Epoch 381/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.8051 - acc: 0.7600 - val_loss: 1.2730 - val_acc: 0.5967\n",
      "Epoch 382/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.8047 - acc: 0.7586 - val_loss: 1.2774 - val_acc: 0.6100\n",
      "Epoch 383/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.8041 - acc: 0.7571 - val_loss: 1.2713 - val_acc: 0.6000\n",
      "Epoch 384/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.8049 - acc: 0.7586 - val_loss: 1.2794 - val_acc: 0.6000\n",
      "Epoch 385/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.8015 - acc: 0.7571 - val_loss: 1.2754 - val_acc: 0.6033\n",
      "Epoch 386/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.8016 - acc: 0.7600 - val_loss: 1.2766 - val_acc: 0.5967\n",
      "Epoch 387/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 0.7968 - acc: 0.757 - 0s 128us/step - loss: 0.8001 - acc: 0.7600 - val_loss: 1.2766 - val_acc: 0.5900\n",
      "Epoch 388/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.8010 - acc: 0.7629 - val_loss: 1.2759 - val_acc: 0.5867\n",
      "Epoch 389/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.8008 - acc: 0.7600 - val_loss: 1.2748 - val_acc: 0.5967\n",
      "Epoch 390/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7985 - acc: 0.7614 - val_loss: 1.2856 - val_acc: 0.6000\n",
      "Epoch 391/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.7999 - acc: 0.7614 - val_loss: 1.2816 - val_acc: 0.6000\n",
      "Epoch 392/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.7981 - acc: 0.7614 - val_loss: 1.2793 - val_acc: 0.6000\n",
      "Epoch 393/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.7979 - acc: 0.7600 - val_loss: 1.2804 - val_acc: 0.5900\n",
      "Epoch 394/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.7973 - acc: 0.7629 - val_loss: 1.2836 - val_acc: 0.5967\n",
      "Epoch 395/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.7961 - acc: 0.7643 - val_loss: 1.2826 - val_acc: 0.5867\n",
      "Epoch 396/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 0.7958 - acc: 0.7643 - val_loss: 1.2776 - val_acc: 0.6000\n",
      "Epoch 397/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7957 - acc: 0.7614 - val_loss: 1.2785 - val_acc: 0.6033\n",
      "Epoch 398/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7956 - acc: 0.7571 - val_loss: 1.2826 - val_acc: 0.5967\n",
      "Epoch 399/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7934 - acc: 0.7614 - val_loss: 1.2821 - val_acc: 0.6000\n",
      "Epoch 400/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.7936 - acc: 0.7643 - val_loss: 1.2843 - val_acc: 0.6033\n",
      "Epoch 401/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7931 - acc: 0.7614 - val_loss: 1.2865 - val_acc: 0.5967\n",
      "Epoch 402/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.7931 - acc: 0.7557 - val_loss: 1.2944 - val_acc: 0.6100\n",
      "Epoch 403/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.7910 - acc: 0.7571 - val_loss: 1.2911 - val_acc: 0.5933\n",
      "Epoch 404/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.7908 - acc: 0.7643 - val_loss: 1.2918 - val_acc: 0.6000\n",
      "Epoch 405/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7904 - acc: 0.7671 - val_loss: 1.2913 - val_acc: 0.5967\n",
      "Epoch 406/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7901 - acc: 0.7643 - val_loss: 1.2915 - val_acc: 0.6100\n",
      "Epoch 407/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7890 - acc: 0.7586 - val_loss: 1.2950 - val_acc: 0.5900\n",
      "Epoch 408/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7874 - acc: 0.7629 - val_loss: 1.2892 - val_acc: 0.5967\n",
      "Epoch 409/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7870 - acc: 0.7643 - val_loss: 1.2974 - val_acc: 0.6033\n",
      "Epoch 410/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7880 - acc: 0.7629 - val_loss: 1.2990 - val_acc: 0.6000\n",
      "Epoch 411/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7880 - acc: 0.7671 - val_loss: 1.2992 - val_acc: 0.6033\n",
      "Epoch 412/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.7865 - acc: 0.7600 - val_loss: 1.2977 - val_acc: 0.5900\n",
      "Epoch 413/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.7859 - acc: 0.7557 - val_loss: 1.3042 - val_acc: 0.6033\n",
      "Epoch 414/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.7870 - acc: 0.7586 - val_loss: 1.2975 - val_acc: 0.5967\n",
      "Epoch 415/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 115us/step - loss: 0.7843 - acc: 0.7714 - val_loss: 1.3033 - val_acc: 0.6067\n",
      "Epoch 416/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 0.7835 - acc: 0.7686 - val_loss: 1.2998 - val_acc: 0.6033\n",
      "Epoch 417/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7829 - acc: 0.7600 - val_loss: 1.3040 - val_acc: 0.6000\n",
      "Epoch 418/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 0.7822 - acc: 0.7657 - val_loss: 1.2991 - val_acc: 0.5933\n",
      "Epoch 419/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.7806 - acc: 0.7686 - val_loss: 1.2987 - val_acc: 0.6000\n",
      "Epoch 420/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7815 - acc: 0.7686 - val_loss: 1.3008 - val_acc: 0.5900\n",
      "Epoch 421/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7814 - acc: 0.7686 - val_loss: 1.3001 - val_acc: 0.6000\n",
      "Epoch 422/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.7805 - acc: 0.7600 - val_loss: 1.3036 - val_acc: 0.5900\n",
      "Epoch 423/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.7800 - acc: 0.7600 - val_loss: 1.3062 - val_acc: 0.6067\n",
      "Epoch 424/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7793 - acc: 0.7700 - val_loss: 1.3051 - val_acc: 0.5967\n",
      "Epoch 425/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7809 - acc: 0.7686 - val_loss: 1.3113 - val_acc: 0.6000\n",
      "Epoch 426/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7783 - acc: 0.7571 - val_loss: 1.3026 - val_acc: 0.6000\n",
      "Epoch 427/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7781 - acc: 0.7700 - val_loss: 1.3037 - val_acc: 0.6000\n",
      "Epoch 428/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7781 - acc: 0.7671 - val_loss: 1.3067 - val_acc: 0.5967\n",
      "Epoch 429/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7777 - acc: 0.7643 - val_loss: 1.3081 - val_acc: 0.5967\n",
      "Epoch 430/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.7761 - acc: 0.7700 - val_loss: 1.3081 - val_acc: 0.6033\n",
      "Epoch 431/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.7758 - acc: 0.7686 - val_loss: 1.3093 - val_acc: 0.5967\n",
      "Epoch 432/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.7744 - acc: 0.7657 - val_loss: 1.3120 - val_acc: 0.6067\n",
      "Epoch 433/3000\n",
      "700/700 [==============================] - 0s 110us/step - loss: 0.7751 - acc: 0.7700 - val_loss: 1.3148 - val_acc: 0.5967\n",
      "Epoch 434/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7728 - acc: 0.7686 - val_loss: 1.3163 - val_acc: 0.6167\n",
      "Epoch 435/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.7748 - acc: 0.7657 - val_loss: 1.3106 - val_acc: 0.5967\n",
      "Epoch 436/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7728 - acc: 0.7671 - val_loss: 1.3209 - val_acc: 0.5933\n",
      "Epoch 437/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.7709 - acc: 0.7729 - val_loss: 1.3156 - val_acc: 0.6000\n",
      "Epoch 438/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7720 - acc: 0.7657 - val_loss: 1.3125 - val_acc: 0.6033\n",
      "Epoch 439/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7698 - acc: 0.7729 - val_loss: 1.3140 - val_acc: 0.6000\n",
      "Epoch 440/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7707 - acc: 0.7714 - val_loss: 1.3259 - val_acc: 0.5967\n",
      "Epoch 441/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.7710 - acc: 0.7743 - val_loss: 1.3265 - val_acc: 0.6000\n",
      "Epoch 442/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.7692 - acc: 0.7743 - val_loss: 1.3155 - val_acc: 0.6000\n",
      "Epoch 443/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7682 - acc: 0.7700 - val_loss: 1.3202 - val_acc: 0.6000\n",
      "Epoch 444/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.7682 - acc: 0.7757 - val_loss: 1.3234 - val_acc: 0.6000\n",
      "Epoch 445/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.7674 - acc: 0.7729 - val_loss: 1.3209 - val_acc: 0.5967\n",
      "Epoch 446/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.7663 - acc: 0.7743 - val_loss: 1.3234 - val_acc: 0.6000\n",
      "Epoch 447/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7669 - acc: 0.7671 - val_loss: 1.3214 - val_acc: 0.6000\n",
      "Epoch 448/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 0.7665 - acc: 0.7714 - val_loss: 1.3252 - val_acc: 0.5967\n",
      "Epoch 449/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 0.7651 - acc: 0.7729 - val_loss: 1.3357 - val_acc: 0.6000\n",
      "Epoch 450/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7641 - acc: 0.7686 - val_loss: 1.3291 - val_acc: 0.5933\n",
      "Epoch 451/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.7644 - acc: 0.7757 - val_loss: 1.3355 - val_acc: 0.5967\n",
      "Epoch 452/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.7644 - acc: 0.7771 - val_loss: 1.3312 - val_acc: 0.5900\n",
      "Epoch 453/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.7644 - acc: 0.7700 - val_loss: 1.3255 - val_acc: 0.5967\n",
      "Epoch 454/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.7643 - acc: 0.7686 - val_loss: 1.3299 - val_acc: 0.5967\n",
      "Epoch 455/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 0.7634 - acc: 0.7729 - val_loss: 1.3320 - val_acc: 0.6000\n",
      "Epoch 456/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.7621 - acc: 0.7714 - val_loss: 1.3350 - val_acc: 0.5900\n",
      "Epoch 457/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7614 - acc: 0.7657 - val_loss: 1.3410 - val_acc: 0.5967\n",
      "Epoch 458/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7593 - acc: 0.7743 - val_loss: 1.3428 - val_acc: 0.5967\n",
      "Epoch 459/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7620 - acc: 0.7771 - val_loss: 1.3343 - val_acc: 0.6000\n",
      "Epoch 460/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.7574 - acc: 0.7757 - val_loss: 1.3444 - val_acc: 0.5967\n",
      "Epoch 461/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7586 - acc: 0.7757 - val_loss: 1.3391 - val_acc: 0.5967\n",
      "Epoch 462/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.7581 - acc: 0.7729 - val_loss: 1.3488 - val_acc: 0.5933\n",
      "Epoch 463/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7591 - acc: 0.7743 - val_loss: 1.3405 - val_acc: 0.5933\n",
      "Epoch 464/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.7589 - acc: 0.7757 - val_loss: 1.3416 - val_acc: 0.5867\n",
      "Epoch 465/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7571 - acc: 0.7743 - val_loss: 1.3439 - val_acc: 0.5867\n",
      "Epoch 466/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.7574 - acc: 0.7757 - val_loss: 1.3397 - val_acc: 0.5933\n",
      "Epoch 467/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7573 - acc: 0.7729 - val_loss: 1.3411 - val_acc: 0.5967\n",
      "Epoch 468/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7543 - acc: 0.7800 - val_loss: 1.3378 - val_acc: 0.5933\n",
      "Epoch 469/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7560 - acc: 0.7700 - val_loss: 1.3511 - val_acc: 0.6000\n",
      "Epoch 470/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7531 - acc: 0.7786 - val_loss: 1.3434 - val_acc: 0.6000\n",
      "Epoch 471/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7546 - acc: 0.7829 - val_loss: 1.3423 - val_acc: 0.5967\n",
      "Epoch 472/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.7548 - acc: 0.7729 - val_loss: 1.3448 - val_acc: 0.5833\n",
      "Epoch 473/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7529 - acc: 0.7771 - val_loss: 1.3449 - val_acc: 0.5933\n",
      "Epoch 474/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 121us/step - loss: 0.7533 - acc: 0.7800 - val_loss: 1.3502 - val_acc: 0.5967\n",
      "Epoch 475/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7518 - acc: 0.7829 - val_loss: 1.3537 - val_acc: 0.5933\n",
      "Epoch 476/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7518 - acc: 0.7800 - val_loss: 1.3537 - val_acc: 0.6000\n",
      "Epoch 477/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7517 - acc: 0.7714 - val_loss: 1.3512 - val_acc: 0.5933\n",
      "Epoch 478/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.7503 - acc: 0.7786 - val_loss: 1.3555 - val_acc: 0.5933\n",
      "Epoch 479/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7515 - acc: 0.7814 - val_loss: 1.3548 - val_acc: 0.5967\n",
      "Epoch 480/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7499 - acc: 0.7829 - val_loss: 1.3564 - val_acc: 0.6000\n",
      "Epoch 481/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.7504 - acc: 0.7843 - val_loss: 1.3556 - val_acc: 0.5967\n",
      "Epoch 482/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7484 - acc: 0.7800 - val_loss: 1.3556 - val_acc: 0.6033\n",
      "Epoch 483/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7488 - acc: 0.7786 - val_loss: 1.3598 - val_acc: 0.5933\n",
      "Epoch 484/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.7472 - acc: 0.7800 - val_loss: 1.3554 - val_acc: 0.5967\n",
      "Epoch 485/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7479 - acc: 0.7729 - val_loss: 1.3603 - val_acc: 0.6000\n",
      "Epoch 486/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7460 - acc: 0.7829 - val_loss: 1.3695 - val_acc: 0.5867\n",
      "Epoch 487/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.7471 - acc: 0.7786 - val_loss: 1.3574 - val_acc: 0.6000\n",
      "Epoch 488/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.7452 - acc: 0.7786 - val_loss: 1.3596 - val_acc: 0.5967\n",
      "Epoch 489/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7440 - acc: 0.7829 - val_loss: 1.3691 - val_acc: 0.5967\n",
      "Epoch 490/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7457 - acc: 0.7843 - val_loss: 1.3620 - val_acc: 0.5967\n",
      "Epoch 491/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7432 - acc: 0.7814 - val_loss: 1.3598 - val_acc: 0.5967\n",
      "Epoch 492/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.7448 - acc: 0.7829 - val_loss: 1.3664 - val_acc: 0.5933\n",
      "Epoch 493/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.7444 - acc: 0.7829 - val_loss: 1.3679 - val_acc: 0.5967\n",
      "Epoch 494/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7429 - acc: 0.7857 - val_loss: 1.3661 - val_acc: 0.5967\n",
      "Epoch 495/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7438 - acc: 0.7771 - val_loss: 1.3626 - val_acc: 0.5967\n",
      "Epoch 496/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.7413 - acc: 0.7843 - val_loss: 1.3704 - val_acc: 0.5833\n",
      "Epoch 497/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7415 - acc: 0.7829 - val_loss: 1.3728 - val_acc: 0.5900\n",
      "Epoch 498/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7411 - acc: 0.7786 - val_loss: 1.3799 - val_acc: 0.6000\n",
      "Epoch 499/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7418 - acc: 0.7843 - val_loss: 1.3772 - val_acc: 0.6000\n",
      "Epoch 500/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7416 - acc: 0.7814 - val_loss: 1.3702 - val_acc: 0.5900\n",
      "Epoch 501/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7407 - acc: 0.7857 - val_loss: 1.3751 - val_acc: 0.5933\n",
      "Epoch 502/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7397 - acc: 0.7871 - val_loss: 1.3769 - val_acc: 0.5933\n",
      "Epoch 503/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7388 - acc: 0.7900 - val_loss: 1.3732 - val_acc: 0.5933\n",
      "Epoch 504/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7385 - acc: 0.7829 - val_loss: 1.3819 - val_acc: 0.6033\n",
      "Epoch 505/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.7382 - acc: 0.7771 - val_loss: 1.3720 - val_acc: 0.5933\n",
      "Epoch 506/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.7363 - acc: 0.7857 - val_loss: 1.3715 - val_acc: 0.5867\n",
      "Epoch 507/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7379 - acc: 0.7829 - val_loss: 1.3824 - val_acc: 0.5933\n",
      "Epoch 508/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.7362 - acc: 0.7857 - val_loss: 1.3798 - val_acc: 0.5867\n",
      "Epoch 509/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7358 - acc: 0.7814 - val_loss: 1.3818 - val_acc: 0.5967\n",
      "Epoch 510/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7353 - acc: 0.7900 - val_loss: 1.3776 - val_acc: 0.5767\n",
      "Epoch 511/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.7361 - acc: 0.7814 - val_loss: 1.3872 - val_acc: 0.5933\n",
      "Epoch 512/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.7351 - acc: 0.7857 - val_loss: 1.3812 - val_acc: 0.5833\n",
      "Epoch 513/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.7335 - acc: 0.7886 - val_loss: 1.3840 - val_acc: 0.5867\n",
      "Epoch 514/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7342 - acc: 0.7886 - val_loss: 1.3891 - val_acc: 0.5900\n",
      "Epoch 515/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7342 - acc: 0.7857 - val_loss: 1.3904 - val_acc: 0.5900\n",
      "Epoch 516/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7339 - acc: 0.7871 - val_loss: 1.3847 - val_acc: 0.5933\n",
      "Epoch 517/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 0.7340 - acc: 0.7857 - val_loss: 1.3872 - val_acc: 0.5933\n",
      "Epoch 518/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7324 - acc: 0.7914 - val_loss: 1.3872 - val_acc: 0.5900\n",
      "Epoch 519/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.7318 - acc: 0.7843 - val_loss: 1.3838 - val_acc: 0.5800\n",
      "Epoch 520/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7297 - acc: 0.7857 - val_loss: 1.3948 - val_acc: 0.5933\n",
      "Epoch 521/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7306 - acc: 0.7886 - val_loss: 1.3939 - val_acc: 0.5933\n",
      "Epoch 522/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7316 - acc: 0.7871 - val_loss: 1.3873 - val_acc: 0.5933\n",
      "Epoch 523/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.7301 - acc: 0.7886 - val_loss: 1.3911 - val_acc: 0.5933\n",
      "Epoch 524/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7292 - acc: 0.7871 - val_loss: 1.3941 - val_acc: 0.5900\n",
      "Epoch 525/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.7278 - acc: 0.7871 - val_loss: 1.3948 - val_acc: 0.5767\n",
      "Epoch 526/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7298 - acc: 0.7900 - val_loss: 1.4008 - val_acc: 0.5900\n",
      "Epoch 527/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7279 - acc: 0.7829 - val_loss: 1.4018 - val_acc: 0.5900\n",
      "Epoch 528/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.7278 - acc: 0.7857 - val_loss: 1.3916 - val_acc: 0.5833\n",
      "Epoch 529/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7264 - acc: 0.7857 - val_loss: 1.3954 - val_acc: 0.5800\n",
      "Epoch 530/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7270 - acc: 0.7886 - val_loss: 1.3983 - val_acc: 0.5900\n",
      "Epoch 531/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7269 - acc: 0.7900 - val_loss: 1.3982 - val_acc: 0.5867\n",
      "Epoch 532/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.7266 - acc: 0.7829 - val_loss: 1.3999 - val_acc: 0.5800\n",
      "Epoch 533/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 119us/step - loss: 0.7253 - acc: 0.7843 - val_loss: 1.4073 - val_acc: 0.5900\n",
      "Epoch 534/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7254 - acc: 0.7886 - val_loss: 1.4083 - val_acc: 0.5867\n",
      "Epoch 535/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7244 - acc: 0.7900 - val_loss: 1.3966 - val_acc: 0.5867\n",
      "Epoch 536/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7226 - acc: 0.7886 - val_loss: 1.4026 - val_acc: 0.5833\n",
      "Epoch 537/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7248 - acc: 0.7886 - val_loss: 1.3992 - val_acc: 0.5900\n",
      "Epoch 538/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.7247 - acc: 0.7886 - val_loss: 1.4013 - val_acc: 0.5900\n",
      "Epoch 539/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7241 - acc: 0.7900 - val_loss: 1.4005 - val_acc: 0.5900\n",
      "Epoch 540/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.7221 - acc: 0.7871 - val_loss: 1.4018 - val_acc: 0.5900\n",
      "Epoch 541/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.7215 - acc: 0.7929 - val_loss: 1.3968 - val_acc: 0.5900\n",
      "Epoch 542/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7234 - acc: 0.7886 - val_loss: 1.4033 - val_acc: 0.5900\n",
      "Epoch 543/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.7216 - acc: 0.7857 - val_loss: 1.4046 - val_acc: 0.5833\n",
      "Epoch 544/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7194 - acc: 0.7871 - val_loss: 1.4062 - val_acc: 0.5867\n",
      "Epoch 545/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7189 - acc: 0.7900 - val_loss: 1.4096 - val_acc: 0.5833\n",
      "Epoch 546/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.7194 - acc: 0.7986 - val_loss: 1.4051 - val_acc: 0.5867\n",
      "Epoch 547/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.7193 - acc: 0.7900 - val_loss: 1.4026 - val_acc: 0.5867\n",
      "Epoch 548/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7181 - acc: 0.7900 - val_loss: 1.4069 - val_acc: 0.5833\n",
      "Epoch 549/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7180 - acc: 0.7900 - val_loss: 1.4046 - val_acc: 0.5833\n",
      "Epoch 550/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7167 - acc: 0.7914 - val_loss: 1.4119 - val_acc: 0.5833\n",
      "Epoch 551/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7173 - acc: 0.7900 - val_loss: 1.4166 - val_acc: 0.5733\n",
      "Epoch 552/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.7170 - acc: 0.7929 - val_loss: 1.4171 - val_acc: 0.5833\n",
      "Epoch 553/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7176 - acc: 0.7914 - val_loss: 1.4200 - val_acc: 0.5767\n",
      "Epoch 554/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7166 - acc: 0.7886 - val_loss: 1.4206 - val_acc: 0.5800\n",
      "Epoch 555/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7133 - acc: 0.7943 - val_loss: 1.4191 - val_acc: 0.5800\n",
      "Epoch 556/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7150 - acc: 0.7914 - val_loss: 1.4148 - val_acc: 0.5900\n",
      "Epoch 557/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7137 - acc: 0.7886 - val_loss: 1.4152 - val_acc: 0.5800\n",
      "Epoch 558/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.7132 - acc: 0.7886 - val_loss: 1.4267 - val_acc: 0.5867\n",
      "Epoch 559/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7151 - acc: 0.7871 - val_loss: 1.4163 - val_acc: 0.5767\n",
      "Epoch 560/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 0.7084 - acc: 0.783 - 0s 161us/step - loss: 0.7127 - acc: 0.7871 - val_loss: 1.4279 - val_acc: 0.5733\n",
      "Epoch 561/3000\n",
      "700/700 [==============================] - 0s 141us/step - loss: 0.7138 - acc: 0.7957 - val_loss: 1.4263 - val_acc: 0.5767\n",
      "Epoch 562/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.7114 - acc: 0.7943 - val_loss: 1.4273 - val_acc: 0.5800\n",
      "Epoch 563/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7117 - acc: 0.7914 - val_loss: 1.4244 - val_acc: 0.5767\n",
      "Epoch 564/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7111 - acc: 0.7871 - val_loss: 1.4264 - val_acc: 0.5767\n",
      "Epoch 565/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7100 - acc: 0.7914 - val_loss: 1.4269 - val_acc: 0.5733\n",
      "Epoch 566/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.7104 - acc: 0.7900 - val_loss: 1.4281 - val_acc: 0.5700\n",
      "Epoch 567/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.7103 - acc: 0.7914 - val_loss: 1.4229 - val_acc: 0.5767\n",
      "Epoch 568/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.7098 - acc: 0.7929 - val_loss: 1.4312 - val_acc: 0.5767\n",
      "Epoch 569/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7084 - acc: 0.7971 - val_loss: 1.4242 - val_acc: 0.5733\n",
      "Epoch 570/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.7085 - acc: 0.7943 - val_loss: 1.4306 - val_acc: 0.5733\n",
      "Epoch 571/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.7100 - acc: 0.7914 - val_loss: 1.4270 - val_acc: 0.5667\n",
      "Epoch 572/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7078 - acc: 0.7914 - val_loss: 1.4378 - val_acc: 0.5767\n",
      "Epoch 573/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7071 - acc: 0.7929 - val_loss: 1.4512 - val_acc: 0.5900\n",
      "Epoch 574/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7060 - acc: 0.7957 - val_loss: 1.4296 - val_acc: 0.5700\n",
      "Epoch 575/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.7062 - acc: 0.7900 - val_loss: 1.4340 - val_acc: 0.5733\n",
      "Epoch 576/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.7055 - acc: 0.7914 - val_loss: 1.4426 - val_acc: 0.5800\n",
      "Epoch 577/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7062 - acc: 0.7929 - val_loss: 1.4410 - val_acc: 0.5800\n",
      "Epoch 578/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.7050 - acc: 0.7900 - val_loss: 1.4404 - val_acc: 0.5667\n",
      "Epoch 579/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.7052 - acc: 0.7914 - val_loss: 1.4350 - val_acc: 0.5767\n",
      "Epoch 580/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 0.7043 - acc: 0.7929 - val_loss: 1.4488 - val_acc: 0.5800\n",
      "Epoch 581/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.7033 - acc: 0.7957 - val_loss: 1.4415 - val_acc: 0.5733\n",
      "Epoch 582/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.7037 - acc: 0.7914 - val_loss: 1.4437 - val_acc: 0.5767\n",
      "Epoch 583/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.7021 - acc: 0.7943 - val_loss: 1.4449 - val_acc: 0.5733\n",
      "Epoch 584/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.7023 - acc: 0.7943 - val_loss: 1.4456 - val_acc: 0.5800\n",
      "Epoch 585/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.7019 - acc: 0.7943 - val_loss: 1.4515 - val_acc: 0.5800\n",
      "Epoch 586/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.7030 - acc: 0.7900 - val_loss: 1.4529 - val_acc: 0.5700\n",
      "Epoch 587/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.6999 - acc: 0.7929 - val_loss: 1.4468 - val_acc: 0.5900\n",
      "Epoch 588/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.7008 - acc: 0.7957 - val_loss: 1.4471 - val_acc: 0.5800\n",
      "Epoch 589/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6989 - acc: 0.7971 - val_loss: 1.4527 - val_acc: 0.5667\n",
      "Epoch 590/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.6997 - acc: 0.7957 - val_loss: 1.4426 - val_acc: 0.5633\n",
      "Epoch 591/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.6999 - acc: 0.7929 - val_loss: 1.4530 - val_acc: 0.5700\n",
      "Epoch 592/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 121us/step - loss: 0.6995 - acc: 0.7929 - val_loss: 1.4534 - val_acc: 0.5767\n",
      "Epoch 593/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.6988 - acc: 0.7943 - val_loss: 1.4487 - val_acc: 0.5633\n",
      "Epoch 594/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6993 - acc: 0.7971 - val_loss: 1.4495 - val_acc: 0.5733\n",
      "Epoch 595/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6977 - acc: 0.7971 - val_loss: 1.4490 - val_acc: 0.5633\n",
      "Epoch 596/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6969 - acc: 0.7943 - val_loss: 1.4563 - val_acc: 0.5733\n",
      "Epoch 597/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6959 - acc: 0.7957 - val_loss: 1.4470 - val_acc: 0.5767\n",
      "Epoch 598/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6977 - acc: 0.7929 - val_loss: 1.4595 - val_acc: 0.5667\n",
      "Epoch 599/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.6975 - acc: 0.7957 - val_loss: 1.4574 - val_acc: 0.5667\n",
      "Epoch 600/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.6959 - acc: 0.7929 - val_loss: 1.4514 - val_acc: 0.5633\n",
      "Epoch 601/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6961 - acc: 0.7957 - val_loss: 1.4539 - val_acc: 0.5633\n",
      "Epoch 602/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6967 - acc: 0.7957 - val_loss: 1.4606 - val_acc: 0.5633\n",
      "Epoch 603/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6954 - acc: 0.7957 - val_loss: 1.4611 - val_acc: 0.5667\n",
      "Epoch 604/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6943 - acc: 0.7943 - val_loss: 1.4619 - val_acc: 0.5667\n",
      "Epoch 605/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6937 - acc: 0.7929 - val_loss: 1.4625 - val_acc: 0.5700\n",
      "Epoch 606/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6943 - acc: 0.7957 - val_loss: 1.4537 - val_acc: 0.5667\n",
      "Epoch 607/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6927 - acc: 0.7957 - val_loss: 1.4563 - val_acc: 0.5667\n",
      "Epoch 608/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6923 - acc: 0.7957 - val_loss: 1.4613 - val_acc: 0.5700\n",
      "Epoch 609/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6933 - acc: 0.7971 - val_loss: 1.4717 - val_acc: 0.5600\n",
      "Epoch 610/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6921 - acc: 0.8000 - val_loss: 1.4663 - val_acc: 0.5633\n",
      "Epoch 611/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6922 - acc: 0.7971 - val_loss: 1.4727 - val_acc: 0.5700\n",
      "Epoch 612/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6906 - acc: 0.7986 - val_loss: 1.4698 - val_acc: 0.5700\n",
      "Epoch 613/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6914 - acc: 0.8000 - val_loss: 1.4665 - val_acc: 0.5567\n",
      "Epoch 614/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6894 - acc: 0.7943 - val_loss: 1.4679 - val_acc: 0.5600\n",
      "Epoch 615/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6899 - acc: 0.7957 - val_loss: 1.4744 - val_acc: 0.5633\n",
      "Epoch 616/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.6902 - acc: 0.7929 - val_loss: 1.4695 - val_acc: 0.5600\n",
      "Epoch 617/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6888 - acc: 0.7957 - val_loss: 1.4695 - val_acc: 0.5633\n",
      "Epoch 618/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6882 - acc: 0.7957 - val_loss: 1.4756 - val_acc: 0.5767\n",
      "Epoch 619/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6887 - acc: 0.8029 - val_loss: 1.4723 - val_acc: 0.5633\n",
      "Epoch 620/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.6892 - acc: 0.8000 - val_loss: 1.4753 - val_acc: 0.5633\n",
      "Epoch 621/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.6876 - acc: 0.7986 - val_loss: 1.4793 - val_acc: 0.5667\n",
      "Epoch 622/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6860 - acc: 0.7986 - val_loss: 1.4818 - val_acc: 0.5633\n",
      "Epoch 623/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.6870 - acc: 0.7971 - val_loss: 1.4785 - val_acc: 0.5667\n",
      "Epoch 624/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6863 - acc: 0.7971 - val_loss: 1.4772 - val_acc: 0.5633\n",
      "Epoch 625/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6868 - acc: 0.7943 - val_loss: 1.4814 - val_acc: 0.5600\n",
      "Epoch 626/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6865 - acc: 0.8000 - val_loss: 1.4813 - val_acc: 0.5600\n",
      "Epoch 627/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6856 - acc: 0.8014 - val_loss: 1.4729 - val_acc: 0.5600\n",
      "Epoch 628/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.6841 - acc: 0.8000 - val_loss: 1.4895 - val_acc: 0.5633\n",
      "Epoch 629/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 0.6840 - acc: 0.7957 - val_loss: 1.4905 - val_acc: 0.5567\n",
      "Epoch 630/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6856 - acc: 0.7929 - val_loss: 1.4938 - val_acc: 0.5667\n",
      "Epoch 631/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6846 - acc: 0.7957 - val_loss: 1.4909 - val_acc: 0.5600\n",
      "Epoch 632/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6830 - acc: 0.7986 - val_loss: 1.4884 - val_acc: 0.5667\n",
      "Epoch 633/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6824 - acc: 0.8000 - val_loss: 1.4820 - val_acc: 0.5600\n",
      "Epoch 634/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6829 - acc: 0.8000 - val_loss: 1.4811 - val_acc: 0.5600\n",
      "Epoch 635/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6823 - acc: 0.8014 - val_loss: 1.4829 - val_acc: 0.5633\n",
      "Epoch 636/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6826 - acc: 0.7971 - val_loss: 1.4880 - val_acc: 0.5633\n",
      "Epoch 637/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6816 - acc: 0.7986 - val_loss: 1.4788 - val_acc: 0.5600\n",
      "Epoch 638/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6824 - acc: 0.7971 - val_loss: 1.4822 - val_acc: 0.5633\n",
      "Epoch 639/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6821 - acc: 0.7986 - val_loss: 1.4875 - val_acc: 0.5633\n",
      "Epoch 640/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6809 - acc: 0.7971 - val_loss: 1.4863 - val_acc: 0.5600\n",
      "Epoch 641/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6811 - acc: 0.7986 - val_loss: 1.4971 - val_acc: 0.5567\n",
      "Epoch 642/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6804 - acc: 0.8000 - val_loss: 1.4947 - val_acc: 0.5600\n",
      "Epoch 643/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6797 - acc: 0.8000 - val_loss: 1.4960 - val_acc: 0.5567\n",
      "Epoch 644/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6791 - acc: 0.7986 - val_loss: 1.4927 - val_acc: 0.5567\n",
      "Epoch 645/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6796 - acc: 0.7971 - val_loss: 1.4957 - val_acc: 0.5567\n",
      "Epoch 646/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6773 - acc: 0.7986 - val_loss: 1.4923 - val_acc: 0.5600\n",
      "Epoch 647/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6783 - acc: 0.7971 - val_loss: 1.4912 - val_acc: 0.5567\n",
      "Epoch 648/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6765 - acc: 0.8043 - val_loss: 1.4966 - val_acc: 0.5633\n",
      "Epoch 649/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6763 - acc: 0.8000 - val_loss: 1.4970 - val_acc: 0.5633\n",
      "Epoch 650/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6782 - acc: 0.7986 - val_loss: 1.5007 - val_acc: 0.5600\n",
      "Epoch 651/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 129us/step - loss: 0.6782 - acc: 0.8000 - val_loss: 1.4943 - val_acc: 0.5600\n",
      "Epoch 652/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.6752 - acc: 0.7986 - val_loss: 1.4960 - val_acc: 0.5600\n",
      "Epoch 653/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.6758 - acc: 0.7957 - val_loss: 1.5004 - val_acc: 0.5667\n",
      "Epoch 654/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6748 - acc: 0.7986 - val_loss: 1.4915 - val_acc: 0.5567\n",
      "Epoch 655/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6757 - acc: 0.8000 - val_loss: 1.5049 - val_acc: 0.5600\n",
      "Epoch 656/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6741 - acc: 0.7986 - val_loss: 1.5127 - val_acc: 0.5600\n",
      "Epoch 657/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6752 - acc: 0.8000 - val_loss: 1.4981 - val_acc: 0.5567\n",
      "Epoch 658/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6746 - acc: 0.8014 - val_loss: 1.5080 - val_acc: 0.5633\n",
      "Epoch 659/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.6737 - acc: 0.8000 - val_loss: 1.5017 - val_acc: 0.5567\n",
      "Epoch 660/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6752 - acc: 0.7986 - val_loss: 1.5051 - val_acc: 0.5567\n",
      "Epoch 661/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6740 - acc: 0.8029 - val_loss: 1.4989 - val_acc: 0.5533\n",
      "Epoch 662/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.6740 - acc: 0.8014 - val_loss: 1.5101 - val_acc: 0.5567\n",
      "Epoch 663/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.6736 - acc: 0.7957 - val_loss: 1.5117 - val_acc: 0.5567\n",
      "Epoch 664/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6723 - acc: 0.8029 - val_loss: 1.5093 - val_acc: 0.5567\n",
      "Epoch 665/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.6711 - acc: 0.8000 - val_loss: 1.5048 - val_acc: 0.5533\n",
      "Epoch 666/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6728 - acc: 0.7986 - val_loss: 1.5164 - val_acc: 0.5533\n",
      "Epoch 667/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6699 - acc: 0.8029 - val_loss: 1.5178 - val_acc: 0.5567\n",
      "Epoch 668/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 0.6709 - acc: 0.8029 - val_loss: 1.5084 - val_acc: 0.5600\n",
      "Epoch 669/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.6717 - acc: 0.8000 - val_loss: 1.5214 - val_acc: 0.5600\n",
      "Epoch 670/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.6698 - acc: 0.8029 - val_loss: 1.5138 - val_acc: 0.5567\n",
      "Epoch 671/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6694 - acc: 0.8057 - val_loss: 1.5085 - val_acc: 0.5533\n",
      "Epoch 672/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6694 - acc: 0.8057 - val_loss: 1.5265 - val_acc: 0.5600\n",
      "Epoch 673/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6690 - acc: 0.8029 - val_loss: 1.5106 - val_acc: 0.5567\n",
      "Epoch 674/3000\n",
      "700/700 [==============================] - 0s 130us/step - loss: 0.6687 - acc: 0.8000 - val_loss: 1.5085 - val_acc: 0.5500\n",
      "Epoch 675/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6675 - acc: 0.8043 - val_loss: 1.5183 - val_acc: 0.5533\n",
      "Epoch 676/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6686 - acc: 0.8043 - val_loss: 1.5171 - val_acc: 0.5600\n",
      "Epoch 677/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.6674 - acc: 0.8043 - val_loss: 1.5175 - val_acc: 0.5567\n",
      "Epoch 678/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6671 - acc: 0.8057 - val_loss: 1.5204 - val_acc: 0.5533\n",
      "Epoch 679/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6669 - acc: 0.8029 - val_loss: 1.5228 - val_acc: 0.5567\n",
      "Epoch 680/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6668 - acc: 0.8000 - val_loss: 1.5279 - val_acc: 0.5567\n",
      "Epoch 681/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.6674 - acc: 0.8043 - val_loss: 1.5232 - val_acc: 0.5600\n",
      "Epoch 682/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.6663 - acc: 0.8029 - val_loss: 1.5266 - val_acc: 0.5600\n",
      "Epoch 683/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6665 - acc: 0.8029 - val_loss: 1.5195 - val_acc: 0.5533\n",
      "Epoch 684/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6659 - acc: 0.8029 - val_loss: 1.5232 - val_acc: 0.5600\n",
      "Epoch 685/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6639 - acc: 0.8043 - val_loss: 1.5373 - val_acc: 0.5533\n",
      "Epoch 686/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.6655 - acc: 0.7986 - val_loss: 1.5278 - val_acc: 0.5567\n",
      "Epoch 687/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6639 - acc: 0.8057 - val_loss: 1.5251 - val_acc: 0.5500\n",
      "Epoch 688/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6634 - acc: 0.8043 - val_loss: 1.5224 - val_acc: 0.5567\n",
      "Epoch 689/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6643 - acc: 0.8029 - val_loss: 1.5364 - val_acc: 0.5567\n",
      "Epoch 690/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6637 - acc: 0.8029 - val_loss: 1.5341 - val_acc: 0.5567\n",
      "Epoch 691/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.6629 - acc: 0.8043 - val_loss: 1.5390 - val_acc: 0.5567\n",
      "Epoch 692/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6621 - acc: 0.8029 - val_loss: 1.5305 - val_acc: 0.5633\n",
      "Epoch 693/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6625 - acc: 0.8057 - val_loss: 1.5378 - val_acc: 0.5500\n",
      "Epoch 694/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6626 - acc: 0.8000 - val_loss: 1.5417 - val_acc: 0.5600\n",
      "Epoch 695/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.6631 - acc: 0.8014 - val_loss: 1.5387 - val_acc: 0.5500\n",
      "Epoch 696/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.6608 - acc: 0.8057 - val_loss: 1.5362 - val_acc: 0.5600\n",
      "Epoch 697/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.6612 - acc: 0.8057 - val_loss: 1.5287 - val_acc: 0.5533\n",
      "Epoch 698/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.6606 - acc: 0.8086 - val_loss: 1.5374 - val_acc: 0.5533\n",
      "Epoch 699/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6606 - acc: 0.8057 - val_loss: 1.5361 - val_acc: 0.5533\n",
      "Epoch 700/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6600 - acc: 0.8071 - val_loss: 1.5328 - val_acc: 0.5567\n",
      "Epoch 701/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6605 - acc: 0.8043 - val_loss: 1.5383 - val_acc: 0.5500\n",
      "Epoch 702/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6595 - acc: 0.8029 - val_loss: 1.5407 - val_acc: 0.5467\n",
      "Epoch 703/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6599 - acc: 0.8043 - val_loss: 1.5433 - val_acc: 0.5500\n",
      "Epoch 704/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6594 - acc: 0.8043 - val_loss: 1.5319 - val_acc: 0.5500\n",
      "Epoch 705/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.6585 - acc: 0.8057 - val_loss: 1.5358 - val_acc: 0.5567\n",
      "Epoch 706/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.6580 - acc: 0.8043 - val_loss: 1.5372 - val_acc: 0.5533\n",
      "Epoch 707/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6584 - acc: 0.8043 - val_loss: 1.5485 - val_acc: 0.5533\n",
      "Epoch 708/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.6582 - acc: 0.8043 - val_loss: 1.5431 - val_acc: 0.5500\n",
      "Epoch 709/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.6574 - acc: 0.8057 - val_loss: 1.5473 - val_acc: 0.5500\n",
      "Epoch 710/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 151us/step - loss: 0.6571 - acc: 0.8043 - val_loss: 1.5449 - val_acc: 0.5533\n",
      "Epoch 711/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6566 - acc: 0.8029 - val_loss: 1.5333 - val_acc: 0.5400\n",
      "Epoch 712/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6585 - acc: 0.8057 - val_loss: 1.5513 - val_acc: 0.5500\n",
      "Epoch 713/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.6557 - acc: 0.8071 - val_loss: 1.5436 - val_acc: 0.5500\n",
      "Epoch 714/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 0.6539 - acc: 0.8043 - val_loss: 1.5590 - val_acc: 0.5533\n",
      "Epoch 715/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.6551 - acc: 0.8071 - val_loss: 1.5561 - val_acc: 0.5467\n",
      "Epoch 716/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6544 - acc: 0.8057 - val_loss: 1.5507 - val_acc: 0.5500\n",
      "Epoch 717/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.6542 - acc: 0.8071 - val_loss: 1.5439 - val_acc: 0.5500\n",
      "Epoch 718/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.6533 - acc: 0.8071 - val_loss: 1.5693 - val_acc: 0.5567\n",
      "Epoch 719/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6546 - acc: 0.8071 - val_loss: 1.5583 - val_acc: 0.5467\n",
      "Epoch 720/3000\n",
      "700/700 [==============================] - 0s 130us/step - loss: 0.6539 - acc: 0.8029 - val_loss: 1.5559 - val_acc: 0.5500\n",
      "Epoch 721/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6528 - acc: 0.8057 - val_loss: 1.5546 - val_acc: 0.5467\n",
      "Epoch 722/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.6533 - acc: 0.8057 - val_loss: 1.5558 - val_acc: 0.5533\n",
      "Epoch 723/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6520 - acc: 0.8071 - val_loss: 1.5529 - val_acc: 0.5500\n",
      "Epoch 724/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6526 - acc: 0.8071 - val_loss: 1.5507 - val_acc: 0.5500\n",
      "Epoch 725/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6502 - acc: 0.8100 - val_loss: 1.5788 - val_acc: 0.5567\n",
      "Epoch 726/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6521 - acc: 0.8043 - val_loss: 1.5631 - val_acc: 0.5567\n",
      "Epoch 727/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6523 - acc: 0.8071 - val_loss: 1.5618 - val_acc: 0.5600\n",
      "Epoch 728/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6513 - acc: 0.8086 - val_loss: 1.5640 - val_acc: 0.5533\n",
      "Epoch 729/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6509 - acc: 0.8029 - val_loss: 1.5649 - val_acc: 0.5467\n",
      "Epoch 730/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.6501 - acc: 0.8086 - val_loss: 1.5569 - val_acc: 0.5433\n",
      "Epoch 731/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6488 - acc: 0.8071 - val_loss: 1.5804 - val_acc: 0.5600\n",
      "Epoch 732/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6499 - acc: 0.8100 - val_loss: 1.5593 - val_acc: 0.5467\n",
      "Epoch 733/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6491 - acc: 0.8057 - val_loss: 1.5629 - val_acc: 0.5500\n",
      "Epoch 734/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6502 - acc: 0.8043 - val_loss: 1.5598 - val_acc: 0.5500\n",
      "Epoch 735/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6491 - acc: 0.8014 - val_loss: 1.5678 - val_acc: 0.5500\n",
      "Epoch 736/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6484 - acc: 0.8086 - val_loss: 1.5632 - val_acc: 0.5500\n",
      "Epoch 737/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.6489 - acc: 0.8071 - val_loss: 1.5673 - val_acc: 0.5567\n",
      "Epoch 738/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6480 - acc: 0.8071 - val_loss: 1.5678 - val_acc: 0.5500\n",
      "Epoch 739/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6486 - acc: 0.8057 - val_loss: 1.5656 - val_acc: 0.5467\n",
      "Epoch 740/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6463 - acc: 0.8114 - val_loss: 1.5718 - val_acc: 0.5500\n",
      "Epoch 741/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6467 - acc: 0.8100 - val_loss: 1.5681 - val_acc: 0.5500\n",
      "Epoch 742/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.6483 - acc: 0.8071 - val_loss: 1.5750 - val_acc: 0.5500\n",
      "Epoch 743/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.6465 - acc: 0.8071 - val_loss: 1.5786 - val_acc: 0.5500\n",
      "Epoch 744/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6457 - acc: 0.8071 - val_loss: 1.5776 - val_acc: 0.5500\n",
      "Epoch 745/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6467 - acc: 0.8043 - val_loss: 1.5718 - val_acc: 0.5500\n",
      "Epoch 746/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6464 - acc: 0.8071 - val_loss: 1.5727 - val_acc: 0.5467\n",
      "Epoch 747/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6437 - acc: 0.8086 - val_loss: 1.5779 - val_acc: 0.5567\n",
      "Epoch 748/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6454 - acc: 0.8086 - val_loss: 1.5815 - val_acc: 0.5500\n",
      "Epoch 749/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6454 - acc: 0.8057 - val_loss: 1.5761 - val_acc: 0.5500\n",
      "Epoch 750/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6439 - acc: 0.8057 - val_loss: 1.5825 - val_acc: 0.5500\n",
      "Epoch 751/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6430 - acc: 0.8100 - val_loss: 1.5711 - val_acc: 0.5500\n",
      "Epoch 752/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.6441 - acc: 0.8086 - val_loss: 1.5788 - val_acc: 0.5500\n",
      "Epoch 753/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.6433 - acc: 0.8086 - val_loss: 1.5763 - val_acc: 0.5567\n",
      "Epoch 754/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6450 - acc: 0.8057 - val_loss: 1.5797 - val_acc: 0.5500\n",
      "Epoch 755/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.6428 - acc: 0.8071 - val_loss: 1.5843 - val_acc: 0.5533\n",
      "Epoch 756/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6421 - acc: 0.8057 - val_loss: 1.5821 - val_acc: 0.5500\n",
      "Epoch 757/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6419 - acc: 0.8071 - val_loss: 1.5798 - val_acc: 0.5500\n",
      "Epoch 758/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6423 - acc: 0.8043 - val_loss: 1.5924 - val_acc: 0.5500\n",
      "Epoch 759/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6420 - acc: 0.8086 - val_loss: 1.5788 - val_acc: 0.5500\n",
      "Epoch 760/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6404 - acc: 0.8086 - val_loss: 1.5865 - val_acc: 0.5500\n",
      "Epoch 761/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6417 - acc: 0.8043 - val_loss: 1.5793 - val_acc: 0.5467\n",
      "Epoch 762/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.6411 - acc: 0.8043 - val_loss: 1.5807 - val_acc: 0.5533\n",
      "Epoch 763/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6405 - acc: 0.8100 - val_loss: 1.5829 - val_acc: 0.5567\n",
      "Epoch 764/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6402 - acc: 0.8071 - val_loss: 1.5907 - val_acc: 0.5533\n",
      "Epoch 765/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6405 - acc: 0.8071 - val_loss: 1.5912 - val_acc: 0.5567\n",
      "Epoch 766/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6391 - acc: 0.8071 - val_loss: 1.6001 - val_acc: 0.5533\n",
      "Epoch 767/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.6388 - acc: 0.8086 - val_loss: 1.6031 - val_acc: 0.5500\n",
      "Epoch 768/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6396 - acc: 0.8086 - val_loss: 1.5994 - val_acc: 0.5500\n",
      "Epoch 769/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 124us/step - loss: 0.6382 - acc: 0.8071 - val_loss: 1.5948 - val_acc: 0.5467\n",
      "Epoch 770/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6376 - acc: 0.8086 - val_loss: 1.5933 - val_acc: 0.5467\n",
      "Epoch 771/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.6384 - acc: 0.8071 - val_loss: 1.5986 - val_acc: 0.5467\n",
      "Epoch 772/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.6387 - acc: 0.8043 - val_loss: 1.6096 - val_acc: 0.5500\n",
      "Epoch 773/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6373 - acc: 0.8100 - val_loss: 1.6005 - val_acc: 0.5500\n",
      "Epoch 774/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6358 - acc: 0.8071 - val_loss: 1.5968 - val_acc: 0.5500\n",
      "Epoch 775/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.6355 - acc: 0.8100 - val_loss: 1.6043 - val_acc: 0.5500\n",
      "Epoch 776/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.6367 - acc: 0.8100 - val_loss: 1.6068 - val_acc: 0.5500\n",
      "Epoch 777/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6360 - acc: 0.8071 - val_loss: 1.5937 - val_acc: 0.5467\n",
      "Epoch 778/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.6355 - acc: 0.8071 - val_loss: 1.6067 - val_acc: 0.5533\n",
      "Epoch 779/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6338 - acc: 0.8100 - val_loss: 1.6053 - val_acc: 0.5500\n",
      "Epoch 780/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6356 - acc: 0.8057 - val_loss: 1.5936 - val_acc: 0.5367\n",
      "Epoch 781/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6352 - acc: 0.8086 - val_loss: 1.6136 - val_acc: 0.5500\n",
      "Epoch 782/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6364 - acc: 0.8057 - val_loss: 1.6070 - val_acc: 0.5467\n",
      "Epoch 783/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6356 - acc: 0.8071 - val_loss: 1.6008 - val_acc: 0.5467\n",
      "Epoch 784/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.6352 - acc: 0.8086 - val_loss: 1.6019 - val_acc: 0.5500\n",
      "Epoch 785/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6338 - acc: 0.8057 - val_loss: 1.6050 - val_acc: 0.5467\n",
      "Epoch 786/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.6343 - acc: 0.8086 - val_loss: 1.6037 - val_acc: 0.5467\n",
      "Epoch 787/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.6330 - acc: 0.8100 - val_loss: 1.6095 - val_acc: 0.5467\n",
      "Epoch 788/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6334 - acc: 0.8086 - val_loss: 1.6195 - val_acc: 0.5533\n",
      "Epoch 789/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6342 - acc: 0.8129 - val_loss: 1.6056 - val_acc: 0.5500\n",
      "Epoch 790/3000\n",
      "700/700 [==============================] - 0s 130us/step - loss: 0.6314 - acc: 0.8100 - val_loss: 1.6248 - val_acc: 0.5533\n",
      "Epoch 791/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6320 - acc: 0.8086 - val_loss: 1.6002 - val_acc: 0.5433\n",
      "Epoch 792/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6315 - acc: 0.8100 - val_loss: 1.6160 - val_acc: 0.5533\n",
      "Epoch 793/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6306 - acc: 0.8114 - val_loss: 1.6152 - val_acc: 0.5533\n",
      "Epoch 794/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6303 - acc: 0.8100 - val_loss: 1.5991 - val_acc: 0.5433\n",
      "Epoch 795/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6297 - acc: 0.8129 - val_loss: 1.6013 - val_acc: 0.5467\n",
      "Epoch 796/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6290 - acc: 0.8100 - val_loss: 1.6277 - val_acc: 0.5533\n",
      "Epoch 797/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6294 - acc: 0.8129 - val_loss: 1.6060 - val_acc: 0.5467\n",
      "Epoch 798/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6296 - acc: 0.8129 - val_loss: 1.6063 - val_acc: 0.5467\n",
      "Epoch 799/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6288 - acc: 0.8100 - val_loss: 1.6021 - val_acc: 0.5467\n",
      "Epoch 800/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6283 - acc: 0.8100 - val_loss: 1.6143 - val_acc: 0.5567\n",
      "Epoch 801/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6288 - acc: 0.8100 - val_loss: 1.6157 - val_acc: 0.5533\n",
      "Epoch 802/3000\n",
      "700/700 [==============================] - 0s 130us/step - loss: 0.6289 - acc: 0.8129 - val_loss: 1.6119 - val_acc: 0.5467\n",
      "Epoch 803/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6272 - acc: 0.8129 - val_loss: 1.6293 - val_acc: 0.5533\n",
      "Epoch 804/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.6273 - acc: 0.8100 - val_loss: 1.6020 - val_acc: 0.5400\n",
      "Epoch 805/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6277 - acc: 0.8129 - val_loss: 1.5993 - val_acc: 0.5433\n",
      "Epoch 806/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.6273 - acc: 0.8129 - val_loss: 1.6245 - val_acc: 0.5533\n",
      "Epoch 807/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.6264 - acc: 0.8100 - val_loss: 1.6077 - val_acc: 0.5400\n",
      "Epoch 808/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6252 - acc: 0.8129 - val_loss: 1.6293 - val_acc: 0.5500\n",
      "Epoch 809/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6260 - acc: 0.8129 - val_loss: 1.6192 - val_acc: 0.5533\n",
      "Epoch 810/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.6240 - acc: 0.8129 - val_loss: 1.6378 - val_acc: 0.5500\n",
      "Epoch 811/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6265 - acc: 0.8114 - val_loss: 1.6259 - val_acc: 0.5433\n",
      "Epoch 812/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6258 - acc: 0.8114 - val_loss: 1.6241 - val_acc: 0.5533\n",
      "Epoch 813/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.6245 - acc: 0.8100 - val_loss: 1.6093 - val_acc: 0.5433\n",
      "Epoch 814/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.6250 - acc: 0.8129 - val_loss: 1.6235 - val_acc: 0.5500\n",
      "Epoch 815/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.6240 - acc: 0.8100 - val_loss: 1.6195 - val_acc: 0.5433\n",
      "Epoch 816/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6245 - acc: 0.8100 - val_loss: 1.6249 - val_acc: 0.5500\n",
      "Epoch 817/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6240 - acc: 0.8114 - val_loss: 1.6180 - val_acc: 0.5500\n",
      "Epoch 818/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.6229 - acc: 0.8129 - val_loss: 1.6302 - val_acc: 0.5533\n",
      "Epoch 819/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.6236 - acc: 0.8143 - val_loss: 1.6176 - val_acc: 0.5433\n",
      "Epoch 820/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6234 - acc: 0.8071 - val_loss: 1.6259 - val_acc: 0.5533\n",
      "Epoch 821/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6221 - acc: 0.8100 - val_loss: 1.6266 - val_acc: 0.5467\n",
      "Epoch 822/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.6223 - acc: 0.8114 - val_loss: 1.6343 - val_acc: 0.5533\n",
      "Epoch 823/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6223 - acc: 0.8129 - val_loss: 1.6221 - val_acc: 0.5500\n",
      "Epoch 824/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6209 - acc: 0.8143 - val_loss: 1.6294 - val_acc: 0.5467\n",
      "Epoch 825/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.6203 - acc: 0.8071 - val_loss: 1.6283 - val_acc: 0.5433\n",
      "Epoch 826/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6214 - acc: 0.8114 - val_loss: 1.6399 - val_acc: 0.5433\n",
      "Epoch 827/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.6205 - acc: 0.8114 - val_loss: 1.6395 - val_acc: 0.5533\n",
      "Epoch 828/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 118us/step - loss: 0.6196 - acc: 0.8143 - val_loss: 1.6463 - val_acc: 0.5600\n",
      "Epoch 829/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6205 - acc: 0.8100 - val_loss: 1.6246 - val_acc: 0.5433\n",
      "Epoch 830/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.6178 - acc: 0.8143 - val_loss: 1.6416 - val_acc: 0.5533\n",
      "Epoch 831/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6194 - acc: 0.8114 - val_loss: 1.6348 - val_acc: 0.5500\n",
      "Epoch 832/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6199 - acc: 0.8143 - val_loss: 1.6343 - val_acc: 0.5500\n",
      "Epoch 833/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6181 - acc: 0.8100 - val_loss: 1.6290 - val_acc: 0.5467\n",
      "Epoch 834/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6198 - acc: 0.8129 - val_loss: 1.6445 - val_acc: 0.5533\n",
      "Epoch 835/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.6184 - acc: 0.8114 - val_loss: 1.6406 - val_acc: 0.5500\n",
      "Epoch 836/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6182 - acc: 0.8129 - val_loss: 1.6465 - val_acc: 0.5533\n",
      "Epoch 837/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.6176 - acc: 0.8143 - val_loss: 1.6400 - val_acc: 0.5467\n",
      "Epoch 838/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6177 - acc: 0.8129 - val_loss: 1.6371 - val_acc: 0.5433\n",
      "Epoch 839/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6165 - acc: 0.8143 - val_loss: 1.6326 - val_acc: 0.5433\n",
      "Epoch 840/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6166 - acc: 0.8114 - val_loss: 1.6347 - val_acc: 0.5433\n",
      "Epoch 841/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 0.6106 - acc: 0.816 - 0s 117us/step - loss: 0.6186 - acc: 0.8143 - val_loss: 1.6369 - val_acc: 0.5500\n",
      "Epoch 842/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.6175 - acc: 0.8129 - val_loss: 1.6373 - val_acc: 0.5467\n",
      "Epoch 843/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6155 - acc: 0.8143 - val_loss: 1.6288 - val_acc: 0.5433\n",
      "Epoch 844/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6156 - acc: 0.8129 - val_loss: 1.6570 - val_acc: 0.5567\n",
      "Epoch 845/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6166 - acc: 0.8114 - val_loss: 1.6548 - val_acc: 0.5500\n",
      "Epoch 846/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6156 - acc: 0.8114 - val_loss: 1.6289 - val_acc: 0.5400\n",
      "Epoch 847/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6162 - acc: 0.8129 - val_loss: 1.6505 - val_acc: 0.5500\n",
      "Epoch 848/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.6161 - acc: 0.8129 - val_loss: 1.6467 - val_acc: 0.5500\n",
      "Epoch 849/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6144 - acc: 0.8157 - val_loss: 1.6554 - val_acc: 0.5533\n",
      "Epoch 850/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6137 - acc: 0.8114 - val_loss: 1.6414 - val_acc: 0.5467\n",
      "Epoch 851/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6125 - acc: 0.8129 - val_loss: 1.6373 - val_acc: 0.5433\n",
      "Epoch 852/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.6139 - acc: 0.8157 - val_loss: 1.6499 - val_acc: 0.5467\n",
      "Epoch 853/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6143 - acc: 0.8071 - val_loss: 1.6527 - val_acc: 0.5500\n",
      "Epoch 854/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6124 - acc: 0.8157 - val_loss: 1.6597 - val_acc: 0.5567\n",
      "Epoch 855/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6151 - acc: 0.8114 - val_loss: 1.6537 - val_acc: 0.5500\n",
      "Epoch 856/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6117 - acc: 0.8129 - val_loss: 1.6485 - val_acc: 0.5433\n",
      "Epoch 857/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6129 - acc: 0.8157 - val_loss: 1.6526 - val_acc: 0.5467\n",
      "Epoch 858/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.6115 - acc: 0.8157 - val_loss: 1.6580 - val_acc: 0.5500\n",
      "Epoch 859/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6124 - acc: 0.8157 - val_loss: 1.6546 - val_acc: 0.5500\n",
      "Epoch 860/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 0.6102 - acc: 0.8114 - val_loss: 1.6534 - val_acc: 0.5433\n",
      "Epoch 861/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6109 - acc: 0.8129 - val_loss: 1.6430 - val_acc: 0.5400\n",
      "Epoch 862/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6113 - acc: 0.8129 - val_loss: 1.6563 - val_acc: 0.5433\n",
      "Epoch 863/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6104 - acc: 0.8157 - val_loss: 1.6639 - val_acc: 0.5533\n",
      "Epoch 864/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6094 - acc: 0.8143 - val_loss: 1.6442 - val_acc: 0.5433\n",
      "Epoch 865/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 0.6106 - acc: 0.8157 - val_loss: 1.6487 - val_acc: 0.5433\n",
      "Epoch 866/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6093 - acc: 0.8186 - val_loss: 1.6561 - val_acc: 0.5500\n",
      "Epoch 867/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6089 - acc: 0.8171 - val_loss: 1.6581 - val_acc: 0.5533\n",
      "Epoch 868/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6103 - acc: 0.8157 - val_loss: 1.6554 - val_acc: 0.5500\n",
      "Epoch 869/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6089 - acc: 0.8157 - val_loss: 1.6691 - val_acc: 0.5500\n",
      "Epoch 870/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6083 - acc: 0.8186 - val_loss: 1.6646 - val_acc: 0.5533\n",
      "Epoch 871/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.6076 - acc: 0.8157 - val_loss: 1.6519 - val_acc: 0.5467\n",
      "Epoch 872/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.6080 - acc: 0.8171 - val_loss: 1.6541 - val_acc: 0.5400\n",
      "Epoch 873/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.6078 - acc: 0.8171 - val_loss: 1.6582 - val_acc: 0.5467\n",
      "Epoch 874/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.6077 - acc: 0.8143 - val_loss: 1.6507 - val_acc: 0.5400\n",
      "Epoch 875/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6078 - acc: 0.8171 - val_loss: 1.6554 - val_acc: 0.5467\n",
      "Epoch 876/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6068 - acc: 0.8143 - val_loss: 1.6698 - val_acc: 0.5533\n",
      "Epoch 877/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.6053 - acc: 0.8186 - val_loss: 1.6529 - val_acc: 0.5467\n",
      "Epoch 878/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6051 - acc: 0.8157 - val_loss: 1.6598 - val_acc: 0.5433\n",
      "Epoch 879/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6054 - acc: 0.8200 - val_loss: 1.6682 - val_acc: 0.5533\n",
      "Epoch 880/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6062 - acc: 0.8171 - val_loss: 1.6700 - val_acc: 0.5500\n",
      "Epoch 881/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6050 - acc: 0.8157 - val_loss: 1.6726 - val_acc: 0.5467\n",
      "Epoch 882/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 0.6055 - acc: 0.8186 - val_loss: 1.6624 - val_acc: 0.5433\n",
      "Epoch 883/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.6060 - acc: 0.8186 - val_loss: 1.6641 - val_acc: 0.5433\n",
      "Epoch 884/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.6053 - acc: 0.8143 - val_loss: 1.6685 - val_acc: 0.5433\n",
      "Epoch 885/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6046 - acc: 0.8200 - val_loss: 1.6728 - val_acc: 0.5467\n",
      "Epoch 886/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.6038 - acc: 0.8200 - val_loss: 1.6790 - val_acc: 0.5467\n",
      "Epoch 887/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 117us/step - loss: 0.6046 - acc: 0.8171 - val_loss: 1.6643 - val_acc: 0.5500\n",
      "Epoch 888/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.6029 - acc: 0.8114 - val_loss: 1.6782 - val_acc: 0.5500\n",
      "Epoch 889/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6032 - acc: 0.8171 - val_loss: 1.6721 - val_acc: 0.5533\n",
      "Epoch 890/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6033 - acc: 0.8157 - val_loss: 1.6643 - val_acc: 0.5467\n",
      "Epoch 891/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6041 - acc: 0.8157 - val_loss: 1.6648 - val_acc: 0.5400\n",
      "Epoch 892/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6023 - acc: 0.8171 - val_loss: 1.6745 - val_acc: 0.5467\n",
      "Epoch 893/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.6020 - acc: 0.8200 - val_loss: 1.6755 - val_acc: 0.5467\n",
      "Epoch 894/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6029 - acc: 0.8143 - val_loss: 1.6747 - val_acc: 0.5467\n",
      "Epoch 895/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.6010 - acc: 0.8186 - val_loss: 1.6940 - val_acc: 0.5567\n",
      "Epoch 896/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.6028 - acc: 0.8143 - val_loss: 1.6703 - val_acc: 0.5433\n",
      "Epoch 897/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.6024 - acc: 0.8200 - val_loss: 1.6746 - val_acc: 0.5467\n",
      "Epoch 898/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.6012 - acc: 0.8100 - val_loss: 1.6790 - val_acc: 0.5467\n",
      "Epoch 899/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.5997 - acc: 0.8157 - val_loss: 1.6755 - val_acc: 0.5467\n",
      "Epoch 900/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.6010 - acc: 0.8186 - val_loss: 1.6749 - val_acc: 0.5433\n",
      "Epoch 901/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.5999 - acc: 0.8157 - val_loss: 1.6772 - val_acc: 0.5467\n",
      "Epoch 902/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.5997 - acc: 0.8200 - val_loss: 1.6887 - val_acc: 0.5567\n",
      "Epoch 903/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.5993 - acc: 0.8157 - val_loss: 1.6993 - val_acc: 0.5600\n",
      "Epoch 904/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 0.5991 - acc: 0.8143 - val_loss: 1.6810 - val_acc: 0.5467\n",
      "Epoch 905/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 0.5999 - acc: 0.8186 - val_loss: 1.6752 - val_acc: 0.5400\n",
      "Epoch 906/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.6002 - acc: 0.8171 - val_loss: 1.6864 - val_acc: 0.5533\n",
      "Epoch 907/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.5992 - acc: 0.8186 - val_loss: 1.6925 - val_acc: 0.5533\n",
      "Epoch 908/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.5990 - acc: 0.8171 - val_loss: 1.6870 - val_acc: 0.5567\n",
      "Epoch 909/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.5998 - acc: 0.8200 - val_loss: 1.6816 - val_acc: 0.5533\n",
      "Epoch 910/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 0.5975 - acc: 0.8186 - val_loss: 1.6826 - val_acc: 0.5467\n",
      "Epoch 911/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.5991 - acc: 0.8200 - val_loss: 1.6809 - val_acc: 0.5467\n",
      "Epoch 912/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 0.5988 - acc: 0.8214 - val_loss: 1.7011 - val_acc: 0.5533\n",
      "Epoch 913/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.5965 - acc: 0.8229 - val_loss: 1.6775 - val_acc: 0.5500\n",
      "Epoch 914/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.5969 - acc: 0.8214 - val_loss: 1.6752 - val_acc: 0.5467\n",
      "Epoch 915/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.5968 - acc: 0.8200 - val_loss: 1.6908 - val_acc: 0.5500\n",
      "Epoch 916/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.5958 - acc: 0.8171 - val_loss: 1.6838 - val_acc: 0.5500\n",
      "Epoch 917/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.5966 - acc: 0.8200 - val_loss: 1.6882 - val_acc: 0.5500\n",
      "Epoch 918/3000\n",
      "700/700 [==============================] - 0s 141us/step - loss: 0.5961 - acc: 0.8143 - val_loss: 1.6918 - val_acc: 0.5500\n",
      "Epoch 919/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.5958 - acc: 0.8171 - val_loss: 1.6874 - val_acc: 0.5467\n",
      "Epoch 920/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5955 - acc: 0.8171 - val_loss: 1.6924 - val_acc: 0.5500\n",
      "Epoch 921/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.5941 - acc: 0.8186 - val_loss: 1.6892 - val_acc: 0.5500\n",
      "Epoch 922/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 0.5958 - acc: 0.8186 - val_loss: 1.6917 - val_acc: 0.5500\n",
      "Epoch 923/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.5965 - acc: 0.8200 - val_loss: 1.6934 - val_acc: 0.5500\n",
      "Epoch 924/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.5935 - acc: 0.8186 - val_loss: 1.7149 - val_acc: 0.5600\n",
      "Epoch 925/3000\n",
      "700/700 [==============================] - 0s 140us/step - loss: 0.5952 - acc: 0.8200 - val_loss: 1.6847 - val_acc: 0.5500\n",
      "Epoch 926/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 0.5946 - acc: 0.8171 - val_loss: 1.6986 - val_acc: 0.5467\n",
      "Epoch 927/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5943 - acc: 0.8186 - val_loss: 1.6952 - val_acc: 0.5500\n",
      "Epoch 928/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 0.5937 - acc: 0.8186 - val_loss: 1.6931 - val_acc: 0.5467\n",
      "Epoch 929/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.5932 - acc: 0.8214 - val_loss: 1.6957 - val_acc: 0.5500\n",
      "Epoch 930/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.5941 - acc: 0.8171 - val_loss: 1.6944 - val_acc: 0.5467\n",
      "Epoch 931/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.5915 - acc: 0.8186 - val_loss: 1.6972 - val_acc: 0.5433\n",
      "Epoch 932/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.5929 - acc: 0.8243 - val_loss: 1.7015 - val_acc: 0.5467\n",
      "Epoch 933/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.5936 - acc: 0.8200 - val_loss: 1.7082 - val_acc: 0.5533\n",
      "Epoch 934/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.5925 - acc: 0.8229 - val_loss: 1.7057 - val_acc: 0.5533\n",
      "Epoch 935/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 0.5933 - acc: 0.8186 - val_loss: 1.7069 - val_acc: 0.5467\n",
      "Epoch 936/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.5929 - acc: 0.8186 - val_loss: 1.7046 - val_acc: 0.5467\n",
      "Epoch 937/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.5914 - acc: 0.8214 - val_loss: 1.7109 - val_acc: 0.5500\n",
      "Epoch 938/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.5905 - acc: 0.8186 - val_loss: 1.7062 - val_acc: 0.5500\n",
      "Epoch 939/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.5924 - acc: 0.8186 - val_loss: 1.6999 - val_acc: 0.5533\n",
      "Epoch 940/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.5914 - acc: 0.8171 - val_loss: 1.7034 - val_acc: 0.5533\n",
      "Epoch 941/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.5910 - acc: 0.8214 - val_loss: 1.7206 - val_acc: 0.5467\n",
      "Epoch 942/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.5908 - acc: 0.8214 - val_loss: 1.7095 - val_acc: 0.5500\n",
      "Epoch 943/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.5903 - acc: 0.8200 - val_loss: 1.7074 - val_acc: 0.5500\n",
      "Epoch 944/3000\n",
      "700/700 [==============================] - 0s 140us/step - loss: 0.5903 - acc: 0.8271 - val_loss: 1.7043 - val_acc: 0.5500\n",
      "Epoch 945/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.5903 - acc: 0.8214 - val_loss: 1.7048 - val_acc: 0.5500\n",
      "Epoch 946/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 134us/step - loss: 0.5892 - acc: 0.8171 - val_loss: 1.7065 - val_acc: 0.5533\n",
      "Epoch 947/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.5895 - acc: 0.8214 - val_loss: 1.7094 - val_acc: 0.5533\n",
      "Epoch 948/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.5898 - acc: 0.8186 - val_loss: 1.7082 - val_acc: 0.5533\n",
      "Epoch 949/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.5871 - acc: 0.8243 - val_loss: 1.7002 - val_acc: 0.5500\n",
      "Epoch 950/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.5880 - acc: 0.8214 - val_loss: 1.7080 - val_acc: 0.5533\n",
      "Epoch 951/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.5886 - acc: 0.8214 - val_loss: 1.7220 - val_acc: 0.5567\n",
      "Epoch 952/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.5881 - acc: 0.8214 - val_loss: 1.7101 - val_acc: 0.5533\n",
      "Epoch 953/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.5887 - acc: 0.8214 - val_loss: 1.7269 - val_acc: 0.5533\n",
      "Epoch 954/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5888 - acc: 0.8229 - val_loss: 1.7131 - val_acc: 0.5533\n",
      "Epoch 955/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.5872 - acc: 0.8229 - val_loss: 1.7146 - val_acc: 0.5533\n",
      "Epoch 956/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.5878 - acc: 0.8157 - val_loss: 1.7135 - val_acc: 0.5533\n",
      "Epoch 957/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.5863 - acc: 0.8186 - val_loss: 1.7136 - val_acc: 0.5500\n",
      "Epoch 958/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 0.5869 - acc: 0.8200 - val_loss: 1.7342 - val_acc: 0.5533\n",
      "Epoch 959/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 0.5881 - acc: 0.8229 - val_loss: 1.7057 - val_acc: 0.5467\n",
      "Epoch 960/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.5869 - acc: 0.8171 - val_loss: 1.7172 - val_acc: 0.5500\n",
      "Epoch 961/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.5858 - acc: 0.8229 - val_loss: 1.7144 - val_acc: 0.5500\n",
      "Epoch 962/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5858 - acc: 0.8229 - val_loss: 1.7125 - val_acc: 0.5467\n",
      "Epoch 963/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.5860 - acc: 0.8200 - val_loss: 1.7253 - val_acc: 0.5500\n",
      "Epoch 964/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.5852 - acc: 0.8229 - val_loss: 1.7217 - val_acc: 0.5500\n",
      "Epoch 965/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.5848 - acc: 0.8171 - val_loss: 1.7340 - val_acc: 0.5500\n",
      "Epoch 966/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.5845 - acc: 0.8257 - val_loss: 1.7187 - val_acc: 0.5500\n",
      "Epoch 967/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.5845 - acc: 0.8229 - val_loss: 1.7201 - val_acc: 0.5500\n",
      "Epoch 968/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.5837 - acc: 0.8214 - val_loss: 1.7138 - val_acc: 0.5433\n",
      "Epoch 969/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.5839 - acc: 0.8271 - val_loss: 1.7267 - val_acc: 0.5433\n",
      "Epoch 970/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.5830 - acc: 0.8286 - val_loss: 1.7208 - val_acc: 0.5500\n",
      "Epoch 971/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.5844 - acc: 0.8229 - val_loss: 1.7274 - val_acc: 0.5500\n",
      "Epoch 972/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 0.5847 - acc: 0.8243 - val_loss: 1.7214 - val_acc: 0.5500\n",
      "Epoch 973/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.5833 - acc: 0.8214 - val_loss: 1.7266 - val_acc: 0.5500\n",
      "Epoch 974/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.5821 - acc: 0.8229 - val_loss: 1.7236 - val_acc: 0.5533\n",
      "Epoch 975/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.5840 - acc: 0.8257 - val_loss: 1.7334 - val_acc: 0.5467\n",
      "Epoch 976/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.5815 - acc: 0.8214 - val_loss: 1.7233 - val_acc: 0.5500\n",
      "Epoch 977/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.5831 - acc: 0.8257 - val_loss: 1.7288 - val_acc: 0.5467\n",
      "Epoch 978/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.5819 - acc: 0.8257 - val_loss: 1.7445 - val_acc: 0.5533\n",
      "Epoch 979/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.5835 - acc: 0.8243 - val_loss: 1.7385 - val_acc: 0.5533\n",
      "Epoch 980/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.5822 - acc: 0.8243 - val_loss: 1.7320 - val_acc: 0.5567\n",
      "Epoch 981/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5808 - acc: 0.8257 - val_loss: 1.7351 - val_acc: 0.5533\n",
      "Epoch 982/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5825 - acc: 0.8257 - val_loss: 1.7309 - val_acc: 0.5533\n",
      "Epoch 983/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.5801 - acc: 0.8243 - val_loss: 1.7363 - val_acc: 0.5500\n",
      "Epoch 984/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5819 - acc: 0.8229 - val_loss: 1.7296 - val_acc: 0.5500\n",
      "Epoch 985/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.5807 - acc: 0.8271 - val_loss: 1.7263 - val_acc: 0.5500\n",
      "Epoch 986/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.5812 - acc: 0.8214 - val_loss: 1.7409 - val_acc: 0.5533\n",
      "Epoch 987/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.5809 - acc: 0.8229 - val_loss: 1.7480 - val_acc: 0.5533\n",
      "Epoch 988/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.5798 - acc: 0.8229 - val_loss: 1.7279 - val_acc: 0.5500\n",
      "Epoch 989/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5776 - acc: 0.8200 - val_loss: 1.7436 - val_acc: 0.5567\n",
      "Epoch 990/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5792 - acc: 0.8257 - val_loss: 1.7340 - val_acc: 0.5500\n",
      "Epoch 991/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 0.5794 - acc: 0.8257 - val_loss: 1.7383 - val_acc: 0.5533\n",
      "Epoch 992/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.5786 - acc: 0.8286 - val_loss: 1.7378 - val_acc: 0.5533\n",
      "Epoch 993/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5796 - acc: 0.8243 - val_loss: 1.7302 - val_acc: 0.5467\n",
      "Epoch 994/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5798 - acc: 0.8229 - val_loss: 1.7347 - val_acc: 0.5500\n",
      "Epoch 995/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.5787 - acc: 0.8271 - val_loss: 1.7333 - val_acc: 0.5500\n",
      "Epoch 996/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5774 - acc: 0.8214 - val_loss: 1.7468 - val_acc: 0.5567\n",
      "Epoch 997/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.5794 - acc: 0.8243 - val_loss: 1.7402 - val_acc: 0.5500\n",
      "Epoch 998/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.5780 - acc: 0.8229 - val_loss: 1.7493 - val_acc: 0.5500\n",
      "Epoch 999/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5782 - acc: 0.8257 - val_loss: 1.7412 - val_acc: 0.5533\n",
      "Epoch 1000/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.5780 - acc: 0.8243 - val_loss: 1.7355 - val_acc: 0.5500\n",
      "Epoch 1001/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.5775 - acc: 0.8243 - val_loss: 1.7333 - val_acc: 0.5500\n",
      "Epoch 1002/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.5769 - acc: 0.8300 - val_loss: 1.7529 - val_acc: 0.5500\n",
      "Epoch 1003/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.5772 - acc: 0.8257 - val_loss: 1.7519 - val_acc: 0.5500\n",
      "Epoch 1004/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.5771 - acc: 0.8257 - val_loss: 1.7568 - val_acc: 0.5533\n",
      "Epoch 1005/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 124us/step - loss: 0.5766 - acc: 0.8257 - val_loss: 1.7457 - val_acc: 0.5567\n",
      "Epoch 1006/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.5758 - acc: 0.8243 - val_loss: 1.7549 - val_acc: 0.5500\n",
      "Epoch 1007/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.5756 - acc: 0.8257 - val_loss: 1.7593 - val_acc: 0.5500\n",
      "Epoch 1008/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.5761 - acc: 0.8271 - val_loss: 1.7471 - val_acc: 0.5533\n",
      "Epoch 1009/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.5754 - acc: 0.8257 - val_loss: 1.7489 - val_acc: 0.5500\n",
      "Epoch 1010/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5754 - acc: 0.8243 - val_loss: 1.7589 - val_acc: 0.5500\n",
      "Epoch 1011/3000\n",
      "700/700 [==============================] - 0s 130us/step - loss: 0.5756 - acc: 0.8257 - val_loss: 1.7431 - val_acc: 0.5467\n",
      "Epoch 1012/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.5752 - acc: 0.8257 - val_loss: 1.7556 - val_acc: 0.5500\n",
      "Epoch 1013/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.5759 - acc: 0.8243 - val_loss: 1.7580 - val_acc: 0.5533\n",
      "Epoch 1014/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 0.5744 - acc: 0.8271 - val_loss: 1.7545 - val_acc: 0.5500\n",
      "Epoch 1015/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5728 - acc: 0.8271 - val_loss: 1.7707 - val_acc: 0.5500\n",
      "Epoch 1016/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 0.5745 - acc: 0.8271 - val_loss: 1.7664 - val_acc: 0.5533\n",
      "Epoch 1017/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 0.5744 - acc: 0.8286 - val_loss: 1.7505 - val_acc: 0.5433\n",
      "Epoch 1018/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.5750 - acc: 0.8271 - val_loss: 1.7564 - val_acc: 0.5467\n",
      "Epoch 1019/3000\n",
      "700/700 [==============================] - 0s 130us/step - loss: 0.5735 - acc: 0.8271 - val_loss: 1.7666 - val_acc: 0.5533\n",
      "Epoch 1020/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5734 - acc: 0.8271 - val_loss: 1.7547 - val_acc: 0.5500\n",
      "Epoch 1021/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 0.5721 - acc: 0.8286 - val_loss: 1.7666 - val_acc: 0.5500\n",
      "Epoch 1022/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5725 - acc: 0.8257 - val_loss: 1.7470 - val_acc: 0.5500\n",
      "Epoch 1023/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5734 - acc: 0.8271 - val_loss: 1.7591 - val_acc: 0.5467\n",
      "Epoch 1024/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.5724 - acc: 0.8257 - val_loss: 1.7574 - val_acc: 0.5500\n",
      "Epoch 1025/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.5715 - acc: 0.8271 - val_loss: 1.7565 - val_acc: 0.5533\n",
      "Epoch 1026/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.5720 - acc: 0.8257 - val_loss: 1.7537 - val_acc: 0.5533\n",
      "Epoch 1027/3000\n",
      "700/700 [==============================] - 0s 130us/step - loss: 0.5722 - acc: 0.8257 - val_loss: 1.7490 - val_acc: 0.5467\n",
      "Epoch 1028/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5716 - acc: 0.8257 - val_loss: 1.7519 - val_acc: 0.5433\n",
      "Epoch 1029/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.5725 - acc: 0.8243 - val_loss: 1.7608 - val_acc: 0.5467\n",
      "Epoch 1030/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.5722 - acc: 0.8271 - val_loss: 1.7662 - val_acc: 0.5567\n",
      "Epoch 1031/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 0.5729 - acc: 0.8243 - val_loss: 1.7688 - val_acc: 0.5500\n",
      "Epoch 1032/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 0.5694 - acc: 0.8271 - val_loss: 1.7658 - val_acc: 0.5533\n",
      "Epoch 1033/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.5708 - acc: 0.8286 - val_loss: 1.7615 - val_acc: 0.5567\n",
      "Epoch 1034/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5693 - acc: 0.8257 - val_loss: 1.7712 - val_acc: 0.5533\n",
      "Epoch 1035/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 0.5720 - acc: 0.8271 - val_loss: 1.7694 - val_acc: 0.5600\n",
      "Epoch 1036/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5711 - acc: 0.8314 - val_loss: 1.7642 - val_acc: 0.5500\n",
      "Epoch 1037/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5697 - acc: 0.8300 - val_loss: 1.7714 - val_acc: 0.5500\n",
      "Epoch 1038/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 0.5692 - acc: 0.8286 - val_loss: 1.7799 - val_acc: 0.5533\n",
      "Epoch 1039/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5705 - acc: 0.8271 - val_loss: 1.7603 - val_acc: 0.5433\n",
      "Epoch 1040/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5701 - acc: 0.8271 - val_loss: 1.7719 - val_acc: 0.5533\n",
      "Epoch 1041/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.5696 - acc: 0.8286 - val_loss: 1.7753 - val_acc: 0.5567\n",
      "Epoch 1042/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.5680 - acc: 0.8300 - val_loss: 1.7756 - val_acc: 0.5500\n",
      "Epoch 1043/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.5682 - acc: 0.8257 - val_loss: 1.7649 - val_acc: 0.5433\n",
      "Epoch 1044/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.5696 - acc: 0.8243 - val_loss: 1.7667 - val_acc: 0.5500\n",
      "Epoch 1045/3000\n",
      "700/700 [==============================] - 0s 130us/step - loss: 0.5686 - acc: 0.8286 - val_loss: 1.7847 - val_acc: 0.5500\n",
      "Epoch 1046/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 0.5674 - acc: 0.8271 - val_loss: 1.7819 - val_acc: 0.5500\n",
      "Epoch 1047/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5679 - acc: 0.8271 - val_loss: 1.7817 - val_acc: 0.5500\n",
      "Epoch 1048/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 0.5673 - acc: 0.8257 - val_loss: 1.7723 - val_acc: 0.5400\n",
      "Epoch 1049/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.5667 - acc: 0.8286 - val_loss: 1.7867 - val_acc: 0.5567\n",
      "Epoch 1050/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.5693 - acc: 0.8286 - val_loss: 1.7767 - val_acc: 0.5467\n",
      "Epoch 1051/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5694 - acc: 0.8286 - val_loss: 1.7803 - val_acc: 0.5533\n",
      "Epoch 1052/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.5676 - acc: 0.8286 - val_loss: 1.7711 - val_acc: 0.5500\n",
      "Epoch 1053/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.5660 - acc: 0.8271 - val_loss: 1.7799 - val_acc: 0.5500\n",
      "Epoch 1054/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5672 - acc: 0.8286 - val_loss: 1.7747 - val_acc: 0.5533\n",
      "Epoch 1055/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.5665 - acc: 0.8271 - val_loss: 1.7895 - val_acc: 0.5500\n",
      "Epoch 1056/3000\n",
      "700/700 [==============================] - 0s 141us/step - loss: 0.5671 - acc: 0.8286 - val_loss: 1.7842 - val_acc: 0.5533\n",
      "Epoch 1057/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5661 - acc: 0.8257 - val_loss: 1.7730 - val_acc: 0.5500\n",
      "Epoch 1058/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 0.5669 - acc: 0.8286 - val_loss: 1.7809 - val_acc: 0.5533\n",
      "Epoch 1059/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.5661 - acc: 0.8286 - val_loss: 1.7784 - val_acc: 0.5533\n",
      "Epoch 1060/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.5669 - acc: 0.8286 - val_loss: 1.7792 - val_acc: 0.5533\n",
      "Epoch 1061/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5648 - acc: 0.8300 - val_loss: 1.7725 - val_acc: 0.5533\n",
      "Epoch 1062/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 0.5654 - acc: 0.8286 - val_loss: 1.7914 - val_acc: 0.5533\n",
      "Epoch 1063/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5661 - acc: 0.8271 - val_loss: 1.7852 - val_acc: 0.5533\n",
      "Epoch 1064/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 131us/step - loss: 0.5655 - acc: 0.8314 - val_loss: 1.7816 - val_acc: 0.5533\n",
      "Epoch 1065/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.5651 - acc: 0.8286 - val_loss: 1.7880 - val_acc: 0.5500\n",
      "Epoch 1066/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 0.5662 - acc: 0.8286 - val_loss: 1.7919 - val_acc: 0.5500\n",
      "Epoch 1067/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.5638 - acc: 0.8286 - val_loss: 1.7940 - val_acc: 0.5567\n",
      "Epoch 1068/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 0.5634 - acc: 0.8300 - val_loss: 1.7823 - val_acc: 0.5533\n",
      "Epoch 1069/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.5638 - acc: 0.8271 - val_loss: 1.7929 - val_acc: 0.5567\n",
      "Epoch 1070/3000\n",
      "700/700 [==============================] - 0s 141us/step - loss: 0.5636 - acc: 0.8286 - val_loss: 1.7878 - val_acc: 0.5500\n",
      "Epoch 1071/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5630 - acc: 0.8300 - val_loss: 1.7944 - val_acc: 0.5500\n",
      "Epoch 1072/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5650 - acc: 0.8271 - val_loss: 1.7921 - val_acc: 0.5533\n",
      "Epoch 1073/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5639 - acc: 0.8286 - val_loss: 1.7961 - val_acc: 0.5533\n",
      "Epoch 1074/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5622 - acc: 0.8300 - val_loss: 1.8029 - val_acc: 0.5533\n",
      "Epoch 1075/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.5639 - acc: 0.8286 - val_loss: 1.7881 - val_acc: 0.5533\n",
      "Epoch 1076/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.5631 - acc: 0.8300 - val_loss: 1.7823 - val_acc: 0.5567\n",
      "Epoch 1077/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5619 - acc: 0.8243 - val_loss: 1.7993 - val_acc: 0.5567\n",
      "Epoch 1078/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5612 - acc: 0.8300 - val_loss: 1.8055 - val_acc: 0.5533\n",
      "Epoch 1079/3000\n",
      "700/700 [==============================] - 0s 141us/step - loss: 0.5620 - acc: 0.8300 - val_loss: 1.7916 - val_acc: 0.5567\n",
      "Epoch 1080/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 0.5617 - acc: 0.8300 - val_loss: 1.8081 - val_acc: 0.5533\n",
      "Epoch 1081/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5629 - acc: 0.8271 - val_loss: 1.7947 - val_acc: 0.5500\n",
      "Epoch 1082/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.5620 - acc: 0.8314 - val_loss: 1.7914 - val_acc: 0.5500\n",
      "Epoch 1083/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5605 - acc: 0.8300 - val_loss: 1.7847 - val_acc: 0.5467\n",
      "Epoch 1084/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5618 - acc: 0.8314 - val_loss: 1.7937 - val_acc: 0.5567\n",
      "Epoch 1085/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.5613 - acc: 0.8286 - val_loss: 1.7925 - val_acc: 0.5533\n",
      "Epoch 1086/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 0.5616 - acc: 0.8286 - val_loss: 1.7938 - val_acc: 0.5467\n",
      "Epoch 1087/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5603 - acc: 0.8300 - val_loss: 1.8051 - val_acc: 0.5533\n",
      "Epoch 1088/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5624 - acc: 0.8329 - val_loss: 1.7994 - val_acc: 0.5500\n",
      "Epoch 1089/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.5607 - acc: 0.8271 - val_loss: 1.8043 - val_acc: 0.5533\n",
      "Epoch 1090/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.5597 - acc: 0.8286 - val_loss: 1.8000 - val_acc: 0.5500\n",
      "Epoch 1091/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 0.5590 - acc: 0.8314 - val_loss: 1.8117 - val_acc: 0.5500\n",
      "Epoch 1092/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 0.5607 - acc: 0.8286 - val_loss: 1.8074 - val_acc: 0.5533\n",
      "Epoch 1093/3000\n",
      "700/700 [==============================] - 0s 130us/step - loss: 0.5596 - acc: 0.8286 - val_loss: 1.8043 - val_acc: 0.5600\n",
      "Epoch 1094/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.5596 - acc: 0.8300 - val_loss: 1.8175 - val_acc: 0.5533\n",
      "Epoch 1095/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 0.5590 - acc: 0.8286 - val_loss: 1.8129 - val_acc: 0.5533\n",
      "Epoch 1096/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.5599 - acc: 0.8286 - val_loss: 1.8058 - val_acc: 0.5633\n",
      "Epoch 1097/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 0.5600 - acc: 0.8300 - val_loss: 1.8112 - val_acc: 0.5533\n",
      "Epoch 1098/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.5587 - acc: 0.8300 - val_loss: 1.8098 - val_acc: 0.5600\n",
      "Epoch 1099/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5575 - acc: 0.8271 - val_loss: 1.8089 - val_acc: 0.5567\n",
      "Epoch 1100/3000\n",
      "700/700 [==============================] - 0s 140us/step - loss: 0.5572 - acc: 0.8300 - val_loss: 1.8180 - val_acc: 0.5533\n",
      "Epoch 1101/3000\n",
      "700/700 [==============================] - 0s 141us/step - loss: 0.5580 - acc: 0.8300 - val_loss: 1.8039 - val_acc: 0.5567\n",
      "Epoch 1102/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.5576 - acc: 0.8286 - val_loss: 1.8115 - val_acc: 0.5567\n",
      "Epoch 1103/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5586 - acc: 0.8286 - val_loss: 1.8028 - val_acc: 0.5533\n",
      "Epoch 1104/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.5586 - acc: 0.8271 - val_loss: 1.8167 - val_acc: 0.5533\n",
      "Epoch 1105/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 0.5572 - acc: 0.8300 - val_loss: 1.8256 - val_acc: 0.5500\n",
      "Epoch 1106/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.5573 - acc: 0.8314 - val_loss: 1.8101 - val_acc: 0.5467\n",
      "Epoch 1107/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.5566 - acc: 0.8286 - val_loss: 1.8071 - val_acc: 0.5467\n",
      "Epoch 1108/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 0.5563 - acc: 0.8286 - val_loss: 1.7992 - val_acc: 0.5433\n",
      "Epoch 1109/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 0.5562 - acc: 0.8300 - val_loss: 1.8035 - val_acc: 0.5433\n",
      "Epoch 1110/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.5561 - acc: 0.8300 - val_loss: 1.8145 - val_acc: 0.5567\n",
      "Epoch 1111/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5569 - acc: 0.8314 - val_loss: 1.8208 - val_acc: 0.5600\n",
      "Epoch 1112/3000\n",
      "700/700 [==============================] - 0s 131us/step - loss: 0.5563 - acc: 0.8300 - val_loss: 1.8212 - val_acc: 0.5567\n",
      "Epoch 1113/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 0.5568 - acc: 0.8300 - val_loss: 1.8111 - val_acc: 0.5567\n",
      "Epoch 1114/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5561 - acc: 0.8286 - val_loss: 1.8114 - val_acc: 0.5533\n",
      "Epoch 1115/3000\n",
      "700/700 [==============================] - 0s 141us/step - loss: 0.5561 - acc: 0.8286 - val_loss: 1.8089 - val_acc: 0.5533\n",
      "Epoch 1116/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5548 - acc: 0.8300 - val_loss: 1.8138 - val_acc: 0.5567\n",
      "Epoch 1117/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5550 - acc: 0.8286 - val_loss: 1.8138 - val_acc: 0.5533\n",
      "Epoch 1118/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.5555 - acc: 0.8300 - val_loss: 1.8254 - val_acc: 0.5533\n",
      "Epoch 1119/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 0.5560 - acc: 0.8343 - val_loss: 1.8187 - val_acc: 0.5500\n",
      "Epoch 1120/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.5545 - acc: 0.8286 - val_loss: 1.8301 - val_acc: 0.5533\n",
      "Epoch 1121/3000\n",
      "700/700 [==============================] - 0s 140us/step - loss: 0.5534 - acc: 0.8286 - val_loss: 1.8242 - val_acc: 0.5533\n",
      "Epoch 1122/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 0.5544 - acc: 0.8314 - val_loss: 1.8145 - val_acc: 0.5567\n",
      "Epoch 1123/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 136us/step - loss: 0.5544 - acc: 0.8271 - val_loss: 1.8159 - val_acc: 0.5467\n",
      "Epoch 1124/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.5527 - acc: 0.8300 - val_loss: 1.8436 - val_acc: 0.5467\n",
      "Epoch 1125/3000\n",
      "700/700 [==============================] - 0s 141us/step - loss: 0.5535 - acc: 0.8314 - val_loss: 1.8261 - val_acc: 0.5567\n",
      "Epoch 1126/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 0.5541 - acc: 0.8300 - val_loss: 1.8246 - val_acc: 0.5533\n",
      "Epoch 1127/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 0.5528 - acc: 0.8286 - val_loss: 1.8337 - val_acc: 0.5533\n",
      "Epoch 1128/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 0.5554 - acc: 0.8286 - val_loss: 1.8235 - val_acc: 0.5567\n",
      "Epoch 1129/3000\n",
      "700/700 [==============================] - 0s 140us/step - loss: 0.5529 - acc: 0.8286 - val_loss: 1.8286 - val_acc: 0.5567\n",
      "Epoch 1130/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.5517 - acc: 0.8300 - val_loss: 1.8260 - val_acc: 0.5633\n",
      "Epoch 1131/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.5519 - acc: 0.8314 - val_loss: 1.8264 - val_acc: 0.5567\n",
      "Epoch 1132/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.5521 - acc: 0.8314 - val_loss: 1.8247 - val_acc: 0.5567\n",
      "Epoch 1133/3000\n",
      "700/700 [==============================] - 0s 141us/step - loss: 0.5522 - acc: 0.8314 - val_loss: 1.8195 - val_acc: 0.5567\n",
      "Epoch 1134/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 0.5526 - acc: 0.8300 - val_loss: 1.8198 - val_acc: 0.5567\n",
      "Epoch 1135/3000\n",
      "700/700 [==============================] - 0s 140us/step - loss: 0.5518 - acc: 0.8314 - val_loss: 1.8419 - val_acc: 0.5533\n",
      "Epoch 1136/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.5515 - acc: 0.8300 - val_loss: 1.8363 - val_acc: 0.5567\n",
      "Epoch 1137/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.5524 - acc: 0.8300 - val_loss: 1.8300 - val_acc: 0.5567\n",
      "Epoch 1138/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.5513 - acc: 0.8300 - val_loss: 1.8272 - val_acc: 0.5567\n",
      "Epoch 1139/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.5508 - acc: 0.8300 - val_loss: 1.8478 - val_acc: 0.5500\n",
      "Epoch 1140/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5504 - acc: 0.8286 - val_loss: 1.8282 - val_acc: 0.5533\n",
      "Epoch 1141/3000\n",
      "700/700 [==============================] - 0s 142us/step - loss: 0.5504 - acc: 0.8300 - val_loss: 1.8446 - val_acc: 0.5533\n",
      "Epoch 1142/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.5503 - acc: 0.8329 - val_loss: 1.8288 - val_acc: 0.5567\n",
      "Epoch 1143/3000\n",
      "700/700 [==============================] - 0s 140us/step - loss: 0.5510 - acc: 0.8300 - val_loss: 1.8256 - val_acc: 0.5567\n",
      "Epoch 1144/3000\n",
      "700/700 [==============================] - 0s 141us/step - loss: 0.5509 - acc: 0.8300 - val_loss: 1.8304 - val_acc: 0.5567\n",
      "Epoch 1145/3000\n",
      "700/700 [==============================] - 0s 142us/step - loss: 0.5488 - acc: 0.8329 - val_loss: 1.8275 - val_acc: 0.5567\n",
      "Epoch 1146/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5497 - acc: 0.8300 - val_loss: 1.8309 - val_acc: 0.5533\n",
      "Epoch 1147/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 0.5502 - acc: 0.8314 - val_loss: 1.8295 - val_acc: 0.5567\n",
      "Epoch 1148/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.5497 - acc: 0.8329 - val_loss: 1.8264 - val_acc: 0.5533\n",
      "Epoch 1149/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 0.5501 - acc: 0.8300 - val_loss: 1.8335 - val_acc: 0.5600\n",
      "Epoch 1150/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.5496 - acc: 0.8300 - val_loss: 1.8379 - val_acc: 0.5567\n",
      "Epoch 1151/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.5481 - acc: 0.8300 - val_loss: 1.8302 - val_acc: 0.5600\n",
      "Epoch 1152/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.5475 - acc: 0.8300 - val_loss: 1.8493 - val_acc: 0.5567\n",
      "Epoch 1153/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.5503 - acc: 0.8300 - val_loss: 1.8328 - val_acc: 0.5633\n",
      "Epoch 1154/3000\n",
      "700/700 [==============================] - 0s 132us/step - loss: 0.5487 - acc: 0.8300 - val_loss: 1.8357 - val_acc: 0.5567\n",
      "Epoch 1155/3000\n",
      "700/700 [==============================] - 0s 139us/step - loss: 0.5490 - acc: 0.8314 - val_loss: 1.8408 - val_acc: 0.5567\n",
      "Epoch 1156/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 0.5480 - acc: 0.8329 - val_loss: 1.8343 - val_acc: 0.5533\n",
      "Epoch 1157/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.5495 - acc: 0.8314 - val_loss: 1.8338 - val_acc: 0.5567\n",
      "Epoch 1158/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.5463 - acc: 0.8300 - val_loss: 1.8348 - val_acc: 0.5567\n",
      "Epoch 1159/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.5482 - acc: 0.8314 - val_loss: 1.8344 - val_acc: 0.5567\n",
      "Epoch 1160/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.5472 - acc: 0.8329 - val_loss: 1.8496 - val_acc: 0.5567\n",
      "Epoch 1161/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.5474 - acc: 0.8314 - val_loss: 1.8413 - val_acc: 0.5533\n",
      "Epoch 1162/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 0.5474 - acc: 0.8329 - val_loss: 1.8628 - val_acc: 0.5533\n",
      "Epoch 1163/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 0.5473 - acc: 0.8271 - val_loss: 1.8357 - val_acc: 0.5500\n",
      "Epoch 1164/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.5480 - acc: 0.8314 - val_loss: 1.8459 - val_acc: 0.5533\n",
      "Epoch 1165/3000\n",
      "700/700 [==============================] - 0s 142us/step - loss: 0.5483 - acc: 0.8314 - val_loss: 1.8376 - val_acc: 0.5567\n",
      "Epoch 1166/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.5484 - acc: 0.8314 - val_loss: 1.8436 - val_acc: 0.5600\n",
      "Epoch 1167/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.5462 - acc: 0.8300 - val_loss: 1.8499 - val_acc: 0.5567\n",
      "Epoch 1168/3000\n",
      "700/700 [==============================] - 0s 135us/step - loss: 0.5463 - acc: 0.8329 - val_loss: 1.8465 - val_acc: 0.5567\n",
      "Epoch 1169/3000\n",
      "700/700 [==============================] - 0s 140us/step - loss: 0.5468 - acc: 0.8314 - val_loss: 1.8397 - val_acc: 0.5567\n",
      "Epoch 1170/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 0.5468 - acc: 0.8314 - val_loss: 1.8547 - val_acc: 0.5633\n",
      "Epoch 1171/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.5448 - acc: 0.8314 - val_loss: 1.8400 - val_acc: 0.5567\n",
      "Epoch 1172/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 0.5476 - acc: 0.8314 - val_loss: 1.8421 - val_acc: 0.5600\n",
      "Epoch 1173/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 0.5450 - acc: 0.8329 - val_loss: 1.8344 - val_acc: 0.5433\n",
      "Epoch 1174/3000\n",
      "700/700 [==============================] - 0s 142us/step - loss: 0.5467 - acc: 0.8314 - val_loss: 1.8598 - val_acc: 0.5567\n",
      "Epoch 1175/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.5449 - acc: 0.8314 - val_loss: 1.8532 - val_acc: 0.5533\n",
      "Epoch 1176/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.5452 - acc: 0.8329 - val_loss: 1.8438 - val_acc: 0.5533\n",
      "Epoch 1177/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.5455 - acc: 0.8329 - val_loss: 1.8460 - val_acc: 0.5533\n",
      "Epoch 1178/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.5451 - acc: 0.8300 - val_loss: 1.8458 - val_acc: 0.5533\n",
      "Epoch 1179/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 0.5452 - acc: 0.8300 - val_loss: 1.8527 - val_acc: 0.5567\n",
      "Epoch 1180/3000\n",
      "700/700 [==============================] - 0s 142us/step - loss: 0.5441 - acc: 0.8314 - val_loss: 1.8490 - val_acc: 0.5600\n",
      "Epoch 1181/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.5441 - acc: 0.8314 - val_loss: 1.8689 - val_acc: 0.5600\n",
      "Epoch 1182/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 160us/step - loss: 0.5429 - acc: 0.8343 - val_loss: 1.8547 - val_acc: 0.5600\n",
      "Epoch 1183/3000\n",
      "700/700 [==============================] - 0s 140us/step - loss: 0.5450 - acc: 0.8314 - val_loss: 1.8553 - val_acc: 0.5500\n",
      "Epoch 1184/3000\n",
      "700/700 [==============================] - 0s 136us/step - loss: 0.5442 - acc: 0.8314 - val_loss: 1.8542 - val_acc: 0.5533\n",
      "Epoch 1185/3000\n",
      "700/700 [==============================] - 0s 140us/step - loss: 0.5436 - acc: 0.8329 - val_loss: 1.8629 - val_acc: 0.5567\n",
      "Epoch 1186/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.5444 - acc: 0.8314 - val_loss: 1.8512 - val_acc: 0.5533\n",
      "Epoch 1187/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.5434 - acc: 0.8329 - val_loss: 1.8737 - val_acc: 0.5567\n",
      "Epoch 1188/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.5438 - acc: 0.8314 - val_loss: 1.8639 - val_acc: 0.5567\n",
      "Epoch 1189/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.5443 - acc: 0.8300 - val_loss: 1.8554 - val_acc: 0.5500\n",
      "Epoch 1190/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.5424 - acc: 0.8300 - val_loss: 1.8641 - val_acc: 0.5567\n",
      "Epoch 1191/3000\n",
      "700/700 [==============================] - 0s 141us/step - loss: 0.5427 - acc: 0.8329 - val_loss: 1.8680 - val_acc: 0.5567\n",
      "Epoch 1192/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.5435 - acc: 0.8286 - val_loss: 1.8623 - val_acc: 0.5567\n",
      "Epoch 1193/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 0.5416 - acc: 0.8329 - val_loss: 1.8577 - val_acc: 0.5633\n",
      "Epoch 1194/3000\n",
      "700/700 [==============================] - 0s 142us/step - loss: 0.5428 - acc: 0.8314 - val_loss: 1.8553 - val_acc: 0.5567\n",
      "Epoch 1195/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.5399 - acc: 0.8314 - val_loss: 1.8661 - val_acc: 0.5500\n",
      "Epoch 1196/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.5432 - acc: 0.8343 - val_loss: 1.8588 - val_acc: 0.5567\n",
      "Epoch 1197/3000\n",
      "700/700 [==============================] - 0s 138us/step - loss: 0.5429 - acc: 0.8314 - val_loss: 1.8671 - val_acc: 0.5567\n",
      "Epoch 1198/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.5411 - acc: 0.8329 - val_loss: 1.8743 - val_acc: 0.5533\n",
      "Epoch 1199/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.5409 - acc: 0.8329 - val_loss: 1.8681 - val_acc: 0.5633\n",
      "Epoch 1200/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.5418 - acc: 0.8329 - val_loss: 1.8610 - val_acc: 0.5467\n",
      "Epoch 1201/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.5411 - acc: 0.8300 - val_loss: 1.8610 - val_acc: 0.5567\n",
      "Epoch 1202/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.5411 - acc: 0.8314 - val_loss: 1.8633 - val_acc: 0.5567\n",
      "Epoch 1203/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.5425 - acc: 0.8314 - val_loss: 1.8640 - val_acc: 0.5567\n",
      "Epoch 1204/3000\n",
      "700/700 [==============================] - 0s 140us/step - loss: 0.5414 - acc: 0.8329 - val_loss: 1.8677 - val_acc: 0.5500\n",
      "Epoch 1205/3000\n",
      "700/700 [==============================] - 0s 141us/step - loss: 0.5407 - acc: 0.8343 - val_loss: 1.8671 - val_acc: 0.5500\n",
      "Epoch 1206/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.5395 - acc: 0.8329 - val_loss: 1.8774 - val_acc: 0.5567\n",
      "Epoch 1207/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.5398 - acc: 0.8343 - val_loss: 1.8759 - val_acc: 0.5600\n",
      "Epoch 1208/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.5409 - acc: 0.8314 - val_loss: 1.8617 - val_acc: 0.5567\n",
      "Epoch 1209/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.5403 - acc: 0.8314 - val_loss: 1.8670 - val_acc: 0.5533\n",
      "Epoch 1210/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.5412 - acc: 0.8314 - val_loss: 1.8713 - val_acc: 0.5500\n",
      "Epoch 1211/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.5404 - acc: 0.8343 - val_loss: 1.8758 - val_acc: 0.5533\n",
      "Epoch 1212/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.5389 - acc: 0.8329 - val_loss: 1.8650 - val_acc: 0.5567\n",
      "Epoch 1213/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.5382 - acc: 0.8329 - val_loss: 1.8688 - val_acc: 0.5533\n",
      "Epoch 1214/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.5396 - acc: 0.8314 - val_loss: 1.8700 - val_acc: 0.5500\n",
      "Epoch 1215/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.5394 - acc: 0.8357 - val_loss: 1.8818 - val_acc: 0.5600\n",
      "Epoch 1216/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.5393 - acc: 0.8329 - val_loss: 1.8727 - val_acc: 0.5500\n",
      "Epoch 1217/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.5387 - acc: 0.8329 - val_loss: 1.8680 - val_acc: 0.5567\n",
      "Epoch 1218/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.5385 - acc: 0.8300 - val_loss: 1.8786 - val_acc: 0.5533\n",
      "Epoch 1219/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.5387 - acc: 0.8343 - val_loss: 1.8756 - val_acc: 0.5600\n",
      "Epoch 1220/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.5378 - acc: 0.8329 - val_loss: 1.8754 - val_acc: 0.5567\n",
      "Epoch 1221/3000\n",
      "700/700 [==============================] - 0s 228us/step - loss: 0.5382 - acc: 0.8329 - val_loss: 1.8792 - val_acc: 0.5600\n",
      "Epoch 1222/3000\n",
      "700/700 [==============================] - 0s 184us/step - loss: 0.5395 - acc: 0.8329 - val_loss: 1.8828 - val_acc: 0.5533\n",
      "Epoch 1223/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.5382 - acc: 0.8343 - val_loss: 1.8802 - val_acc: 0.5533\n",
      "Epoch 1224/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.5374 - acc: 0.8329 - val_loss: 1.8808 - val_acc: 0.5600\n",
      "Epoch 1225/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.5372 - acc: 0.8343 - val_loss: 1.8862 - val_acc: 0.5567\n",
      "Epoch 1226/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.5380 - acc: 0.8329 - val_loss: 1.8819 - val_acc: 0.5533\n",
      "Epoch 1227/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.5371 - acc: 0.8329 - val_loss: 1.8734 - val_acc: 0.5567\n",
      "Epoch 1228/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.5370 - acc: 0.8329 - val_loss: 1.8826 - val_acc: 0.5533\n",
      "Epoch 1229/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.5348 - acc: 0.8329 - val_loss: 1.8759 - val_acc: 0.5400\n",
      "Epoch 1230/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.5364 - acc: 0.8329 - val_loss: 1.8840 - val_acc: 0.5533\n",
      "Epoch 1231/3000\n",
      "700/700 [==============================] - 0s 184us/step - loss: 0.5355 - acc: 0.8343 - val_loss: 1.8831 - val_acc: 0.5500\n",
      "Epoch 1232/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.5363 - acc: 0.8329 - val_loss: 1.8896 - val_acc: 0.5633\n",
      "Epoch 1233/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.5363 - acc: 0.8343 - val_loss: 1.8773 - val_acc: 0.5533\n",
      "Epoch 1234/3000\n",
      "700/700 [==============================] - 0s 191us/step - loss: 0.5370 - acc: 0.8343 - val_loss: 1.8880 - val_acc: 0.5567\n",
      "Epoch 1235/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.5368 - acc: 0.8300 - val_loss: 1.8823 - val_acc: 0.5533\n",
      "Epoch 1236/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.5363 - acc: 0.8343 - val_loss: 1.8901 - val_acc: 0.5633\n",
      "Epoch 1237/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.5360 - acc: 0.8300 - val_loss: 1.8789 - val_acc: 0.5567\n",
      "Epoch 1238/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.5385 - acc: 0.8286 - val_loss: 1.8806 - val_acc: 0.5533\n",
      "Epoch 1239/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.5359 - acc: 0.8314 - val_loss: 1.8855 - val_acc: 0.5533\n",
      "Epoch 1240/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.5349 - acc: 0.8329 - val_loss: 1.8822 - val_acc: 0.5533\n",
      "Epoch 1241/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 187us/step - loss: 0.5366 - acc: 0.8314 - val_loss: 1.8901 - val_acc: 0.5533\n",
      "Epoch 1242/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.5346 - acc: 0.8314 - val_loss: 1.8792 - val_acc: 0.5500\n",
      "Epoch 1243/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.5354 - acc: 0.8329 - val_loss: 1.8817 - val_acc: 0.5533\n",
      "Epoch 1244/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.5343 - acc: 0.8314 - val_loss: 1.8952 - val_acc: 0.5567\n",
      "Epoch 1245/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.5352 - acc: 0.8343 - val_loss: 1.9018 - val_acc: 0.5600\n",
      "Epoch 1246/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.5339 - acc: 0.8329 - val_loss: 1.8798 - val_acc: 0.5533\n",
      "Epoch 1247/3000\n",
      "700/700 [==============================] - 0s 179us/step - loss: 0.5340 - acc: 0.8329 - val_loss: 1.8806 - val_acc: 0.5567\n",
      "Epoch 1248/3000\n",
      "700/700 [==============================] - 0s 178us/step - loss: 0.5339 - acc: 0.8314 - val_loss: 1.8846 - val_acc: 0.5533\n",
      "Epoch 1249/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.5346 - acc: 0.8343 - val_loss: 1.8878 - val_acc: 0.5533\n",
      "Epoch 1250/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.5326 - acc: 0.8343 - val_loss: 1.8971 - val_acc: 0.5567\n",
      "Epoch 1251/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.5343 - acc: 0.8343 - val_loss: 1.8900 - val_acc: 0.5533\n",
      "Epoch 1252/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.5329 - acc: 0.8314 - val_loss: 1.8912 - val_acc: 0.5500\n",
      "Epoch 1253/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.5326 - acc: 0.8343 - val_loss: 1.8904 - val_acc: 0.5533\n",
      "Epoch 1254/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.5338 - acc: 0.8343 - val_loss: 1.8985 - val_acc: 0.5533\n",
      "Epoch 1255/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.5334 - acc: 0.8329 - val_loss: 1.8902 - val_acc: 0.5567\n",
      "Epoch 1256/3000\n",
      "700/700 [==============================] - 0s 187us/step - loss: 0.5329 - acc: 0.8343 - val_loss: 1.8947 - val_acc: 0.5533\n",
      "Epoch 1257/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.5324 - acc: 0.8329 - val_loss: 1.9003 - val_acc: 0.5567\n",
      "Epoch 1258/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.5332 - acc: 0.8329 - val_loss: 1.8984 - val_acc: 0.5500\n",
      "Epoch 1259/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.5340 - acc: 0.8314 - val_loss: 1.8915 - val_acc: 0.5433\n",
      "Epoch 1260/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.5344 - acc: 0.8343 - val_loss: 1.9026 - val_acc: 0.5633\n",
      "Epoch 1261/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.5334 - acc: 0.8343 - val_loss: 1.9089 - val_acc: 0.5667\n",
      "Epoch 1262/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.5309 - acc: 0.8371 - val_loss: 1.8995 - val_acc: 0.5567\n",
      "Epoch 1263/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.5312 - acc: 0.8329 - val_loss: 1.9061 - val_acc: 0.5567\n",
      "Epoch 1264/3000\n",
      "700/700 [==============================] - 0s 179us/step - loss: 0.5320 - acc: 0.8314 - val_loss: 1.9021 - val_acc: 0.5567\n",
      "Epoch 1265/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.5306 - acc: 0.8357 - val_loss: 1.9015 - val_acc: 0.5500\n",
      "Epoch 1266/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.5311 - acc: 0.8357 - val_loss: 1.9032 - val_acc: 0.5467\n",
      "Epoch 1267/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.5317 - acc: 0.8371 - val_loss: 1.9039 - val_acc: 0.5533\n",
      "Epoch 1268/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.5297 - acc: 0.8343 - val_loss: 1.9217 - val_acc: 0.5633\n",
      "Epoch 1269/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.5307 - acc: 0.8371 - val_loss: 1.9058 - val_acc: 0.5533\n",
      "Epoch 1270/3000\n",
      "700/700 [==============================] - 0s 184us/step - loss: 0.5292 - acc: 0.8371 - val_loss: 1.9133 - val_acc: 0.5500\n",
      "Epoch 1271/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.5285 - acc: 0.8343 - val_loss: 1.8974 - val_acc: 0.5500\n",
      "Epoch 1272/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.5287 - acc: 0.8357 - val_loss: 1.9180 - val_acc: 0.5600\n",
      "Epoch 1273/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.5299 - acc: 0.8343 - val_loss: 1.9260 - val_acc: 0.5600\n",
      "Epoch 1274/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.5272 - acc: 0.8400 - val_loss: 1.9287 - val_acc: 0.5567\n",
      "Epoch 1275/3000\n",
      "700/700 [==============================] - 0s 190us/step - loss: 0.5294 - acc: 0.8371 - val_loss: 1.9109 - val_acc: 0.5533\n",
      "Epoch 1276/3000\n",
      "700/700 [==============================] - 0s 178us/step - loss: 0.5292 - acc: 0.8371 - val_loss: 1.9031 - val_acc: 0.5533\n",
      "Epoch 1277/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.5289 - acc: 0.8357 - val_loss: 1.9068 - val_acc: 0.5533\n",
      "Epoch 1278/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.5277 - acc: 0.8343 - val_loss: 1.9130 - val_acc: 0.5600\n",
      "Epoch 1279/3000\n",
      "700/700 [==============================] - 0s 183us/step - loss: 0.5291 - acc: 0.8371 - val_loss: 1.9073 - val_acc: 0.5567\n",
      "Epoch 1280/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.5282 - acc: 0.8343 - val_loss: 1.9173 - val_acc: 0.5600\n",
      "Epoch 1281/3000\n",
      "700/700 [==============================] - 0s 192us/step - loss: 0.5276 - acc: 0.8371 - val_loss: 1.9181 - val_acc: 0.5567\n",
      "Epoch 1282/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.5273 - acc: 0.8343 - val_loss: 1.9124 - val_acc: 0.5567\n",
      "Epoch 1283/3000\n",
      "700/700 [==============================] - 0s 176us/step - loss: 0.5276 - acc: 0.8357 - val_loss: 1.9215 - val_acc: 0.5567\n",
      "Epoch 1284/3000\n",
      "700/700 [==============================] - 0s 185us/step - loss: 0.5248 - acc: 0.8386 - val_loss: 1.9097 - val_acc: 0.5500\n",
      "Epoch 1285/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.5244 - acc: 0.8371 - val_loss: 1.9193 - val_acc: 0.5567\n",
      "Epoch 1286/3000\n",
      "700/700 [==============================] - 0s 185us/step - loss: 0.5262 - acc: 0.8371 - val_loss: 1.9422 - val_acc: 0.5633\n",
      "Epoch 1287/3000\n",
      "700/700 [==============================] - 0s 183us/step - loss: 0.5259 - acc: 0.8400 - val_loss: 1.9147 - val_acc: 0.5533\n",
      "Epoch 1288/3000\n",
      "700/700 [==============================] - 0s 178us/step - loss: 0.5256 - acc: 0.8400 - val_loss: 1.9174 - val_acc: 0.5600\n",
      "Epoch 1289/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.5252 - acc: 0.8343 - val_loss: 1.9284 - val_acc: 0.5567\n",
      "Epoch 1290/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.5229 - acc: 0.8386 - val_loss: 1.9371 - val_acc: 0.5600\n",
      "Epoch 1291/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.5227 - acc: 0.8357 - val_loss: 1.9207 - val_acc: 0.5467\n",
      "Epoch 1292/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.5229 - acc: 0.8371 - val_loss: 1.9241 - val_acc: 0.5567\n",
      "Epoch 1293/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.5239 - acc: 0.8400 - val_loss: 1.9195 - val_acc: 0.5500\n",
      "Epoch 1294/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.5228 - acc: 0.8371 - val_loss: 1.9301 - val_acc: 0.5533\n",
      "Epoch 1295/3000\n",
      "700/700 [==============================] - 0s 181us/step - loss: 0.5234 - acc: 0.8400 - val_loss: 1.9214 - val_acc: 0.5533\n",
      "Epoch 1296/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.5232 - acc: 0.8400 - val_loss: 1.9187 - val_acc: 0.5533\n",
      "Epoch 1297/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.5225 - acc: 0.8400 - val_loss: 1.9194 - val_acc: 0.5500\n",
      "Epoch 1298/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.5223 - acc: 0.8386 - val_loss: 1.9304 - val_acc: 0.5567\n",
      "Epoch 1299/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.5223 - acc: 0.8386 - val_loss: 1.9230 - val_acc: 0.5500\n",
      "Epoch 1300/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 181us/step - loss: 0.5211 - acc: 0.8386 - val_loss: 1.9250 - val_acc: 0.5533\n",
      "Epoch 1301/3000\n",
      "700/700 [==============================] - 0s 227us/step - loss: 0.5220 - acc: 0.8400 - val_loss: 1.9246 - val_acc: 0.5600\n",
      "Epoch 1302/3000\n",
      "700/700 [==============================] - 0s 239us/step - loss: 0.5205 - acc: 0.8371 - val_loss: 1.9255 - val_acc: 0.5567\n",
      "Epoch 1303/3000\n",
      "700/700 [==============================] - 0s 183us/step - loss: 0.5222 - acc: 0.8386 - val_loss: 1.9237 - val_acc: 0.5600\n",
      "Epoch 1304/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.5206 - acc: 0.8400 - val_loss: 1.9226 - val_acc: 0.5500\n",
      "Epoch 1305/3000\n",
      "700/700 [==============================] - 0s 222us/step - loss: 0.5205 - acc: 0.8371 - val_loss: 1.9430 - val_acc: 0.5600\n",
      "Epoch 1306/3000\n",
      "700/700 [==============================] - 0s 224us/step - loss: 0.5213 - acc: 0.8386 - val_loss: 1.9248 - val_acc: 0.5600\n",
      "Epoch 1307/3000\n",
      "700/700 [==============================] - 0s 205us/step - loss: 0.5209 - acc: 0.8400 - val_loss: 1.9332 - val_acc: 0.5633\n",
      "Epoch 1308/3000\n",
      "700/700 [==============================] - 0s 226us/step - loss: 0.5213 - acc: 0.8400 - val_loss: 1.9247 - val_acc: 0.5500\n",
      "Epoch 1309/3000\n",
      "700/700 [==============================] - 0s 215us/step - loss: 0.5222 - acc: 0.8371 - val_loss: 1.9285 - val_acc: 0.5533\n",
      "Epoch 1310/3000\n",
      "700/700 [==============================] - 0s 216us/step - loss: 0.5206 - acc: 0.8400 - val_loss: 1.9315 - val_acc: 0.5567\n",
      "Epoch 1311/3000\n",
      "700/700 [==============================] - 0s 232us/step - loss: 0.5210 - acc: 0.8414 - val_loss: 1.9352 - val_acc: 0.5600\n",
      "Epoch 1312/3000\n",
      "700/700 [==============================] - 0s 201us/step - loss: 0.5202 - acc: 0.8386 - val_loss: 1.9288 - val_acc: 0.5600\n",
      "Epoch 1313/3000\n",
      "700/700 [==============================] - 0s 199us/step - loss: 0.5206 - acc: 0.8371 - val_loss: 1.9288 - val_acc: 0.5567\n",
      "Epoch 1314/3000\n",
      "700/700 [==============================] - 0s 236us/step - loss: 0.5203 - acc: 0.8371 - val_loss: 1.9317 - val_acc: 0.5533\n",
      "Epoch 1315/3000\n",
      "700/700 [==============================] - 0s 231us/step - loss: 0.5213 - acc: 0.8400 - val_loss: 1.9308 - val_acc: 0.5567\n",
      "Epoch 1316/3000\n",
      "700/700 [==============================] - 0s 202us/step - loss: 0.5187 - acc: 0.8414 - val_loss: 1.9351 - val_acc: 0.5533\n",
      "Epoch 1317/3000\n",
      "700/700 [==============================] - 0s 205us/step - loss: 0.5189 - acc: 0.8386 - val_loss: 1.9268 - val_acc: 0.5567\n",
      "Epoch 1318/3000\n",
      "700/700 [==============================] - 0s 180us/step - loss: 0.5201 - acc: 0.8371 - val_loss: 1.9199 - val_acc: 0.5500\n",
      "Epoch 1319/3000\n",
      "700/700 [==============================] - 0s 208us/step - loss: 0.5191 - acc: 0.8400 - val_loss: 1.9305 - val_acc: 0.5567\n",
      "Epoch 1320/3000\n",
      "700/700 [==============================] - 0s 206us/step - loss: 0.5188 - acc: 0.8386 - val_loss: 1.9320 - val_acc: 0.5533\n",
      "Epoch 1321/3000\n",
      "700/700 [==============================] - 0s 213us/step - loss: 0.5180 - acc: 0.8414 - val_loss: 1.9295 - val_acc: 0.5467\n",
      "Epoch 1322/3000\n",
      "700/700 [==============================] - 0s 195us/step - loss: 0.5184 - acc: 0.8400 - val_loss: 1.9395 - val_acc: 0.5567\n",
      "Epoch 1323/3000\n",
      "700/700 [==============================] - 0s 204us/step - loss: 0.5185 - acc: 0.8414 - val_loss: 1.9350 - val_acc: 0.5533\n",
      "Epoch 1324/3000\n",
      "700/700 [==============================] - 0s 219us/step - loss: 0.5184 - acc: 0.8386 - val_loss: 1.9379 - val_acc: 0.5567\n",
      "Epoch 1325/3000\n",
      "700/700 [==============================] - 0s 200us/step - loss: 0.5190 - acc: 0.8386 - val_loss: 1.9311 - val_acc: 0.5500\n",
      "Epoch 1326/3000\n",
      "700/700 [==============================] - 0s 213us/step - loss: 0.5187 - acc: 0.8386 - val_loss: 1.9462 - val_acc: 0.5567\n",
      "Epoch 1327/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.5180 - acc: 0.8429 - val_loss: 1.9416 - val_acc: 0.5533\n",
      "Epoch 1328/3000\n",
      "700/700 [==============================] - 0s 206us/step - loss: 0.5189 - acc: 0.8371 - val_loss: 1.9508 - val_acc: 0.5600\n",
      "Epoch 1329/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.5179 - acc: 0.8400 - val_loss: 1.9380 - val_acc: 0.5533\n",
      "Epoch 1330/3000\n",
      "700/700 [==============================] - 0s 193us/step - loss: 0.5176 - acc: 0.8414 - val_loss: 1.9431 - val_acc: 0.5533\n",
      "Epoch 1331/3000\n",
      "700/700 [==============================] - 0s 207us/step - loss: 0.5166 - acc: 0.8400 - val_loss: 1.9498 - val_acc: 0.5567\n",
      "Epoch 1332/3000\n",
      "700/700 [==============================] - 0s 193us/step - loss: 0.5163 - acc: 0.8400 - val_loss: 1.9379 - val_acc: 0.5467\n",
      "Epoch 1333/3000\n",
      "700/700 [==============================] - 0s 192us/step - loss: 0.5163 - acc: 0.8386 - val_loss: 1.9401 - val_acc: 0.5533\n",
      "Epoch 1334/3000\n",
      "700/700 [==============================] - 0s 200us/step - loss: 0.5156 - acc: 0.8371 - val_loss: 1.9351 - val_acc: 0.5467\n",
      "Epoch 1335/3000\n",
      "700/700 [==============================] - 0s 198us/step - loss: 0.5161 - acc: 0.8429 - val_loss: 1.9367 - val_acc: 0.5500\n",
      "Epoch 1336/3000\n",
      "700/700 [==============================] - 0s 235us/step - loss: 0.5161 - acc: 0.8414 - val_loss: 1.9657 - val_acc: 0.5533\n",
      "Epoch 1337/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.5165 - acc: 0.8429 - val_loss: 1.9409 - val_acc: 0.5567\n",
      "Epoch 1338/3000\n",
      "700/700 [==============================] - 0s 218us/step - loss: 0.5155 - acc: 0.8371 - val_loss: 1.9550 - val_acc: 0.5567\n",
      "Epoch 1339/3000\n",
      "700/700 [==============================] - 0s 201us/step - loss: 0.5156 - acc: 0.8400 - val_loss: 1.9460 - val_acc: 0.5600\n",
      "Epoch 1340/3000\n",
      "700/700 [==============================] - 0s 231us/step - loss: 0.5148 - acc: 0.8414 - val_loss: 1.9464 - val_acc: 0.5567\n",
      "Epoch 1341/3000\n",
      "700/700 [==============================] - 0s 221us/step - loss: 0.5143 - acc: 0.8414 - val_loss: 1.9600 - val_acc: 0.5567\n",
      "Epoch 1342/3000\n",
      "700/700 [==============================] - 0s 221us/step - loss: 0.5172 - acc: 0.8400 - val_loss: 1.9503 - val_acc: 0.5533\n",
      "Epoch 1343/3000\n",
      "700/700 [==============================] - 0s 246us/step - loss: 0.5153 - acc: 0.8414 - val_loss: 1.9676 - val_acc: 0.5533\n",
      "Epoch 1344/3000\n",
      "700/700 [==============================] - 0s 233us/step - loss: 0.5154 - acc: 0.8414 - val_loss: 1.9533 - val_acc: 0.5567\n",
      "Epoch 1345/3000\n",
      "700/700 [==============================] - 0s 237us/step - loss: 0.5144 - acc: 0.8429 - val_loss: 1.9488 - val_acc: 0.5533\n",
      "Epoch 1346/3000\n",
      "700/700 [==============================] - 0s 276us/step - loss: 0.5140 - acc: 0.8386 - val_loss: 1.9573 - val_acc: 0.5500\n",
      "Epoch 1347/3000\n",
      "700/700 [==============================] - 0s 233us/step - loss: 0.5161 - acc: 0.8371 - val_loss: 1.9593 - val_acc: 0.5533\n",
      "Epoch 1348/3000\n",
      "700/700 [==============================] - 0s 257us/step - loss: 0.5151 - acc: 0.8414 - val_loss: 1.9473 - val_acc: 0.5533\n",
      "Epoch 1349/3000\n",
      "700/700 [==============================] - 0s 232us/step - loss: 0.5152 - acc: 0.8400 - val_loss: 1.9450 - val_acc: 0.5500\n",
      "Epoch 1350/3000\n",
      "700/700 [==============================] - 0s 200us/step - loss: 0.5140 - acc: 0.8429 - val_loss: 1.9445 - val_acc: 0.5467\n",
      "Epoch 1351/3000\n",
      "700/700 [==============================] - 0s 219us/step - loss: 0.5158 - acc: 0.8400 - val_loss: 1.9537 - val_acc: 0.5567\n",
      "Epoch 1352/3000\n",
      "700/700 [==============================] - 0s 232us/step - loss: 0.5134 - acc: 0.8429 - val_loss: 1.9462 - val_acc: 0.5533\n",
      "Epoch 1353/3000\n",
      "700/700 [==============================] - 0s 208us/step - loss: 0.5144 - acc: 0.8400 - val_loss: 1.9455 - val_acc: 0.5567\n",
      "Epoch 1354/3000\n",
      "700/700 [==============================] - 0s 216us/step - loss: 0.5137 - acc: 0.8414 - val_loss: 1.9568 - val_acc: 0.5467\n",
      "Epoch 1355/3000\n",
      "700/700 [==============================] - 0s 222us/step - loss: 0.5126 - acc: 0.8400 - val_loss: 1.9529 - val_acc: 0.5533\n",
      "Epoch 1356/3000\n",
      "700/700 [==============================] - 0s 213us/step - loss: 0.5157 - acc: 0.8400 - val_loss: 1.9604 - val_acc: 0.5533\n",
      "Epoch 1357/3000\n",
      "700/700 [==============================] - 0s 212us/step - loss: 0.5139 - acc: 0.8414 - val_loss: 1.9698 - val_acc: 0.5533\n",
      "Epoch 1358/3000\n",
      "700/700 [==============================] - 0s 207us/step - loss: 0.5127 - acc: 0.8414 - val_loss: 1.9590 - val_acc: 0.5567\n",
      "Epoch 1359/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 221us/step - loss: 0.5129 - acc: 0.8400 - val_loss: 1.9640 - val_acc: 0.5567\n",
      "Epoch 1360/3000\n",
      "700/700 [==============================] - 0s 249us/step - loss: 0.5123 - acc: 0.8429 - val_loss: 1.9639 - val_acc: 0.5533\n",
      "Epoch 1361/3000\n",
      "700/700 [==============================] - 0s 235us/step - loss: 0.5134 - acc: 0.8429 - val_loss: 1.9775 - val_acc: 0.5533\n",
      "Epoch 1362/3000\n",
      "700/700 [==============================] - 0s 213us/step - loss: 0.5118 - acc: 0.8400 - val_loss: 1.9538 - val_acc: 0.5533\n",
      "Epoch 1363/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 0.5130 - acc: 0.8400 - val_loss: 1.9591 - val_acc: 0.5600\n",
      "Epoch 1364/3000\n",
      "700/700 [==============================] - 0s 232us/step - loss: 0.5125 - acc: 0.8414 - val_loss: 1.9700 - val_acc: 0.5567\n",
      "Epoch 1365/3000\n",
      "700/700 [==============================] - 0s 208us/step - loss: 0.5123 - acc: 0.8414 - val_loss: 1.9773 - val_acc: 0.5500\n",
      "Epoch 1366/3000\n",
      "700/700 [==============================] - 0s 202us/step - loss: 0.5120 - acc: 0.8386 - val_loss: 1.9636 - val_acc: 0.5567\n",
      "Epoch 1367/3000\n",
      "700/700 [==============================] - 0s 233us/step - loss: 0.5117 - acc: 0.8400 - val_loss: 1.9661 - val_acc: 0.5500\n",
      "Epoch 1368/3000\n",
      "700/700 [==============================] - 0s 214us/step - loss: 0.5107 - acc: 0.8429 - val_loss: 1.9796 - val_acc: 0.5567\n",
      "Epoch 1369/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.5132 - acc: 0.8400 - val_loss: 1.9687 - val_acc: 0.5567\n",
      "Epoch 1370/3000\n",
      "700/700 [==============================] - 0s 205us/step - loss: 0.5119 - acc: 0.8414 - val_loss: 1.9756 - val_acc: 0.5633\n",
      "Epoch 1371/3000\n",
      "700/700 [==============================] - 0s 231us/step - loss: 0.5113 - acc: 0.8414 - val_loss: 1.9742 - val_acc: 0.5600\n",
      "Epoch 1372/3000\n",
      "700/700 [==============================] - 0s 205us/step - loss: 0.5101 - acc: 0.8429 - val_loss: 1.9742 - val_acc: 0.5533\n",
      "Epoch 1373/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.5113 - acc: 0.8414 - val_loss: 1.9696 - val_acc: 0.5600\n",
      "Epoch 1374/3000\n",
      "700/700 [==============================] - 0s 200us/step - loss: 0.5111 - acc: 0.8443 - val_loss: 1.9702 - val_acc: 0.5600\n",
      "Epoch 1375/3000\n",
      "700/700 [==============================] - 0s 210us/step - loss: 0.5101 - acc: 0.8414 - val_loss: 1.9771 - val_acc: 0.5600\n",
      "Epoch 1376/3000\n",
      "700/700 [==============================] - 0s 222us/step - loss: 0.5101 - acc: 0.8443 - val_loss: 1.9640 - val_acc: 0.5467\n",
      "Epoch 1377/3000\n",
      "700/700 [==============================] - 0s 221us/step - loss: 0.5112 - acc: 0.8414 - val_loss: 1.9678 - val_acc: 0.5600\n",
      "Epoch 1378/3000\n",
      "700/700 [==============================] - 0s 225us/step - loss: 0.5092 - acc: 0.8414 - val_loss: 1.9524 - val_acc: 0.5600\n",
      "Epoch 1379/3000\n",
      "700/700 [==============================] - 0s 229us/step - loss: 0.5104 - acc: 0.8414 - val_loss: 1.9598 - val_acc: 0.5533\n",
      "Epoch 1380/3000\n",
      "700/700 [==============================] - 0s 206us/step - loss: 0.5118 - acc: 0.8414 - val_loss: 1.9720 - val_acc: 0.5600\n",
      "Epoch 1381/3000\n",
      "700/700 [==============================] - 0s 208us/step - loss: 0.5097 - acc: 0.8429 - val_loss: 1.9699 - val_acc: 0.5567\n",
      "Epoch 1382/3000\n",
      "700/700 [==============================] - 0s 220us/step - loss: 0.5094 - acc: 0.8400 - val_loss: 1.9845 - val_acc: 0.5567\n",
      "Epoch 1383/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 0.5194 - acc: 0.838 - 0s 211us/step - loss: 0.5093 - acc: 0.8429 - val_loss: 1.9921 - val_acc: 0.5533\n",
      "Epoch 1384/3000\n",
      "700/700 [==============================] - 0s 191us/step - loss: 0.5114 - acc: 0.8400 - val_loss: 1.9713 - val_acc: 0.5533\n",
      "Epoch 1385/3000\n",
      "700/700 [==============================] - 0s 191us/step - loss: 0.5100 - acc: 0.8429 - val_loss: 1.9708 - val_acc: 0.5600\n",
      "Epoch 1386/3000\n",
      "700/700 [==============================] - 0s 190us/step - loss: 0.5085 - acc: 0.8400 - val_loss: 1.9817 - val_acc: 0.5567\n",
      "Epoch 1387/3000\n",
      "700/700 [==============================] - 0s 229us/step - loss: 0.5102 - acc: 0.8414 - val_loss: 1.9725 - val_acc: 0.5600\n",
      "Epoch 1388/3000\n",
      "700/700 [==============================] - 0s 209us/step - loss: 0.5092 - acc: 0.8400 - val_loss: 1.9814 - val_acc: 0.5600\n",
      "Epoch 1389/3000\n",
      "700/700 [==============================] - 0s 199us/step - loss: 0.5088 - acc: 0.8429 - val_loss: 1.9831 - val_acc: 0.5600\n",
      "Epoch 1390/3000\n",
      "700/700 [==============================] - 0s 204us/step - loss: 0.5096 - acc: 0.8414 - val_loss: 1.9833 - val_acc: 0.5533\n",
      "Epoch 1391/3000\n",
      "700/700 [==============================] - 0s 196us/step - loss: 0.5086 - acc: 0.8414 - val_loss: 1.9721 - val_acc: 0.5600\n",
      "Epoch 1392/3000\n",
      "700/700 [==============================] - 0s 184us/step - loss: 0.5081 - acc: 0.8429 - val_loss: 1.9777 - val_acc: 0.5567\n",
      "Epoch 1393/3000\n",
      "700/700 [==============================] - 0s 179us/step - loss: 0.5085 - acc: 0.8429 - val_loss: 1.9932 - val_acc: 0.5567\n",
      "Epoch 1394/3000\n",
      "700/700 [==============================] - 0s 207us/step - loss: 0.5073 - acc: 0.8414 - val_loss: 1.9648 - val_acc: 0.5467\n",
      "Epoch 1395/3000\n",
      "700/700 [==============================] - 0s 195us/step - loss: 0.5078 - acc: 0.8443 - val_loss: 1.9786 - val_acc: 0.5567\n",
      "Epoch 1396/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.5090 - acc: 0.8414 - val_loss: 1.9803 - val_acc: 0.5567\n",
      "Epoch 1397/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.5091 - acc: 0.8414 - val_loss: 1.9787 - val_acc: 0.5600\n",
      "Epoch 1398/3000\n",
      "700/700 [==============================] - 0s 181us/step - loss: 0.5079 - acc: 0.8414 - val_loss: 1.9707 - val_acc: 0.5467\n",
      "Epoch 1399/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 0.5081 - acc: 0.8443 - val_loss: 1.9786 - val_acc: 0.5567\n",
      "Epoch 1400/3000\n",
      "700/700 [==============================] - 0s 202us/step - loss: 0.5075 - acc: 0.8429 - val_loss: 1.9813 - val_acc: 0.5633\n",
      "Epoch 1401/3000\n",
      "700/700 [==============================] - 0s 201us/step - loss: 0.5083 - acc: 0.8429 - val_loss: 1.9791 - val_acc: 0.5633\n",
      "Epoch 1402/3000\n",
      "700/700 [==============================] - 0s 206us/step - loss: 0.5066 - acc: 0.8400 - val_loss: 1.9844 - val_acc: 0.5600\n",
      "Epoch 1403/3000\n",
      "700/700 [==============================] - 0s 182us/step - loss: 0.5074 - acc: 0.8414 - val_loss: 1.9886 - val_acc: 0.5633\n",
      "Epoch 1404/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.5082 - acc: 0.8400 - val_loss: 1.9796 - val_acc: 0.5533\n",
      "Epoch 1405/3000\n",
      "700/700 [==============================] - 0s 180us/step - loss: 0.5064 - acc: 0.8414 - val_loss: 1.9895 - val_acc: 0.5600\n",
      "Epoch 1406/3000\n",
      "700/700 [==============================] - 0s 186us/step - loss: 0.5087 - acc: 0.8400 - val_loss: 1.9873 - val_acc: 0.5533\n",
      "Epoch 1407/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.5071 - acc: 0.8429 - val_loss: 1.9846 - val_acc: 0.5567\n",
      "Epoch 1408/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.5062 - acc: 0.8443 - val_loss: 1.9887 - val_acc: 0.5567\n",
      "Epoch 1409/3000\n",
      "700/700 [==============================] - 0s 212us/step - loss: 0.5063 - acc: 0.8443 - val_loss: 1.9776 - val_acc: 0.5533\n",
      "Epoch 1410/3000\n",
      "700/700 [==============================] - 0s 193us/step - loss: 0.5059 - acc: 0.8414 - val_loss: 1.9815 - val_acc: 0.5633\n",
      "Epoch 1411/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.5054 - acc: 0.8443 - val_loss: 1.9896 - val_acc: 0.5567\n",
      "Epoch 1412/3000\n",
      "700/700 [==============================] - 0s 176us/step - loss: 0.5064 - acc: 0.8429 - val_loss: 1.9824 - val_acc: 0.5500\n",
      "Epoch 1413/3000\n",
      "700/700 [==============================] - 0s 190us/step - loss: 0.5040 - acc: 0.8429 - val_loss: 1.9956 - val_acc: 0.5600\n",
      "Epoch 1414/3000\n",
      "700/700 [==============================] - 0s 196us/step - loss: 0.5068 - acc: 0.8443 - val_loss: 1.9926 - val_acc: 0.5567\n",
      "Epoch 1415/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.5057 - acc: 0.8429 - val_loss: 1.9862 - val_acc: 0.5600\n",
      "Epoch 1416/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.5054 - acc: 0.8429 - val_loss: 1.9869 - val_acc: 0.5633\n",
      "Epoch 1417/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.5054 - acc: 0.8457 - val_loss: 1.9858 - val_acc: 0.5600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1418/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.5040 - acc: 0.8443 - val_loss: 1.9809 - val_acc: 0.5533\n",
      "Epoch 1419/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.5033 - acc: 0.8443 - val_loss: 2.0020 - val_acc: 0.5567\n",
      "Epoch 1420/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.5058 - acc: 0.8443 - val_loss: 1.9993 - val_acc: 0.5567\n",
      "Epoch 1421/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.5047 - acc: 0.8443 - val_loss: 1.9909 - val_acc: 0.5600\n",
      "Epoch 1422/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.5047 - acc: 0.8429 - val_loss: 2.0012 - val_acc: 0.5567\n",
      "Epoch 1423/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.5047 - acc: 0.8443 - val_loss: 1.9865 - val_acc: 0.5533\n",
      "Epoch 1424/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.5049 - acc: 0.8414 - val_loss: 2.0049 - val_acc: 0.5567\n",
      "Epoch 1425/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.5052 - acc: 0.8414 - val_loss: 1.9891 - val_acc: 0.5567\n",
      "Epoch 1426/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.5047 - acc: 0.8457 - val_loss: 1.9890 - val_acc: 0.5567\n",
      "Epoch 1427/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.5028 - acc: 0.8443 - val_loss: 1.9997 - val_acc: 0.5600\n",
      "Epoch 1428/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.5044 - acc: 0.8457 - val_loss: 2.0015 - val_acc: 0.5600\n",
      "Epoch 1429/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.5031 - acc: 0.8457 - val_loss: 2.0022 - val_acc: 0.5567\n",
      "Epoch 1430/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.5039 - acc: 0.8429 - val_loss: 1.9891 - val_acc: 0.5567\n",
      "Epoch 1431/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.5028 - acc: 0.8429 - val_loss: 2.0027 - val_acc: 0.5567\n",
      "Epoch 1432/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.5041 - acc: 0.8443 - val_loss: 2.0026 - val_acc: 0.5633\n",
      "Epoch 1433/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.5019 - acc: 0.8429 - val_loss: 1.9918 - val_acc: 0.5500\n",
      "Epoch 1434/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.5026 - acc: 0.8429 - val_loss: 2.0054 - val_acc: 0.5600\n",
      "Epoch 1435/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.5033 - acc: 0.8457 - val_loss: 2.0042 - val_acc: 0.5600\n",
      "Epoch 1436/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.5029 - acc: 0.8443 - val_loss: 1.9920 - val_acc: 0.5500\n",
      "Epoch 1437/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.5030 - acc: 0.8414 - val_loss: 2.0091 - val_acc: 0.5600\n",
      "Epoch 1438/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.5016 - acc: 0.8429 - val_loss: 1.9934 - val_acc: 0.5467\n",
      "Epoch 1439/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.5025 - acc: 0.8414 - val_loss: 2.0164 - val_acc: 0.5600\n",
      "Epoch 1440/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.5029 - acc: 0.8443 - val_loss: 1.9982 - val_acc: 0.5567\n",
      "Epoch 1441/3000\n",
      "700/700 [==============================] - 0s 176us/step - loss: 0.5030 - acc: 0.8429 - val_loss: 1.9957 - val_acc: 0.5567\n",
      "Epoch 1442/3000\n",
      "700/700 [==============================] - 0s 178us/step - loss: 0.5036 - acc: 0.8400 - val_loss: 1.9952 - val_acc: 0.5567\n",
      "Epoch 1443/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.5025 - acc: 0.8443 - val_loss: 2.0037 - val_acc: 0.5533\n",
      "Epoch 1444/3000\n",
      "700/700 [==============================] - 0s 253us/step - loss: 0.5025 - acc: 0.8443 - val_loss: 2.0026 - val_acc: 0.5567\n",
      "Epoch 1445/3000\n",
      "700/700 [==============================] - 0s 179us/step - loss: 0.5023 - acc: 0.8443 - val_loss: 2.0020 - val_acc: 0.5633\n",
      "Epoch 1446/3000\n",
      "700/700 [==============================] - 0s 220us/step - loss: 0.5013 - acc: 0.8443 - val_loss: 2.0009 - val_acc: 0.5633\n",
      "Epoch 1447/3000\n",
      "700/700 [==============================] - 0s 186us/step - loss: 0.5008 - acc: 0.8429 - val_loss: 2.0000 - val_acc: 0.5533\n",
      "Epoch 1448/3000\n",
      "700/700 [==============================] - 0s 189us/step - loss: 0.5014 - acc: 0.8400 - val_loss: 2.0116 - val_acc: 0.5600\n",
      "Epoch 1449/3000\n",
      "700/700 [==============================] - 0s 191us/step - loss: 0.5010 - acc: 0.8443 - val_loss: 1.9997 - val_acc: 0.5467\n",
      "Epoch 1450/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.5041 - acc: 0.8414 - val_loss: 2.0108 - val_acc: 0.5600\n",
      "Epoch 1451/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.5014 - acc: 0.8443 - val_loss: 2.0054 - val_acc: 0.5567\n",
      "Epoch 1452/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.5017 - acc: 0.8443 - val_loss: 2.0079 - val_acc: 0.5633\n",
      "Epoch 1453/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.5007 - acc: 0.8443 - val_loss: 2.0063 - val_acc: 0.5600\n",
      "Epoch 1454/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.5011 - acc: 0.8457 - val_loss: 2.0099 - val_acc: 0.5600\n",
      "Epoch 1455/3000\n",
      "700/700 [==============================] - 0s 207us/step - loss: 0.5002 - acc: 0.8457 - val_loss: 2.0078 - val_acc: 0.5533\n",
      "Epoch 1456/3000\n",
      "700/700 [==============================] - 0s 180us/step - loss: 0.5025 - acc: 0.8443 - val_loss: 2.0121 - val_acc: 0.5633\n",
      "Epoch 1457/3000\n",
      "700/700 [==============================] - 0s 204us/step - loss: 0.4993 - acc: 0.8443 - val_loss: 2.0173 - val_acc: 0.5600\n",
      "Epoch 1458/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.5003 - acc: 0.8443 - val_loss: 2.0168 - val_acc: 0.5600\n",
      "Epoch 1459/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.5000 - acc: 0.8414 - val_loss: 2.0128 - val_acc: 0.5600\n",
      "Epoch 1460/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.5005 - acc: 0.8471 - val_loss: 2.0115 - val_acc: 0.5600\n",
      "Epoch 1461/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.5006 - acc: 0.8443 - val_loss: 2.0096 - val_acc: 0.5600\n",
      "Epoch 1462/3000\n",
      "700/700 [==============================] - 0s 198us/step - loss: 0.5002 - acc: 0.8414 - val_loss: 2.0195 - val_acc: 0.5533\n",
      "Epoch 1463/3000\n",
      "700/700 [==============================] - 0s 195us/step - loss: 0.4996 - acc: 0.8443 - val_loss: 2.0072 - val_acc: 0.5533\n",
      "Epoch 1464/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.4989 - acc: 0.8443 - val_loss: 2.0084 - val_acc: 0.5567\n",
      "Epoch 1465/3000\n",
      "700/700 [==============================] - 0s 196us/step - loss: 0.4980 - acc: 0.8457 - val_loss: 2.0132 - val_acc: 0.5600\n",
      "Epoch 1466/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.4987 - acc: 0.8457 - val_loss: 2.0177 - val_acc: 0.5567\n",
      "Epoch 1467/3000\n",
      "700/700 [==============================] - 0s 195us/step - loss: 0.4987 - acc: 0.8443 - val_loss: 2.0289 - val_acc: 0.5533\n",
      "Epoch 1468/3000\n",
      "700/700 [==============================] - 0s 190us/step - loss: 0.4995 - acc: 0.8457 - val_loss: 2.0217 - val_acc: 0.5633\n",
      "Epoch 1469/3000\n",
      "700/700 [==============================] - 0s 180us/step - loss: 0.4994 - acc: 0.8457 - val_loss: 2.0185 - val_acc: 0.5567\n",
      "Epoch 1470/3000\n",
      "700/700 [==============================] - 0s 182us/step - loss: 0.4984 - acc: 0.8429 - val_loss: 2.0156 - val_acc: 0.5600\n",
      "Epoch 1471/3000\n",
      "700/700 [==============================] - 0s 189us/step - loss: 0.4980 - acc: 0.8443 - val_loss: 2.0184 - val_acc: 0.5567\n",
      "Epoch 1472/3000\n",
      "700/700 [==============================] - 0s 214us/step - loss: 0.4985 - acc: 0.8443 - val_loss: 2.0107 - val_acc: 0.5600\n",
      "Epoch 1473/3000\n",
      "700/700 [==============================] - 0s 186us/step - loss: 0.4979 - acc: 0.8457 - val_loss: 2.0112 - val_acc: 0.5600\n",
      "Epoch 1474/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4979 - acc: 0.8443 - val_loss: 2.0125 - val_acc: 0.5600\n",
      "Epoch 1475/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4972 - acc: 0.8443 - val_loss: 2.0240 - val_acc: 0.5633\n",
      "Epoch 1476/3000\n",
      "700/700 [==============================] - 0s 192us/step - loss: 0.4991 - acc: 0.8443 - val_loss: 2.0307 - val_acc: 0.5600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1477/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4988 - acc: 0.8457 - val_loss: 2.0180 - val_acc: 0.5533\n",
      "Epoch 1478/3000\n",
      "700/700 [==============================] - 0s 178us/step - loss: 0.4990 - acc: 0.8443 - val_loss: 2.0313 - val_acc: 0.5600\n",
      "Epoch 1479/3000\n",
      "700/700 [==============================] - 0s 192us/step - loss: 0.4980 - acc: 0.8443 - val_loss: 2.0248 - val_acc: 0.5600\n",
      "Epoch 1480/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.4966 - acc: 0.8443 - val_loss: 2.0322 - val_acc: 0.5567\n",
      "Epoch 1481/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4975 - acc: 0.8457 - val_loss: 2.0235 - val_acc: 0.5567\n",
      "Epoch 1482/3000\n",
      "700/700 [==============================] - 0s 192us/step - loss: 0.4978 - acc: 0.8443 - val_loss: 2.0329 - val_acc: 0.5600\n",
      "Epoch 1483/3000\n",
      "700/700 [==============================] - 0s 180us/step - loss: 0.4972 - acc: 0.8457 - val_loss: 2.0300 - val_acc: 0.5500\n",
      "Epoch 1484/3000\n",
      "700/700 [==============================] - 0s 191us/step - loss: 0.4964 - acc: 0.8457 - val_loss: 2.0357 - val_acc: 0.5567\n",
      "Epoch 1485/3000\n",
      "700/700 [==============================] - 0s 185us/step - loss: 0.4948 - acc: 0.8443 - val_loss: 2.0297 - val_acc: 0.5567\n",
      "Epoch 1486/3000\n",
      "700/700 [==============================] - 0s 201us/step - loss: 0.4970 - acc: 0.8429 - val_loss: 2.0272 - val_acc: 0.5467\n",
      "Epoch 1487/3000\n",
      "700/700 [==============================] - 0s 206us/step - loss: 0.4964 - acc: 0.8443 - val_loss: 2.0268 - val_acc: 0.5600\n",
      "Epoch 1488/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 0.4957 - acc: 0.8457 - val_loss: 2.0390 - val_acc: 0.5633\n",
      "Epoch 1489/3000\n",
      "700/700 [==============================] - 0s 200us/step - loss: 0.4953 - acc: 0.8471 - val_loss: 2.0320 - val_acc: 0.5600\n",
      "Epoch 1490/3000\n",
      "700/700 [==============================] - 0s 215us/step - loss: 0.4954 - acc: 0.8457 - val_loss: 2.0299 - val_acc: 0.5633\n",
      "Epoch 1491/3000\n",
      "700/700 [==============================] - 0s 198us/step - loss: 0.4948 - acc: 0.8443 - val_loss: 2.0357 - val_acc: 0.5600\n",
      "Epoch 1492/3000\n",
      "700/700 [==============================] - 0s 226us/step - loss: 0.4945 - acc: 0.8471 - val_loss: 2.0394 - val_acc: 0.5600\n",
      "Epoch 1493/3000\n",
      "700/700 [==============================] - 0s 195us/step - loss: 0.4936 - acc: 0.8457 - val_loss: 2.0432 - val_acc: 0.5600\n",
      "Epoch 1494/3000\n",
      "700/700 [==============================] - 0s 227us/step - loss: 0.4954 - acc: 0.8457 - val_loss: 2.0395 - val_acc: 0.5600\n",
      "Epoch 1495/3000\n",
      "700/700 [==============================] - 0s 198us/step - loss: 0.4950 - acc: 0.8457 - val_loss: 2.0391 - val_acc: 0.5567\n",
      "Epoch 1496/3000\n",
      "700/700 [==============================] - 0s 192us/step - loss: 0.4949 - acc: 0.8443 - val_loss: 2.0346 - val_acc: 0.5633\n",
      "Epoch 1497/3000\n",
      "700/700 [==============================] - 0s 199us/step - loss: 0.4946 - acc: 0.8457 - val_loss: 2.0412 - val_acc: 0.5600\n",
      "Epoch 1498/3000\n",
      "700/700 [==============================] - 0s 198us/step - loss: 0.4947 - acc: 0.8486 - val_loss: 2.0317 - val_acc: 0.5600\n",
      "Epoch 1499/3000\n",
      "700/700 [==============================] - 0s 190us/step - loss: 0.4948 - acc: 0.8443 - val_loss: 2.0352 - val_acc: 0.5633\n",
      "Epoch 1500/3000\n",
      "700/700 [==============================] - 0s 179us/step - loss: 0.4944 - acc: 0.8443 - val_loss: 2.0450 - val_acc: 0.5567\n",
      "Epoch 1501/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 0.4933 - acc: 0.8457 - val_loss: 2.0427 - val_acc: 0.5600\n",
      "Epoch 1502/3000\n",
      "700/700 [==============================] - 0s 186us/step - loss: 0.4926 - acc: 0.8486 - val_loss: 2.0591 - val_acc: 0.5567\n",
      "Epoch 1503/3000\n",
      "700/700 [==============================] - 0s 179us/step - loss: 0.4928 - acc: 0.8443 - val_loss: 2.0372 - val_acc: 0.5567\n",
      "Epoch 1504/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4934 - acc: 0.8471 - val_loss: 2.0356 - val_acc: 0.5633\n",
      "Epoch 1505/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4935 - acc: 0.8457 - val_loss: 2.0518 - val_acc: 0.5600\n",
      "Epoch 1506/3000\n",
      "700/700 [==============================] - 0s 207us/step - loss: 0.4942 - acc: 0.8457 - val_loss: 2.0621 - val_acc: 0.5533\n",
      "Epoch 1507/3000\n",
      "700/700 [==============================] - 0s 204us/step - loss: 0.4924 - acc: 0.8471 - val_loss: 2.0401 - val_acc: 0.5533\n",
      "Epoch 1508/3000\n",
      "700/700 [==============================] - 0s 200us/step - loss: 0.4932 - acc: 0.8457 - val_loss: 2.0375 - val_acc: 0.5567\n",
      "Epoch 1509/3000\n",
      "700/700 [==============================] - 0s 198us/step - loss: 0.4935 - acc: 0.8443 - val_loss: 2.0463 - val_acc: 0.5600\n",
      "Epoch 1510/3000\n",
      "700/700 [==============================] - 0s 201us/step - loss: 0.4929 - acc: 0.8486 - val_loss: 2.0538 - val_acc: 0.5567\n",
      "Epoch 1511/3000\n",
      "700/700 [==============================] - 0s 204us/step - loss: 0.4926 - acc: 0.8471 - val_loss: 2.0525 - val_acc: 0.5633\n",
      "Epoch 1512/3000\n",
      "700/700 [==============================] - 0s 216us/step - loss: 0.4922 - acc: 0.8471 - val_loss: 2.0565 - val_acc: 0.5633\n",
      "Epoch 1513/3000\n",
      "700/700 [==============================] - 0s 207us/step - loss: 0.4915 - acc: 0.8457 - val_loss: 2.0502 - val_acc: 0.5567\n",
      "Epoch 1514/3000\n",
      "700/700 [==============================] - 0s 208us/step - loss: 0.4916 - acc: 0.8457 - val_loss: 2.0503 - val_acc: 0.5600\n",
      "Epoch 1515/3000\n",
      "700/700 [==============================] - 0s 183us/step - loss: 0.4914 - acc: 0.8471 - val_loss: 2.0455 - val_acc: 0.5533\n",
      "Epoch 1516/3000\n",
      "700/700 [==============================] - 0s 204us/step - loss: 0.4925 - acc: 0.8443 - val_loss: 2.0469 - val_acc: 0.5600\n",
      "Epoch 1517/3000\n",
      "700/700 [==============================] - 0s 232us/step - loss: 0.4917 - acc: 0.8443 - val_loss: 2.0441 - val_acc: 0.5633\n",
      "Epoch 1518/3000\n",
      "700/700 [==============================] - 0s 200us/step - loss: 0.4922 - acc: 0.8486 - val_loss: 2.0476 - val_acc: 0.5600\n",
      "Epoch 1519/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4922 - acc: 0.8443 - val_loss: 2.0516 - val_acc: 0.5533\n",
      "Epoch 1520/3000\n",
      "700/700 [==============================] - 0s 190us/step - loss: 0.4919 - acc: 0.8457 - val_loss: 2.0649 - val_acc: 0.5567\n",
      "Epoch 1521/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 0.4916 - acc: 0.8443 - val_loss: 2.0511 - val_acc: 0.5600\n",
      "Epoch 1522/3000\n",
      "700/700 [==============================] - 0s 212us/step - loss: 0.4902 - acc: 0.8471 - val_loss: 2.0511 - val_acc: 0.5600\n",
      "Epoch 1523/3000\n",
      "700/700 [==============================] - 0s 221us/step - loss: 0.4910 - acc: 0.8471 - val_loss: 2.0450 - val_acc: 0.5567\n",
      "Epoch 1524/3000\n",
      "700/700 [==============================] - 0s 196us/step - loss: 0.4914 - acc: 0.8486 - val_loss: 2.0517 - val_acc: 0.5633\n",
      "Epoch 1525/3000\n",
      "700/700 [==============================] - 0s 216us/step - loss: 0.4908 - acc: 0.8486 - val_loss: 2.0398 - val_acc: 0.5433\n",
      "Epoch 1526/3000\n",
      "700/700 [==============================] - 0s 198us/step - loss: 0.4913 - acc: 0.8457 - val_loss: 2.0581 - val_acc: 0.5600\n",
      "Epoch 1527/3000\n",
      "700/700 [==============================] - 0s 204us/step - loss: 0.4912 - acc: 0.8457 - val_loss: 2.0663 - val_acc: 0.5567\n",
      "Epoch 1528/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4913 - acc: 0.8471 - val_loss: 2.0623 - val_acc: 0.5600\n",
      "Epoch 1529/3000\n",
      "700/700 [==============================] - 0s 223us/step - loss: 0.4900 - acc: 0.8486 - val_loss: 2.0583 - val_acc: 0.5533\n",
      "Epoch 1530/3000\n",
      "700/700 [==============================] - 0s 212us/step - loss: 0.4904 - acc: 0.8471 - val_loss: 2.0601 - val_acc: 0.5600\n",
      "Epoch 1531/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 0.4906 - acc: 0.8486 - val_loss: 2.0463 - val_acc: 0.5500\n",
      "Epoch 1532/3000\n",
      "700/700 [==============================] - 0s 219us/step - loss: 0.4902 - acc: 0.8471 - val_loss: 2.0575 - val_acc: 0.5533\n",
      "Epoch 1533/3000\n",
      "700/700 [==============================] - 0s 220us/step - loss: 0.4909 - acc: 0.8471 - val_loss: 2.0653 - val_acc: 0.5567\n",
      "Epoch 1534/3000\n",
      "700/700 [==============================] - 0s 279us/step - loss: 0.4913 - acc: 0.8471 - val_loss: 2.0574 - val_acc: 0.5633\n",
      "Epoch 1535/3000\n",
      "700/700 [==============================] - 0s 244us/step - loss: 0.4896 - acc: 0.8457 - val_loss: 2.0647 - val_acc: 0.5567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1536/3000\n",
      "700/700 [==============================] - 0s 224us/step - loss: 0.4900 - acc: 0.8471 - val_loss: 2.0647 - val_acc: 0.5600\n",
      "Epoch 1537/3000\n",
      "700/700 [==============================] - 0s 219us/step - loss: 0.4896 - acc: 0.8471 - val_loss: 2.0624 - val_acc: 0.5600\n",
      "Epoch 1538/3000\n",
      "700/700 [==============================] - 0s 227us/step - loss: 0.4896 - acc: 0.8457 - val_loss: 2.0597 - val_acc: 0.5567\n",
      "Epoch 1539/3000\n",
      "700/700 [==============================] - 0s 218us/step - loss: 0.4887 - acc: 0.8471 - val_loss: 2.0727 - val_acc: 0.5633\n",
      "Epoch 1540/3000\n",
      "700/700 [==============================] - 0s 209us/step - loss: 0.4893 - acc: 0.8471 - val_loss: 2.0688 - val_acc: 0.5600\n",
      "Epoch 1541/3000\n",
      "700/700 [==============================] - 0s 227us/step - loss: 0.4897 - acc: 0.8457 - val_loss: 2.0653 - val_acc: 0.5633\n",
      "Epoch 1542/3000\n",
      "700/700 [==============================] - 0s 227us/step - loss: 0.4886 - acc: 0.8500 - val_loss: 2.0736 - val_acc: 0.5600\n",
      "Epoch 1543/3000\n",
      "700/700 [==============================] - 0s 216us/step - loss: 0.4892 - acc: 0.8471 - val_loss: 2.0617 - val_acc: 0.5633\n",
      "Epoch 1544/3000\n",
      "700/700 [==============================] - 0s 206us/step - loss: 0.4880 - acc: 0.8471 - val_loss: 2.0615 - val_acc: 0.5567\n",
      "Epoch 1545/3000\n",
      "700/700 [==============================] - 0s 205us/step - loss: 0.4886 - acc: 0.8471 - val_loss: 2.0731 - val_acc: 0.5633\n",
      "Epoch 1546/3000\n",
      "700/700 [==============================] - 0s 219us/step - loss: 0.4888 - acc: 0.8457 - val_loss: 2.0662 - val_acc: 0.5600\n",
      "Epoch 1547/3000\n",
      "700/700 [==============================] - 0s 214us/step - loss: 0.4880 - acc: 0.8486 - val_loss: 2.0644 - val_acc: 0.5500\n",
      "Epoch 1548/3000\n",
      "700/700 [==============================] - 0s 222us/step - loss: 0.4887 - acc: 0.8486 - val_loss: 2.0755 - val_acc: 0.5567\n",
      "Epoch 1549/3000\n",
      "700/700 [==============================] - 0s 205us/step - loss: 0.4887 - acc: 0.8471 - val_loss: 2.0770 - val_acc: 0.5567\n",
      "Epoch 1550/3000\n",
      "700/700 [==============================] - 0s 222us/step - loss: 0.4885 - acc: 0.8486 - val_loss: 2.0581 - val_acc: 0.5600\n",
      "Epoch 1551/3000\n",
      "700/700 [==============================] - 0s 200us/step - loss: 0.4881 - acc: 0.8471 - val_loss: 2.0637 - val_acc: 0.5567\n",
      "Epoch 1552/3000\n",
      "700/700 [==============================] - 0s 214us/step - loss: 0.4878 - acc: 0.8486 - val_loss: 2.0634 - val_acc: 0.5567\n",
      "Epoch 1553/3000\n",
      "700/700 [==============================] - 0s 216us/step - loss: 0.4873 - acc: 0.8471 - val_loss: 2.0670 - val_acc: 0.5533\n",
      "Epoch 1554/3000\n",
      "700/700 [==============================] - 0s 204us/step - loss: 0.4876 - acc: 0.8457 - val_loss: 2.0588 - val_acc: 0.5500\n",
      "Epoch 1555/3000\n",
      "700/700 [==============================] - 0s 224us/step - loss: 0.4872 - acc: 0.8471 - val_loss: 2.0660 - val_acc: 0.5500\n",
      "Epoch 1556/3000\n",
      "700/700 [==============================] - 0s 200us/step - loss: 0.4872 - acc: 0.8486 - val_loss: 2.0685 - val_acc: 0.5567\n",
      "Epoch 1557/3000\n",
      "700/700 [==============================] - 0s 224us/step - loss: 0.4872 - acc: 0.8471 - val_loss: 2.0655 - val_acc: 0.5567\n",
      "Epoch 1558/3000\n",
      "700/700 [==============================] - 0s 241us/step - loss: 0.4873 - acc: 0.8471 - val_loss: 2.0617 - val_acc: 0.5567\n",
      "Epoch 1559/3000\n",
      "700/700 [==============================] - 0s 237us/step - loss: 0.4874 - acc: 0.8486 - val_loss: 2.0619 - val_acc: 0.5600\n",
      "Epoch 1560/3000\n",
      "700/700 [==============================] - 0s 230us/step - loss: 0.4877 - acc: 0.8471 - val_loss: 2.0665 - val_acc: 0.5533\n",
      "Epoch 1561/3000\n",
      "700/700 [==============================] - 0s 241us/step - loss: 0.4866 - acc: 0.8457 - val_loss: 2.0666 - val_acc: 0.5633\n",
      "Epoch 1562/3000\n",
      "700/700 [==============================] - 0s 233us/step - loss: 0.4880 - acc: 0.8471 - val_loss: 2.0692 - val_acc: 0.5567\n",
      "Epoch 1563/3000\n",
      "700/700 [==============================] - 0s 231us/step - loss: 0.4872 - acc: 0.8471 - val_loss: 2.0813 - val_acc: 0.5533\n",
      "Epoch 1564/3000\n",
      "700/700 [==============================] - 0s 219us/step - loss: 0.4872 - acc: 0.8471 - val_loss: 2.0734 - val_acc: 0.5533\n",
      "Epoch 1565/3000\n",
      "700/700 [==============================] - 0s 218us/step - loss: 0.4866 - acc: 0.8486 - val_loss: 2.0714 - val_acc: 0.5600\n",
      "Epoch 1566/3000\n",
      "700/700 [==============================] - 0s 214us/step - loss: 0.4865 - acc: 0.8486 - val_loss: 2.0740 - val_acc: 0.5600\n",
      "Epoch 1567/3000\n",
      "700/700 [==============================] - 0s 232us/step - loss: 0.4865 - acc: 0.8457 - val_loss: 2.0740 - val_acc: 0.5600\n",
      "Epoch 1568/3000\n",
      "700/700 [==============================] - 0s 233us/step - loss: 0.4859 - acc: 0.8471 - val_loss: 2.0779 - val_acc: 0.5567\n",
      "Epoch 1569/3000\n",
      "700/700 [==============================] - 0s 224us/step - loss: 0.4857 - acc: 0.8471 - val_loss: 2.0763 - val_acc: 0.5600\n",
      "Epoch 1570/3000\n",
      "700/700 [==============================] - 0s 219us/step - loss: 0.4854 - acc: 0.8471 - val_loss: 2.0655 - val_acc: 0.5500\n",
      "Epoch 1571/3000\n",
      "700/700 [==============================] - 0s 212us/step - loss: 0.4864 - acc: 0.8471 - val_loss: 2.0885 - val_acc: 0.5567\n",
      "Epoch 1572/3000\n",
      "700/700 [==============================] - 0s 227us/step - loss: 0.4850 - acc: 0.8471 - val_loss: 2.0964 - val_acc: 0.5600\n",
      "Epoch 1573/3000\n",
      "700/700 [==============================] - 0s 214us/step - loss: 0.4861 - acc: 0.8471 - val_loss: 2.0830 - val_acc: 0.5533\n",
      "Epoch 1574/3000\n",
      "700/700 [==============================] - 0s 244us/step - loss: 0.4858 - acc: 0.8471 - val_loss: 2.0913 - val_acc: 0.5600\n",
      "Epoch 1575/3000\n",
      "700/700 [==============================] - 0s 248us/step - loss: 0.4858 - acc: 0.8457 - val_loss: 2.0709 - val_acc: 0.5467\n",
      "Epoch 1576/3000\n",
      "700/700 [==============================] - 0s 236us/step - loss: 0.4848 - acc: 0.8486 - val_loss: 2.0758 - val_acc: 0.5467\n",
      "Epoch 1577/3000\n",
      "700/700 [==============================] - 0s 243us/step - loss: 0.4847 - acc: 0.8457 - val_loss: 2.0843 - val_acc: 0.5600\n",
      "Epoch 1578/3000\n",
      "700/700 [==============================] - 0s 219us/step - loss: 0.4849 - acc: 0.8500 - val_loss: 2.0888 - val_acc: 0.5567\n",
      "Epoch 1579/3000\n",
      "700/700 [==============================] - 0s 215us/step - loss: 0.4862 - acc: 0.8471 - val_loss: 2.0972 - val_acc: 0.5600\n",
      "Epoch 1580/3000\n",
      "700/700 [==============================] - 0s 232us/step - loss: 0.4861 - acc: 0.8486 - val_loss: 2.0814 - val_acc: 0.5567\n",
      "Epoch 1581/3000\n",
      "700/700 [==============================] - 0s 225us/step - loss: 0.4839 - acc: 0.8500 - val_loss: 2.0741 - val_acc: 0.5567\n",
      "Epoch 1582/3000\n",
      "700/700 [==============================] - 0s 229us/step - loss: 0.4837 - acc: 0.8500 - val_loss: 2.0789 - val_acc: 0.5600\n",
      "Epoch 1583/3000\n",
      "700/700 [==============================] - 0s 236us/step - loss: 0.4849 - acc: 0.8471 - val_loss: 2.0701 - val_acc: 0.5500\n",
      "Epoch 1584/3000\n",
      "700/700 [==============================] - 0s 231us/step - loss: 0.4846 - acc: 0.8486 - val_loss: 2.1008 - val_acc: 0.5600\n",
      "Epoch 1585/3000\n",
      "700/700 [==============================] - 0s 243us/step - loss: 0.4837 - acc: 0.8471 - val_loss: 2.0875 - val_acc: 0.5567\n",
      "Epoch 1586/3000\n",
      "700/700 [==============================] - 0s 247us/step - loss: 0.4845 - acc: 0.8471 - val_loss: 2.1066 - val_acc: 0.5533\n",
      "Epoch 1587/3000\n",
      "700/700 [==============================] - 0s 227us/step - loss: 0.4838 - acc: 0.8486 - val_loss: 2.0810 - val_acc: 0.5567\n",
      "Epoch 1588/3000\n",
      "700/700 [==============================] - 0s 224us/step - loss: 0.4828 - acc: 0.8471 - val_loss: 2.0952 - val_acc: 0.5567\n",
      "Epoch 1589/3000\n",
      "700/700 [==============================] - 0s 254us/step - loss: 0.4828 - acc: 0.8471 - val_loss: 2.1003 - val_acc: 0.5567\n",
      "Epoch 1590/3000\n",
      "700/700 [==============================] - 0s 226us/step - loss: 0.4843 - acc: 0.8486 - val_loss: 2.0982 - val_acc: 0.5600\n",
      "Epoch 1591/3000\n",
      "700/700 [==============================] - 0s 238us/step - loss: 0.4850 - acc: 0.8471 - val_loss: 2.0845 - val_acc: 0.5567\n",
      "Epoch 1592/3000\n",
      "700/700 [==============================] - 0s 258us/step - loss: 0.4836 - acc: 0.8486 - val_loss: 2.0837 - val_acc: 0.5533\n",
      "Epoch 1593/3000\n",
      "700/700 [==============================] - 0s 226us/step - loss: 0.4828 - acc: 0.8486 - val_loss: 2.0959 - val_acc: 0.5633\n",
      "Epoch 1594/3000\n",
      "700/700 [==============================] - 0s 216us/step - loss: 0.4842 - acc: 0.8486 - val_loss: 2.0948 - val_acc: 0.5633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1595/3000\n",
      "700/700 [==============================] - 0s 224us/step - loss: 0.4839 - acc: 0.8471 - val_loss: 2.0906 - val_acc: 0.5567\n",
      "Epoch 1596/3000\n",
      "700/700 [==============================] - 0s 219us/step - loss: 0.4836 - acc: 0.8471 - val_loss: 2.1045 - val_acc: 0.5633\n",
      "Epoch 1597/3000\n",
      "700/700 [==============================] - 0s 237us/step - loss: 0.4830 - acc: 0.8471 - val_loss: 2.1022 - val_acc: 0.5567\n",
      "Epoch 1598/3000\n",
      "700/700 [==============================] - 0s 248us/step - loss: 0.4837 - acc: 0.8457 - val_loss: 2.0879 - val_acc: 0.5433\n",
      "Epoch 1599/3000\n",
      "700/700 [==============================] - 0s 235us/step - loss: 0.4828 - acc: 0.8471 - val_loss: 2.0922 - val_acc: 0.5433\n",
      "Epoch 1600/3000\n",
      "700/700 [==============================] - 0s 234us/step - loss: 0.4830 - acc: 0.8457 - val_loss: 2.0971 - val_acc: 0.5533\n",
      "Epoch 1601/3000\n",
      "700/700 [==============================] - 0s 238us/step - loss: 0.4826 - acc: 0.8471 - val_loss: 2.1015 - val_acc: 0.5600\n",
      "Epoch 1602/3000\n",
      "700/700 [==============================] - 0s 246us/step - loss: 0.4820 - acc: 0.8486 - val_loss: 2.1012 - val_acc: 0.5567\n",
      "Epoch 1603/3000\n",
      "700/700 [==============================] - 0s 245us/step - loss: 0.4827 - acc: 0.8486 - val_loss: 2.0932 - val_acc: 0.5567\n",
      "Epoch 1604/3000\n",
      "700/700 [==============================] - 0s 276us/step - loss: 0.4822 - acc: 0.8471 - val_loss: 2.1018 - val_acc: 0.5600\n",
      "Epoch 1605/3000\n",
      "700/700 [==============================] - 0s 241us/step - loss: 0.4825 - acc: 0.8486 - val_loss: 2.0958 - val_acc: 0.5600\n",
      "Epoch 1606/3000\n",
      "700/700 [==============================] - 0s 259us/step - loss: 0.4819 - acc: 0.8471 - val_loss: 2.1008 - val_acc: 0.5567\n",
      "Epoch 1607/3000\n",
      "700/700 [==============================] - 0s 321us/step - loss: 0.4835 - acc: 0.8486 - val_loss: 2.0955 - val_acc: 0.5500\n",
      "Epoch 1608/3000\n",
      "700/700 [==============================] - 0s 228us/step - loss: 0.4817 - acc: 0.8471 - val_loss: 2.0868 - val_acc: 0.5433\n",
      "Epoch 1609/3000\n",
      "700/700 [==============================] - 0s 235us/step - loss: 0.4818 - acc: 0.8486 - val_loss: 2.1045 - val_acc: 0.5600\n",
      "Epoch 1610/3000\n",
      "700/700 [==============================] - 0s 234us/step - loss: 0.4816 - acc: 0.8471 - val_loss: 2.1044 - val_acc: 0.5533\n",
      "Epoch 1611/3000\n",
      "700/700 [==============================] - 0s 236us/step - loss: 0.4818 - acc: 0.8471 - val_loss: 2.1016 - val_acc: 0.5600\n",
      "Epoch 1612/3000\n",
      "700/700 [==============================] - 0s 239us/step - loss: 0.4814 - acc: 0.8486 - val_loss: 2.1053 - val_acc: 0.5567\n",
      "Epoch 1613/3000\n",
      "700/700 [==============================] - 0s 238us/step - loss: 0.4819 - acc: 0.8500 - val_loss: 2.1024 - val_acc: 0.5600\n",
      "Epoch 1614/3000\n",
      "700/700 [==============================] - 0s 241us/step - loss: 0.4810 - acc: 0.8500 - val_loss: 2.0962 - val_acc: 0.5533\n",
      "Epoch 1615/3000\n",
      "700/700 [==============================] - 0s 247us/step - loss: 0.4806 - acc: 0.8500 - val_loss: 2.0956 - val_acc: 0.5567\n",
      "Epoch 1616/3000\n",
      "700/700 [==============================] - 0s 240us/step - loss: 0.4815 - acc: 0.8486 - val_loss: 2.1065 - val_acc: 0.5533\n",
      "Epoch 1617/3000\n",
      "700/700 [==============================] - 0s 246us/step - loss: 0.4810 - acc: 0.8500 - val_loss: 2.1136 - val_acc: 0.5600\n",
      "Epoch 1618/3000\n",
      "700/700 [==============================] - 0s 233us/step - loss: 0.4808 - acc: 0.8471 - val_loss: 2.1084 - val_acc: 0.5600\n",
      "Epoch 1619/3000\n",
      "700/700 [==============================] - 0s 227us/step - loss: 0.4817 - acc: 0.8486 - val_loss: 2.1020 - val_acc: 0.5500\n",
      "Epoch 1620/3000\n",
      "700/700 [==============================] - 0s 225us/step - loss: 0.4806 - acc: 0.8500 - val_loss: 2.1111 - val_acc: 0.5600\n",
      "Epoch 1621/3000\n",
      "700/700 [==============================] - 0s 246us/step - loss: 0.4807 - acc: 0.8486 - val_loss: 2.1118 - val_acc: 0.5567\n",
      "Epoch 1622/3000\n",
      "700/700 [==============================] - 0s 226us/step - loss: 0.4799 - acc: 0.8486 - val_loss: 2.1069 - val_acc: 0.5533\n",
      "Epoch 1623/3000\n",
      "700/700 [==============================] - 0s 225us/step - loss: 0.4803 - acc: 0.8486 - val_loss: 2.1146 - val_acc: 0.5567\n",
      "Epoch 1624/3000\n",
      "700/700 [==============================] - 0s 233us/step - loss: 0.4803 - acc: 0.8500 - val_loss: 2.1117 - val_acc: 0.5533\n",
      "Epoch 1625/3000\n",
      "700/700 [==============================] - 0s 218us/step - loss: 0.4800 - acc: 0.8486 - val_loss: 2.1249 - val_acc: 0.5500\n",
      "Epoch 1626/3000\n",
      "700/700 [==============================] - 0s 235us/step - loss: 0.4803 - acc: 0.8471 - val_loss: 2.1053 - val_acc: 0.5433\n",
      "Epoch 1627/3000\n",
      "700/700 [==============================] - 0s 255us/step - loss: 0.4807 - acc: 0.8471 - val_loss: 2.1118 - val_acc: 0.5567\n",
      "Epoch 1628/3000\n",
      "700/700 [==============================] - 0s 224us/step - loss: 0.4795 - acc: 0.8500 - val_loss: 2.1127 - val_acc: 0.5567\n",
      "Epoch 1629/3000\n",
      "700/700 [==============================] - 0s 234us/step - loss: 0.4798 - acc: 0.8486 - val_loss: 2.1140 - val_acc: 0.5633\n",
      "Epoch 1630/3000\n",
      "700/700 [==============================] - 0s 243us/step - loss: 0.4800 - acc: 0.8471 - val_loss: 2.1159 - val_acc: 0.5567\n",
      "Epoch 1631/3000\n",
      "700/700 [==============================] - 0s 224us/step - loss: 0.4802 - acc: 0.8486 - val_loss: 2.1119 - val_acc: 0.5567\n",
      "Epoch 1632/3000\n",
      "700/700 [==============================] - 0s 227us/step - loss: 0.4805 - acc: 0.8471 - val_loss: 2.1109 - val_acc: 0.5567\n",
      "Epoch 1633/3000\n",
      "700/700 [==============================] - 0s 246us/step - loss: 0.4799 - acc: 0.8486 - val_loss: 2.1058 - val_acc: 0.5533\n",
      "Epoch 1634/3000\n",
      "700/700 [==============================] - 0s 235us/step - loss: 0.4788 - acc: 0.8486 - val_loss: 2.1120 - val_acc: 0.5600\n",
      "Epoch 1635/3000\n",
      "700/700 [==============================] - 0s 229us/step - loss: 0.4797 - acc: 0.8486 - val_loss: 2.1124 - val_acc: 0.5600\n",
      "Epoch 1636/3000\n",
      "700/700 [==============================] - 0s 225us/step - loss: 0.4796 - acc: 0.8471 - val_loss: 2.1310 - val_acc: 0.5600\n",
      "Epoch 1637/3000\n",
      "700/700 [==============================] - 0s 244us/step - loss: 0.4778 - acc: 0.8486 - val_loss: 2.1188 - val_acc: 0.5633\n",
      "Epoch 1638/3000\n",
      "700/700 [==============================] - 0s 230us/step - loss: 0.4778 - acc: 0.8500 - val_loss: 2.1268 - val_acc: 0.5500\n",
      "Epoch 1639/3000\n",
      "700/700 [==============================] - 0s 246us/step - loss: 0.4793 - acc: 0.8500 - val_loss: 2.1253 - val_acc: 0.5567\n",
      "Epoch 1640/3000\n",
      "700/700 [==============================] - 0s 289us/step - loss: 0.4793 - acc: 0.8471 - val_loss: 2.1280 - val_acc: 0.5500\n",
      "Epoch 1641/3000\n",
      "700/700 [==============================] - 0s 271us/step - loss: 0.4792 - acc: 0.8500 - val_loss: 2.1252 - val_acc: 0.5533\n",
      "Epoch 1642/3000\n",
      "700/700 [==============================] - 0s 234us/step - loss: 0.4789 - acc: 0.8486 - val_loss: 2.1091 - val_acc: 0.5567\n",
      "Epoch 1643/3000\n",
      "700/700 [==============================] - 0s 248us/step - loss: 0.4778 - acc: 0.8486 - val_loss: 2.1189 - val_acc: 0.5567\n",
      "Epoch 1644/3000\n",
      "700/700 [==============================] - 0s 233us/step - loss: 0.4798 - acc: 0.8486 - val_loss: 2.1170 - val_acc: 0.5567\n",
      "Epoch 1645/3000\n",
      "700/700 [==============================] - 0s 243us/step - loss: 0.4794 - acc: 0.8486 - val_loss: 2.1109 - val_acc: 0.5433\n",
      "Epoch 1646/3000\n",
      "700/700 [==============================] - 0s 238us/step - loss: 0.4792 - acc: 0.8486 - val_loss: 2.1232 - val_acc: 0.5533\n",
      "Epoch 1647/3000\n",
      "700/700 [==============================] - 0s 231us/step - loss: 0.4784 - acc: 0.8486 - val_loss: 2.1139 - val_acc: 0.5567\n",
      "Epoch 1648/3000\n",
      "700/700 [==============================] - 0s 238us/step - loss: 0.4783 - acc: 0.8486 - val_loss: 2.1170 - val_acc: 0.5567\n",
      "Epoch 1649/3000\n",
      "700/700 [==============================] - 0s 230us/step - loss: 0.4772 - acc: 0.8529 - val_loss: 2.1241 - val_acc: 0.5567\n",
      "Epoch 1650/3000\n",
      "700/700 [==============================] - 0s 239us/step - loss: 0.4780 - acc: 0.8486 - val_loss: 2.1165 - val_acc: 0.5467\n",
      "Epoch 1651/3000\n",
      "700/700 [==============================] - 0s 257us/step - loss: 0.4769 - acc: 0.8486 - val_loss: 2.1157 - val_acc: 0.5500\n",
      "Epoch 1652/3000\n",
      "700/700 [==============================] - 0s 237us/step - loss: 0.4777 - acc: 0.8486 - val_loss: 2.1295 - val_acc: 0.5600\n",
      "Epoch 1653/3000\n",
      "700/700 [==============================] - 0s 230us/step - loss: 0.4772 - acc: 0.8486 - val_loss: 2.1398 - val_acc: 0.5533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1654/3000\n",
      "700/700 [==============================] - 0s 237us/step - loss: 0.4778 - acc: 0.8471 - val_loss: 2.1274 - val_acc: 0.5500\n",
      "Epoch 1655/3000\n",
      "700/700 [==============================] - 0s 237us/step - loss: 0.4776 - acc: 0.8471 - val_loss: 2.1325 - val_acc: 0.5467\n",
      "Epoch 1656/3000\n",
      "700/700 [==============================] - 0s 252us/step - loss: 0.4761 - acc: 0.8500 - val_loss: 2.1355 - val_acc: 0.5467\n",
      "Epoch 1657/3000\n",
      "700/700 [==============================] - 0s 241us/step - loss: 0.4774 - acc: 0.8471 - val_loss: 2.1178 - val_acc: 0.5433\n",
      "Epoch 1658/3000\n",
      "700/700 [==============================] - 0s 241us/step - loss: 0.4770 - acc: 0.8486 - val_loss: 2.1290 - val_acc: 0.5500\n",
      "Epoch 1659/3000\n",
      "700/700 [==============================] - 0s 228us/step - loss: 0.4766 - acc: 0.8500 - val_loss: 2.1228 - val_acc: 0.5467\n",
      "Epoch 1660/3000\n",
      "700/700 [==============================] - 0s 228us/step - loss: 0.4771 - acc: 0.8486 - val_loss: 2.1381 - val_acc: 0.5533\n",
      "Epoch 1661/3000\n",
      "700/700 [==============================] - 0s 229us/step - loss: 0.4773 - acc: 0.8486 - val_loss: 2.1222 - val_acc: 0.5533\n",
      "Epoch 1662/3000\n",
      "700/700 [==============================] - 0s 230us/step - loss: 0.4769 - acc: 0.8486 - val_loss: 2.1205 - val_acc: 0.5500\n",
      "Epoch 1663/3000\n",
      "700/700 [==============================] - 0s 240us/step - loss: 0.4770 - acc: 0.8486 - val_loss: 2.1258 - val_acc: 0.5600\n",
      "Epoch 1664/3000\n",
      "700/700 [==============================] - 0s 231us/step - loss: 0.4759 - acc: 0.8486 - val_loss: 2.1243 - val_acc: 0.5467\n",
      "Epoch 1665/3000\n",
      "700/700 [==============================] - 0s 238us/step - loss: 0.4775 - acc: 0.8486 - val_loss: 2.1252 - val_acc: 0.5433\n",
      "Epoch 1666/3000\n",
      "700/700 [==============================] - 0s 238us/step - loss: 0.4765 - acc: 0.8486 - val_loss: 2.1357 - val_acc: 0.5533\n",
      "Epoch 1667/3000\n",
      "700/700 [==============================] - 0s 244us/step - loss: 0.4773 - acc: 0.8486 - val_loss: 2.1257 - val_acc: 0.5500\n",
      "Epoch 1668/3000\n",
      "700/700 [==============================] - 0s 235us/step - loss: 0.4756 - acc: 0.8471 - val_loss: 2.1234 - val_acc: 0.5500\n",
      "Epoch 1669/3000\n",
      "700/700 [==============================] - 0s 266us/step - loss: 0.4770 - acc: 0.8486 - val_loss: 2.1448 - val_acc: 0.5533\n",
      "Epoch 1670/3000\n",
      "700/700 [==============================] - 0s 244us/step - loss: 0.4768 - acc: 0.8486 - val_loss: 2.1338 - val_acc: 0.5600\n",
      "Epoch 1671/3000\n",
      "700/700 [==============================] - 0s 253us/step - loss: 0.4770 - acc: 0.8471 - val_loss: 2.1236 - val_acc: 0.5467\n",
      "Epoch 1672/3000\n",
      "700/700 [==============================] - 0s 240us/step - loss: 0.4771 - acc: 0.8486 - val_loss: 2.1457 - val_acc: 0.5533\n",
      "Epoch 1673/3000\n",
      "700/700 [==============================] - 0s 270us/step - loss: 0.4764 - acc: 0.8486 - val_loss: 2.1431 - val_acc: 0.5467\n",
      "Epoch 1674/3000\n",
      "700/700 [==============================] - 0s 281us/step - loss: 0.4759 - acc: 0.8471 - val_loss: 2.1289 - val_acc: 0.5533\n",
      "Epoch 1675/3000\n",
      "700/700 [==============================] - 0s 264us/step - loss: 0.4757 - acc: 0.8486 - val_loss: 2.1389 - val_acc: 0.5533\n",
      "Epoch 1676/3000\n",
      "700/700 [==============================] - 0s 270us/step - loss: 0.4763 - acc: 0.8500 - val_loss: 2.1325 - val_acc: 0.5567\n",
      "Epoch 1677/3000\n",
      "700/700 [==============================] - 0s 240us/step - loss: 0.4756 - acc: 0.8486 - val_loss: 2.1337 - val_acc: 0.5467\n",
      "Epoch 1678/3000\n",
      "700/700 [==============================] - 0s 261us/step - loss: 0.4753 - acc: 0.8500 - val_loss: 2.1339 - val_acc: 0.5567\n",
      "Epoch 1679/3000\n",
      "700/700 [==============================] - 0s 253us/step - loss: 0.4750 - acc: 0.8500 - val_loss: 2.1438 - val_acc: 0.5500\n",
      "Epoch 1680/3000\n",
      "700/700 [==============================] - 0s 257us/step - loss: 0.4749 - acc: 0.8500 - val_loss: 2.1386 - val_acc: 0.5467\n",
      "Epoch 1681/3000\n",
      "700/700 [==============================] - 0s 260us/step - loss: 0.4751 - acc: 0.8486 - val_loss: 2.1411 - val_acc: 0.5600\n",
      "Epoch 1682/3000\n",
      "700/700 [==============================] - 0s 272us/step - loss: 0.4749 - acc: 0.8486 - val_loss: 2.1394 - val_acc: 0.5500\n",
      "Epoch 1683/3000\n",
      "700/700 [==============================] - 0s 270us/step - loss: 0.4768 - acc: 0.8486 - val_loss: 2.1384 - val_acc: 0.5500\n",
      "Epoch 1684/3000\n",
      "700/700 [==============================] - 0s 269us/step - loss: 0.4747 - acc: 0.8500 - val_loss: 2.1413 - val_acc: 0.5567\n",
      "Epoch 1685/3000\n",
      "700/700 [==============================] - 0s 334us/step - loss: 0.4745 - acc: 0.8486 - val_loss: 2.1506 - val_acc: 0.5500\n",
      "Epoch 1686/3000\n",
      "700/700 [==============================] - 0s 331us/step - loss: 0.4734 - acc: 0.8500 - val_loss: 2.1534 - val_acc: 0.5567\n",
      "Epoch 1687/3000\n",
      "700/700 [==============================] - 0s 311us/step - loss: 0.4736 - acc: 0.8486 - val_loss: 2.1721 - val_acc: 0.5367\n",
      "Epoch 1688/3000\n",
      "700/700 [==============================] - 0s 290us/step - loss: 0.4741 - acc: 0.8486 - val_loss: 2.1393 - val_acc: 0.5500\n",
      "Epoch 1689/3000\n",
      "700/700 [==============================] - 0s 293us/step - loss: 0.4741 - acc: 0.8486 - val_loss: 2.1460 - val_acc: 0.5567\n",
      "Epoch 1690/3000\n",
      "700/700 [==============================] - 0s 303us/step - loss: 0.4745 - acc: 0.8486 - val_loss: 2.1449 - val_acc: 0.5433\n",
      "Epoch 1691/3000\n",
      "700/700 [==============================] - 0s 287us/step - loss: 0.4735 - acc: 0.8500 - val_loss: 2.1481 - val_acc: 0.5533\n",
      "Epoch 1692/3000\n",
      "700/700 [==============================] - 0s 326us/step - loss: 0.4735 - acc: 0.8500 - val_loss: 2.1504 - val_acc: 0.5433\n",
      "Epoch 1693/3000\n",
      "700/700 [==============================] - 0s 291us/step - loss: 0.4749 - acc: 0.8471 - val_loss: 2.1499 - val_acc: 0.5467\n",
      "Epoch 1694/3000\n",
      "700/700 [==============================] - 0s 318us/step - loss: 0.4738 - acc: 0.8500 - val_loss: 2.1478 - val_acc: 0.5500\n",
      "Epoch 1695/3000\n",
      "700/700 [==============================] - 0s 335us/step - loss: 0.4740 - acc: 0.8500 - val_loss: 2.1423 - val_acc: 0.5533\n",
      "Epoch 1696/3000\n",
      "700/700 [==============================] - 0s 356us/step - loss: 0.4737 - acc: 0.8500 - val_loss: 2.1506 - val_acc: 0.5567\n",
      "Epoch 1697/3000\n",
      "700/700 [==============================] - 0s 336us/step - loss: 0.4741 - acc: 0.8500 - val_loss: 2.1551 - val_acc: 0.5433\n",
      "Epoch 1698/3000\n",
      "700/700 [==============================] - 0s 329us/step - loss: 0.4730 - acc: 0.8486 - val_loss: 2.1429 - val_acc: 0.5567\n",
      "Epoch 1699/3000\n",
      "700/700 [==============================] - 0s 318us/step - loss: 0.4745 - acc: 0.8486 - val_loss: 2.1599 - val_acc: 0.5433\n",
      "Epoch 1700/3000\n",
      "700/700 [==============================] - 0s 316us/step - loss: 0.4732 - acc: 0.8486 - val_loss: 2.1528 - val_acc: 0.5533\n",
      "Epoch 1701/3000\n",
      "700/700 [==============================] - 0s 297us/step - loss: 0.4720 - acc: 0.8500 - val_loss: 2.1491 - val_acc: 0.5500\n",
      "Epoch 1702/3000\n",
      "700/700 [==============================] - 0s 297us/step - loss: 0.4734 - acc: 0.8486 - val_loss: 2.1329 - val_acc: 0.5400\n",
      "Epoch 1703/3000\n",
      "700/700 [==============================] - 0s 309us/step - loss: 0.4732 - acc: 0.8500 - val_loss: 2.1489 - val_acc: 0.5500\n",
      "Epoch 1704/3000\n",
      "700/700 [==============================] - 0s 315us/step - loss: 0.4718 - acc: 0.8486 - val_loss: 2.1465 - val_acc: 0.5433\n",
      "Epoch 1705/3000\n",
      "700/700 [==============================] - 0s 297us/step - loss: 0.4730 - acc: 0.8500 - val_loss: 2.1555 - val_acc: 0.5567\n",
      "Epoch 1706/3000\n",
      "700/700 [==============================] - 0s 308us/step - loss: 0.4727 - acc: 0.8486 - val_loss: 2.1658 - val_acc: 0.5400\n",
      "Epoch 1707/3000\n",
      "700/700 [==============================] - 0s 288us/step - loss: 0.4731 - acc: 0.8486 - val_loss: 2.1442 - val_acc: 0.5533\n",
      "Epoch 1708/3000\n",
      "700/700 [==============================] - 0s 319us/step - loss: 0.4718 - acc: 0.8500 - val_loss: 2.1455 - val_acc: 0.5500\n",
      "Epoch 1709/3000\n",
      "700/700 [==============================] - 0s 343us/step - loss: 0.4731 - acc: 0.8500 - val_loss: 2.1575 - val_acc: 0.5433\n",
      "Epoch 1710/3000\n",
      "700/700 [==============================] - 0s 302us/step - loss: 0.4723 - acc: 0.8471 - val_loss: 2.1525 - val_acc: 0.5500\n",
      "Epoch 1711/3000\n",
      "700/700 [==============================] - 0s 287us/step - loss: 0.4711 - acc: 0.8500 - val_loss: 2.1534 - val_acc: 0.5467\n",
      "Epoch 1712/3000\n",
      "700/700 [==============================] - 0s 313us/step - loss: 0.4727 - acc: 0.8486 - val_loss: 2.1470 - val_acc: 0.5500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1713/3000\n",
      "700/700 [==============================] - 0s 293us/step - loss: 0.4724 - acc: 0.8500 - val_loss: 2.1472 - val_acc: 0.5533\n",
      "Epoch 1714/3000\n",
      "700/700 [==============================] - 0s 311us/step - loss: 0.4725 - acc: 0.8471 - val_loss: 2.1637 - val_acc: 0.5400\n",
      "Epoch 1715/3000\n",
      "700/700 [==============================] - 0s 289us/step - loss: 0.4746 - acc: 0.8486 - val_loss: 2.1492 - val_acc: 0.5567\n",
      "Epoch 1716/3000\n",
      "700/700 [==============================] - 0s 301us/step - loss: 0.4724 - acc: 0.8486 - val_loss: 2.1530 - val_acc: 0.5533\n",
      "Epoch 1717/3000\n",
      "700/700 [==============================] - 0s 301us/step - loss: 0.4716 - acc: 0.8486 - val_loss: 2.1466 - val_acc: 0.5533\n",
      "Epoch 1718/3000\n",
      "700/700 [==============================] - 0s 300us/step - loss: 0.4719 - acc: 0.8500 - val_loss: 2.1570 - val_acc: 0.5500\n",
      "Epoch 1719/3000\n",
      "700/700 [==============================] - 0s 295us/step - loss: 0.4720 - acc: 0.8486 - val_loss: 2.1509 - val_acc: 0.5400\n",
      "Epoch 1720/3000\n",
      "700/700 [==============================] - 0s 292us/step - loss: 0.4724 - acc: 0.8486 - val_loss: 2.1391 - val_acc: 0.5433\n",
      "Epoch 1721/3000\n",
      "700/700 [==============================] - 0s 310us/step - loss: 0.4705 - acc: 0.8500 - val_loss: 2.1585 - val_acc: 0.5567\n",
      "Epoch 1722/3000\n",
      "700/700 [==============================] - 0s 306us/step - loss: 0.4710 - acc: 0.8486 - val_loss: 2.1532 - val_acc: 0.5500\n",
      "Epoch 1723/3000\n",
      "700/700 [==============================] - 0s 307us/step - loss: 0.4711 - acc: 0.8486 - val_loss: 2.1611 - val_acc: 0.5500\n",
      "Epoch 1724/3000\n",
      "700/700 [==============================] - 0s 298us/step - loss: 0.4714 - acc: 0.8500 - val_loss: 2.1523 - val_acc: 0.5500\n",
      "Epoch 1725/3000\n",
      "700/700 [==============================] - 0s 263us/step - loss: 0.4715 - acc: 0.8500 - val_loss: 2.1520 - val_acc: 0.5467\n",
      "Epoch 1726/3000\n",
      "700/700 [==============================] - 0s 280us/step - loss: 0.4708 - acc: 0.8500 - val_loss: 2.1551 - val_acc: 0.5500\n",
      "Epoch 1727/3000\n",
      "700/700 [==============================] - 0s 263us/step - loss: 0.4706 - acc: 0.8471 - val_loss: 2.1631 - val_acc: 0.5500\n",
      "Epoch 1728/3000\n",
      "700/700 [==============================] - 0s 267us/step - loss: 0.4700 - acc: 0.8500 - val_loss: 2.1557 - val_acc: 0.5500\n",
      "Epoch 1729/3000\n",
      "700/700 [==============================] - 0s 265us/step - loss: 0.4709 - acc: 0.8514 - val_loss: 2.1648 - val_acc: 0.5467\n",
      "Epoch 1730/3000\n",
      "700/700 [==============================] - 0s 260us/step - loss: 0.4701 - acc: 0.8471 - val_loss: 2.1579 - val_acc: 0.5567\n",
      "Epoch 1731/3000\n",
      "700/700 [==============================] - 0s 253us/step - loss: 0.4707 - acc: 0.8486 - val_loss: 2.1583 - val_acc: 0.5500\n",
      "Epoch 1732/3000\n",
      "700/700 [==============================] - 0s 319us/step - loss: 0.4702 - acc: 0.8486 - val_loss: 2.1797 - val_acc: 0.5433\n",
      "Epoch 1733/3000\n",
      "700/700 [==============================] - 0s 259us/step - loss: 0.4708 - acc: 0.8500 - val_loss: 2.1648 - val_acc: 0.5533\n",
      "Epoch 1734/3000\n",
      "700/700 [==============================] - 0s 256us/step - loss: 0.4695 - acc: 0.8500 - val_loss: 2.1648 - val_acc: 0.5467\n",
      "Epoch 1735/3000\n",
      "700/700 [==============================] - 0s 259us/step - loss: 0.4699 - acc: 0.8500 - val_loss: 2.1514 - val_acc: 0.5400\n",
      "Epoch 1736/3000\n",
      "700/700 [==============================] - 0s 256us/step - loss: 0.4692 - acc: 0.8514 - val_loss: 2.1692 - val_acc: 0.5500\n",
      "Epoch 1737/3000\n",
      "700/700 [==============================] - 0s 270us/step - loss: 0.4701 - acc: 0.8500 - val_loss: 2.1780 - val_acc: 0.5433\n",
      "Epoch 1738/3000\n",
      "700/700 [==============================] - 0s 238us/step - loss: 0.4700 - acc: 0.8486 - val_loss: 2.1807 - val_acc: 0.5400\n",
      "Epoch 1739/3000\n",
      "700/700 [==============================] - 0s 246us/step - loss: 0.4706 - acc: 0.8486 - val_loss: 2.1736 - val_acc: 0.5467\n",
      "Epoch 1740/3000\n",
      "700/700 [==============================] - 0s 233us/step - loss: 0.4712 - acc: 0.8486 - val_loss: 2.1676 - val_acc: 0.5500\n",
      "Epoch 1741/3000\n",
      "700/700 [==============================] - 0s 239us/step - loss: 0.4700 - acc: 0.8486 - val_loss: 2.1695 - val_acc: 0.5500\n",
      "Epoch 1742/3000\n",
      "700/700 [==============================] - 0s 236us/step - loss: 0.4696 - acc: 0.8529 - val_loss: 2.1656 - val_acc: 0.5533\n",
      "Epoch 1743/3000\n",
      "700/700 [==============================] - 0s 260us/step - loss: 0.4690 - acc: 0.8500 - val_loss: 2.1726 - val_acc: 0.5433\n",
      "Epoch 1744/3000\n",
      "700/700 [==============================] - 0s 237us/step - loss: 0.4691 - acc: 0.8514 - val_loss: 2.1643 - val_acc: 0.5500\n",
      "Epoch 1745/3000\n",
      "700/700 [==============================] - 0s 245us/step - loss: 0.4679 - acc: 0.8500 - val_loss: 2.1724 - val_acc: 0.5500\n",
      "Epoch 1746/3000\n",
      "700/700 [==============================] - 0s 234us/step - loss: 0.4686 - acc: 0.8514 - val_loss: 2.1638 - val_acc: 0.5500\n",
      "Epoch 1747/3000\n",
      "700/700 [==============================] - 0s 248us/step - loss: 0.4689 - acc: 0.8486 - val_loss: 2.1664 - val_acc: 0.5433\n",
      "Epoch 1748/3000\n",
      "700/700 [==============================] - 0s 227us/step - loss: 0.4692 - acc: 0.8500 - val_loss: 2.1635 - val_acc: 0.5500\n",
      "Epoch 1749/3000\n",
      "700/700 [==============================] - 0s 243us/step - loss: 0.4688 - acc: 0.8500 - val_loss: 2.1625 - val_acc: 0.5467\n",
      "Epoch 1750/3000\n",
      "700/700 [==============================] - 0s 230us/step - loss: 0.4684 - acc: 0.8500 - val_loss: 2.1790 - val_acc: 0.5567\n",
      "Epoch 1751/3000\n",
      "700/700 [==============================] - 0s 227us/step - loss: 0.4691 - acc: 0.8486 - val_loss: 2.1830 - val_acc: 0.5433\n",
      "Epoch 1752/3000\n",
      "700/700 [==============================] - 0s 243us/step - loss: 0.4689 - acc: 0.8514 - val_loss: 2.1690 - val_acc: 0.5533\n",
      "Epoch 1753/3000\n",
      "700/700 [==============================] - 0s 214us/step - loss: 0.4687 - acc: 0.8514 - val_loss: 2.1758 - val_acc: 0.5533\n",
      "Epoch 1754/3000\n",
      "700/700 [==============================] - 0s 218us/step - loss: 0.4679 - acc: 0.8500 - val_loss: 2.1777 - val_acc: 0.5500\n",
      "Epoch 1755/3000\n",
      "700/700 [==============================] - 0s 237us/step - loss: 0.4682 - acc: 0.8500 - val_loss: 2.1698 - val_acc: 0.5467\n",
      "Epoch 1756/3000\n",
      "700/700 [==============================] - 0s 211us/step - loss: 0.4687 - acc: 0.8514 - val_loss: 2.1736 - val_acc: 0.5500\n",
      "Epoch 1757/3000\n",
      "700/700 [==============================] - 0s 227us/step - loss: 0.4679 - acc: 0.8486 - val_loss: 2.1792 - val_acc: 0.5467\n",
      "Epoch 1758/3000\n",
      "700/700 [==============================] - 0s 218us/step - loss: 0.4684 - acc: 0.8500 - val_loss: 2.1789 - val_acc: 0.5467\n",
      "Epoch 1759/3000\n",
      "700/700 [==============================] - 0s 218us/step - loss: 0.4679 - acc: 0.8514 - val_loss: 2.1952 - val_acc: 0.5433\n",
      "Epoch 1760/3000\n",
      "700/700 [==============================] - 0s 217us/step - loss: 0.4681 - acc: 0.8500 - val_loss: 2.1732 - val_acc: 0.5433\n",
      "Epoch 1761/3000\n",
      "700/700 [==============================] - 0s 231us/step - loss: 0.4692 - acc: 0.8514 - val_loss: 2.1886 - val_acc: 0.5500\n",
      "Epoch 1762/3000\n",
      "700/700 [==============================] - 0s 212us/step - loss: 0.4684 - acc: 0.8486 - val_loss: 2.1782 - val_acc: 0.5533\n",
      "Epoch 1763/3000\n",
      "700/700 [==============================] - 0s 230us/step - loss: 0.4684 - acc: 0.8486 - val_loss: 2.1840 - val_acc: 0.5467\n",
      "Epoch 1764/3000\n",
      "700/700 [==============================] - 0s 207us/step - loss: 0.4672 - acc: 0.8500 - val_loss: 2.1704 - val_acc: 0.5433\n",
      "Epoch 1765/3000\n",
      "700/700 [==============================] - 0s 221us/step - loss: 0.4669 - acc: 0.8500 - val_loss: 2.1752 - val_acc: 0.5533\n",
      "Epoch 1766/3000\n",
      "700/700 [==============================] - 0s 225us/step - loss: 0.4690 - acc: 0.8486 - val_loss: 2.1806 - val_acc: 0.5533\n",
      "Epoch 1767/3000\n",
      "700/700 [==============================] - 0s 218us/step - loss: 0.4670 - acc: 0.8500 - val_loss: 2.1770 - val_acc: 0.5500\n",
      "Epoch 1768/3000\n",
      "700/700 [==============================] - 0s 233us/step - loss: 0.4676 - acc: 0.8500 - val_loss: 2.1882 - val_acc: 0.5500\n",
      "Epoch 1769/3000\n",
      "700/700 [==============================] - 0s 225us/step - loss: 0.4668 - acc: 0.8486 - val_loss: 2.1760 - val_acc: 0.5467\n",
      "Epoch 1770/3000\n",
      "700/700 [==============================] - 0s 223us/step - loss: 0.4674 - acc: 0.8486 - val_loss: 2.1820 - val_acc: 0.5500\n",
      "Epoch 1771/3000\n",
      "700/700 [==============================] - 0s 211us/step - loss: 0.4678 - acc: 0.8500 - val_loss: 2.1969 - val_acc: 0.5367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1772/3000\n",
      "700/700 [==============================] - 0s 222us/step - loss: 0.4659 - acc: 0.8500 - val_loss: 2.1827 - val_acc: 0.5433\n",
      "Epoch 1773/3000\n",
      "700/700 [==============================] - 0s 214us/step - loss: 0.4672 - acc: 0.8500 - val_loss: 2.1714 - val_acc: 0.5367\n",
      "Epoch 1774/3000\n",
      "700/700 [==============================] - 0s 216us/step - loss: 0.4672 - acc: 0.8486 - val_loss: 2.1888 - val_acc: 0.5500\n",
      "Epoch 1775/3000\n",
      "700/700 [==============================] - 0s 217us/step - loss: 0.4669 - acc: 0.8514 - val_loss: 2.1832 - val_acc: 0.5533\n",
      "Epoch 1776/3000\n",
      "700/700 [==============================] - 0s 205us/step - loss: 0.4661 - acc: 0.8500 - val_loss: 2.1913 - val_acc: 0.5467\n",
      "Epoch 1777/3000\n",
      "700/700 [==============================] - 0s 208us/step - loss: 0.4659 - acc: 0.8500 - val_loss: 2.1875 - val_acc: 0.5533\n",
      "Epoch 1778/3000\n",
      "700/700 [==============================] - 0s 219us/step - loss: 0.4665 - acc: 0.8500 - val_loss: 2.1934 - val_acc: 0.5467\n",
      "Epoch 1779/3000\n",
      "700/700 [==============================] - 0s 210us/step - loss: 0.4660 - acc: 0.8500 - val_loss: 2.1876 - val_acc: 0.5500\n",
      "Epoch 1780/3000\n",
      "700/700 [==============================] - 0s 214us/step - loss: 0.4661 - acc: 0.8514 - val_loss: 2.1932 - val_acc: 0.5400\n",
      "Epoch 1781/3000\n",
      "700/700 [==============================] - 0s 218us/step - loss: 0.4656 - acc: 0.8486 - val_loss: 2.1875 - val_acc: 0.5533\n",
      "Epoch 1782/3000\n",
      "700/700 [==============================] - 0s 205us/step - loss: 0.4666 - acc: 0.8486 - val_loss: 2.1914 - val_acc: 0.5467\n",
      "Epoch 1783/3000\n",
      "700/700 [==============================] - 0s 213us/step - loss: 0.4654 - acc: 0.8514 - val_loss: 2.1841 - val_acc: 0.5467\n",
      "Epoch 1784/3000\n",
      "700/700 [==============================] - 0s 209us/step - loss: 0.4654 - acc: 0.8529 - val_loss: 2.1888 - val_acc: 0.5533\n",
      "Epoch 1785/3000\n",
      "700/700 [==============================] - 0s 225us/step - loss: 0.4658 - acc: 0.8486 - val_loss: 2.2012 - val_acc: 0.5400\n",
      "Epoch 1786/3000\n",
      "700/700 [==============================] - 0s 209us/step - loss: 0.4655 - acc: 0.8500 - val_loss: 2.2014 - val_acc: 0.5467\n",
      "Epoch 1787/3000\n",
      "700/700 [==============================] - 0s 208us/step - loss: 0.4663 - acc: 0.8543 - val_loss: 2.1993 - val_acc: 0.5433\n",
      "Epoch 1788/3000\n",
      "700/700 [==============================] - 0s 202us/step - loss: 0.4667 - acc: 0.8500 - val_loss: 2.1973 - val_acc: 0.5533\n",
      "Epoch 1789/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 0.4648 - acc: 0.8514 - val_loss: 2.1998 - val_acc: 0.5467\n",
      "Epoch 1790/3000\n",
      "700/700 [==============================] - 0s 206us/step - loss: 0.4644 - acc: 0.8500 - val_loss: 2.1890 - val_acc: 0.5467\n",
      "Epoch 1791/3000\n",
      "700/700 [==============================] - 0s 226us/step - loss: 0.4657 - acc: 0.8486 - val_loss: 2.2013 - val_acc: 0.5433\n",
      "Epoch 1792/3000\n",
      "700/700 [==============================] - 0s 230us/step - loss: 0.4648 - acc: 0.8486 - val_loss: 2.1912 - val_acc: 0.5467\n",
      "Epoch 1793/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 0.4648 - acc: 0.8514 - val_loss: 2.1943 - val_acc: 0.5500\n",
      "Epoch 1794/3000\n",
      "700/700 [==============================] - 0s 224us/step - loss: 0.4658 - acc: 0.8500 - val_loss: 2.1984 - val_acc: 0.5500\n",
      "Epoch 1795/3000\n",
      "700/700 [==============================] - 0s 179us/step - loss: 0.4646 - acc: 0.8500 - val_loss: 2.2041 - val_acc: 0.5433\n",
      "Epoch 1796/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4658 - acc: 0.8486 - val_loss: 2.1869 - val_acc: 0.5400\n",
      "Epoch 1797/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.4651 - acc: 0.8543 - val_loss: 2.1926 - val_acc: 0.5500\n",
      "Epoch 1798/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4650 - acc: 0.8514 - val_loss: 2.2049 - val_acc: 0.5500\n",
      "Epoch 1799/3000\n",
      "700/700 [==============================] - 0s 214us/step - loss: 0.4644 - acc: 0.8500 - val_loss: 2.1928 - val_acc: 0.5467\n",
      "Epoch 1800/3000\n",
      "700/700 [==============================] - 0s 207us/step - loss: 0.4651 - acc: 0.8514 - val_loss: 2.2048 - val_acc: 0.5433\n",
      "Epoch 1801/3000\n",
      "700/700 [==============================] - 0s 217us/step - loss: 0.4650 - acc: 0.8486 - val_loss: 2.1993 - val_acc: 0.5500\n",
      "Epoch 1802/3000\n",
      "700/700 [==============================] - 0s 200us/step - loss: 0.4642 - acc: 0.8486 - val_loss: 2.1776 - val_acc: 0.5367\n",
      "Epoch 1803/3000\n",
      "700/700 [==============================] - 0s 204us/step - loss: 0.4656 - acc: 0.8500 - val_loss: 2.1931 - val_acc: 0.5433\n",
      "Epoch 1804/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4649 - acc: 0.8500 - val_loss: 2.2057 - val_acc: 0.5400\n",
      "Epoch 1805/3000\n",
      "700/700 [==============================] - 0s 193us/step - loss: 0.4646 - acc: 0.8514 - val_loss: 2.1963 - val_acc: 0.5467\n",
      "Epoch 1806/3000\n",
      "700/700 [==============================] - 0s 202us/step - loss: 0.4641 - acc: 0.8514 - val_loss: 2.1954 - val_acc: 0.5400\n",
      "Epoch 1807/3000\n",
      "700/700 [==============================] - 0s 201us/step - loss: 0.4655 - acc: 0.8514 - val_loss: 2.1933 - val_acc: 0.5467\n",
      "Epoch 1808/3000\n",
      "700/700 [==============================] - 0s 214us/step - loss: 0.4641 - acc: 0.8529 - val_loss: 2.2149 - val_acc: 0.5400\n",
      "Epoch 1809/3000\n",
      "700/700 [==============================] - 0s 306us/step - loss: 0.4632 - acc: 0.8500 - val_loss: 2.2085 - val_acc: 0.5467\n",
      "Epoch 1810/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 0.4640 - acc: 0.8514 - val_loss: 2.2146 - val_acc: 0.5433\n",
      "Epoch 1811/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4628 - acc: 0.8514 - val_loss: 2.2124 - val_acc: 0.5500\n",
      "Epoch 1812/3000\n",
      "700/700 [==============================] - 0s 204us/step - loss: 0.4630 - acc: 0.8514 - val_loss: 2.2004 - val_acc: 0.5400\n",
      "Epoch 1813/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 0.4635 - acc: 0.8500 - val_loss: 2.2163 - val_acc: 0.5400\n",
      "Epoch 1814/3000\n",
      "700/700 [==============================] - 0s 210us/step - loss: 0.4639 - acc: 0.8500 - val_loss: 2.2159 - val_acc: 0.5400\n",
      "Epoch 1815/3000\n",
      "700/700 [==============================] - 0s 193us/step - loss: 0.4628 - acc: 0.8500 - val_loss: 2.2081 - val_acc: 0.5400\n",
      "Epoch 1816/3000\n",
      "700/700 [==============================] - 0s 225us/step - loss: 0.4626 - acc: 0.8500 - val_loss: 2.1966 - val_acc: 0.5533\n",
      "Epoch 1817/3000\n",
      "700/700 [==============================] - 0s 202us/step - loss: 0.4625 - acc: 0.8529 - val_loss: 2.2118 - val_acc: 0.5467\n",
      "Epoch 1818/3000\n",
      "700/700 [==============================] - 0s 201us/step - loss: 0.4628 - acc: 0.8514 - val_loss: 2.2053 - val_acc: 0.5533\n",
      "Epoch 1819/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.4637 - acc: 0.8500 - val_loss: 2.2168 - val_acc: 0.5500\n",
      "Epoch 1820/3000\n",
      "700/700 [==============================] - 0s 198us/step - loss: 0.4626 - acc: 0.8514 - val_loss: 2.2070 - val_acc: 0.5467\n",
      "Epoch 1821/3000\n",
      "700/700 [==============================] - 0s 201us/step - loss: 0.4624 - acc: 0.8514 - val_loss: 2.2254 - val_acc: 0.5433\n",
      "Epoch 1822/3000\n",
      "700/700 [==============================] - 0s 187us/step - loss: 0.4625 - acc: 0.8514 - val_loss: 2.2074 - val_acc: 0.5400\n",
      "Epoch 1823/3000\n",
      "700/700 [==============================] - 0s 198us/step - loss: 0.4623 - acc: 0.8500 - val_loss: 2.2052 - val_acc: 0.5400\n",
      "Epoch 1824/3000\n",
      "700/700 [==============================] - 0s 181us/step - loss: 0.4626 - acc: 0.8529 - val_loss: 2.2166 - val_acc: 0.5400\n",
      "Epoch 1825/3000\n",
      "700/700 [==============================] - 0s 184us/step - loss: 0.4626 - acc: 0.8486 - val_loss: 2.2159 - val_acc: 0.5400\n",
      "Epoch 1826/3000\n",
      "700/700 [==============================] - 0s 186us/step - loss: 0.4622 - acc: 0.8486 - val_loss: 2.2003 - val_acc: 0.5400\n",
      "Epoch 1827/3000\n",
      "700/700 [==============================] - 0s 189us/step - loss: 0.4616 - acc: 0.8529 - val_loss: 2.2065 - val_acc: 0.5367\n",
      "Epoch 1828/3000\n",
      "700/700 [==============================] - 0s 192us/step - loss: 0.4621 - acc: 0.8529 - val_loss: 2.2173 - val_acc: 0.5467\n",
      "Epoch 1829/3000\n",
      "700/700 [==============================] - 0s 193us/step - loss: 0.4629 - acc: 0.8514 - val_loss: 2.2167 - val_acc: 0.5533\n",
      "Epoch 1830/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.4621 - acc: 0.8500 - val_loss: 2.2259 - val_acc: 0.5433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1831/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4616 - acc: 0.8500 - val_loss: 2.2269 - val_acc: 0.5400\n",
      "Epoch 1832/3000\n",
      "700/700 [==============================] - 0s 189us/step - loss: 0.4616 - acc: 0.8500 - val_loss: 2.2261 - val_acc: 0.5400\n",
      "Epoch 1833/3000\n",
      "700/700 [==============================] - 0s 201us/step - loss: 0.4622 - acc: 0.8500 - val_loss: 2.2180 - val_acc: 0.5400\n",
      "Epoch 1834/3000\n",
      "700/700 [==============================] - 0s 202us/step - loss: 0.4615 - acc: 0.8500 - val_loss: 2.2231 - val_acc: 0.5433\n",
      "Epoch 1835/3000\n",
      "700/700 [==============================] - 0s 182us/step - loss: 0.4615 - acc: 0.8500 - val_loss: 2.2187 - val_acc: 0.5467\n",
      "Epoch 1836/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4625 - acc: 0.8514 - val_loss: 2.2196 - val_acc: 0.5400\n",
      "Epoch 1837/3000\n",
      "700/700 [==============================] - 0s 188us/step - loss: 0.4610 - acc: 0.8529 - val_loss: 2.2134 - val_acc: 0.5400\n",
      "Epoch 1838/3000\n",
      "700/700 [==============================] - 0s 188us/step - loss: 0.4615 - acc: 0.8500 - val_loss: 2.2242 - val_acc: 0.5400\n",
      "Epoch 1839/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.4618 - acc: 0.8500 - val_loss: 2.2237 - val_acc: 0.5467\n",
      "Epoch 1840/3000\n",
      "700/700 [==============================] - 0s 190us/step - loss: 0.4619 - acc: 0.8500 - val_loss: 2.2117 - val_acc: 0.5400\n",
      "Epoch 1841/3000\n",
      "700/700 [==============================] - 0s 200us/step - loss: 0.4618 - acc: 0.8529 - val_loss: 2.2210 - val_acc: 0.5367\n",
      "Epoch 1842/3000\n",
      "700/700 [==============================] - 0s 202us/step - loss: 0.4612 - acc: 0.8486 - val_loss: 2.2051 - val_acc: 0.5333\n",
      "Epoch 1843/3000\n",
      "700/700 [==============================] - 0s 180us/step - loss: 0.4614 - acc: 0.8514 - val_loss: 2.2366 - val_acc: 0.5400\n",
      "Epoch 1844/3000\n",
      "700/700 [==============================] - 0s 192us/step - loss: 0.4612 - acc: 0.8529 - val_loss: 2.2276 - val_acc: 0.5400\n",
      "Epoch 1845/3000\n",
      "700/700 [==============================] - 0s 178us/step - loss: 0.4603 - acc: 0.8529 - val_loss: 2.2370 - val_acc: 0.5400\n",
      "Epoch 1846/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4610 - acc: 0.8529 - val_loss: 2.2190 - val_acc: 0.5367\n",
      "Epoch 1847/3000\n",
      "700/700 [==============================] - 0s 181us/step - loss: 0.4611 - acc: 0.8500 - val_loss: 2.2301 - val_acc: 0.5467\n",
      "Epoch 1848/3000\n",
      "700/700 [==============================] - 0s 178us/step - loss: 0.4607 - acc: 0.8514 - val_loss: 2.2235 - val_acc: 0.5467\n",
      "Epoch 1849/3000\n",
      "700/700 [==============================] - 0s 178us/step - loss: 0.4618 - acc: 0.8500 - val_loss: 2.2166 - val_acc: 0.5400\n",
      "Epoch 1850/3000\n",
      "700/700 [==============================] - 0s 186us/step - loss: 0.4599 - acc: 0.8543 - val_loss: 2.2435 - val_acc: 0.5400\n",
      "Epoch 1851/3000\n",
      "700/700 [==============================] - 0s 223us/step - loss: 0.4610 - acc: 0.8486 - val_loss: 2.2394 - val_acc: 0.5400\n",
      "Epoch 1852/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4608 - acc: 0.8514 - val_loss: 2.2288 - val_acc: 0.5367\n",
      "Epoch 1853/3000\n",
      "700/700 [==============================] - 0s 190us/step - loss: 0.4615 - acc: 0.8514 - val_loss: 2.2228 - val_acc: 0.5467\n",
      "Epoch 1854/3000\n",
      "700/700 [==============================] - 0s 191us/step - loss: 0.4606 - acc: 0.8529 - val_loss: 2.2415 - val_acc: 0.5400\n",
      "Epoch 1855/3000\n",
      "700/700 [==============================] - 0s 180us/step - loss: 0.4602 - acc: 0.8514 - val_loss: 2.2261 - val_acc: 0.5433\n",
      "Epoch 1856/3000\n",
      "700/700 [==============================] - 0s 198us/step - loss: 0.4597 - acc: 0.8500 - val_loss: 2.2279 - val_acc: 0.5400\n",
      "Epoch 1857/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.4613 - acc: 0.8486 - val_loss: 2.2308 - val_acc: 0.5367\n",
      "Epoch 1858/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4596 - acc: 0.8500 - val_loss: 2.2391 - val_acc: 0.5400\n",
      "Epoch 1859/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.4598 - acc: 0.8529 - val_loss: 2.2562 - val_acc: 0.5400\n",
      "Epoch 1860/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4608 - acc: 0.8514 - val_loss: 2.2385 - val_acc: 0.5400\n",
      "Epoch 1861/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.4592 - acc: 0.8529 - val_loss: 2.2267 - val_acc: 0.5367\n",
      "Epoch 1862/3000\n",
      "700/700 [==============================] - 0s 176us/step - loss: 0.4600 - acc: 0.8500 - val_loss: 2.2347 - val_acc: 0.5400\n",
      "Epoch 1863/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4600 - acc: 0.8500 - val_loss: 2.2443 - val_acc: 0.5433\n",
      "Epoch 1864/3000\n",
      "700/700 [==============================] - 0s 192us/step - loss: 0.4596 - acc: 0.8514 - val_loss: 2.2328 - val_acc: 0.5467\n",
      "Epoch 1865/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4590 - acc: 0.8529 - val_loss: 2.2404 - val_acc: 0.5400\n",
      "Epoch 1866/3000\n",
      "700/700 [==============================] - 0s 214us/step - loss: 0.4604 - acc: 0.8529 - val_loss: 2.2412 - val_acc: 0.5433\n",
      "Epoch 1867/3000\n",
      "700/700 [==============================] - 0s 225us/step - loss: 0.4591 - acc: 0.8514 - val_loss: 2.2426 - val_acc: 0.5400\n",
      "Epoch 1868/3000\n",
      "700/700 [==============================] - 0s 200us/step - loss: 0.4594 - acc: 0.8529 - val_loss: 2.2396 - val_acc: 0.5400\n",
      "Epoch 1869/3000\n",
      "700/700 [==============================] - 0s 181us/step - loss: 0.4589 - acc: 0.8514 - val_loss: 2.2483 - val_acc: 0.5400\n",
      "Epoch 1870/3000\n",
      "700/700 [==============================] - 0s 204us/step - loss: 0.4592 - acc: 0.8514 - val_loss: 2.2541 - val_acc: 0.5367\n",
      "Epoch 1871/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4595 - acc: 0.8486 - val_loss: 2.2400 - val_acc: 0.5467\n",
      "Epoch 1872/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4578 - acc: 0.8514 - val_loss: 2.2412 - val_acc: 0.5400\n",
      "Epoch 1873/3000\n",
      "700/700 [==============================] - 0s 176us/step - loss: 0.4589 - acc: 0.8529 - val_loss: 2.2464 - val_acc: 0.5500\n",
      "Epoch 1874/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4584 - acc: 0.8514 - val_loss: 2.2451 - val_acc: 0.5500\n",
      "Epoch 1875/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4590 - acc: 0.8529 - val_loss: 2.2395 - val_acc: 0.5433\n",
      "Epoch 1876/3000\n",
      "700/700 [==============================] - 0s 189us/step - loss: 0.4580 - acc: 0.8500 - val_loss: 2.2602 - val_acc: 0.5367\n",
      "Epoch 1877/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.4579 - acc: 0.8514 - val_loss: 2.2497 - val_acc: 0.5467\n",
      "Epoch 1878/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4582 - acc: 0.8514 - val_loss: 2.2387 - val_acc: 0.5367\n",
      "Epoch 1879/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4584 - acc: 0.8514 - val_loss: 2.2577 - val_acc: 0.5400\n",
      "Epoch 1880/3000\n",
      "700/700 [==============================] - 0s 202us/step - loss: 0.4580 - acc: 0.8514 - val_loss: 2.2417 - val_acc: 0.5400\n",
      "Epoch 1881/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4582 - acc: 0.8529 - val_loss: 2.2500 - val_acc: 0.5433\n",
      "Epoch 1882/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4572 - acc: 0.8514 - val_loss: 2.2529 - val_acc: 0.5367\n",
      "Epoch 1883/3000\n",
      "700/700 [==============================] - 0s 176us/step - loss: 0.4583 - acc: 0.8529 - val_loss: 2.2414 - val_acc: 0.5433\n",
      "Epoch 1884/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4579 - acc: 0.8514 - val_loss: 2.2536 - val_acc: 0.5433\n",
      "Epoch 1885/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.4583 - acc: 0.8514 - val_loss: 2.2422 - val_acc: 0.5400\n",
      "Epoch 1886/3000\n",
      "700/700 [==============================] - 0s 186us/step - loss: 0.4583 - acc: 0.8514 - val_loss: 2.2437 - val_acc: 0.5400\n",
      "Epoch 1887/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.4577 - acc: 0.8514 - val_loss: 2.2509 - val_acc: 0.5433\n",
      "Epoch 1888/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4576 - acc: 0.8529 - val_loss: 2.2480 - val_acc: 0.5467\n",
      "Epoch 1889/3000\n",
      "700/700 [==============================] - 0s 179us/step - loss: 0.4571 - acc: 0.8514 - val_loss: 2.2502 - val_acc: 0.5400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1890/3000\n",
      "700/700 [==============================] - 0s 181us/step - loss: 0.4582 - acc: 0.8514 - val_loss: 2.2421 - val_acc: 0.5300\n",
      "Epoch 1891/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.4581 - acc: 0.8514 - val_loss: 2.2652 - val_acc: 0.5400\n",
      "Epoch 1892/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4573 - acc: 0.8514 - val_loss: 2.2449 - val_acc: 0.5333\n",
      "Epoch 1893/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.4565 - acc: 0.8529 - val_loss: 2.2532 - val_acc: 0.5467\n",
      "Epoch 1894/3000\n",
      "700/700 [==============================] - 0s 187us/step - loss: 0.4575 - acc: 0.8514 - val_loss: 2.2551 - val_acc: 0.5400\n",
      "Epoch 1895/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4569 - acc: 0.8514 - val_loss: 2.2649 - val_acc: 0.5400\n",
      "Epoch 1896/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4569 - acc: 0.8543 - val_loss: 2.2538 - val_acc: 0.5400\n",
      "Epoch 1897/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4577 - acc: 0.8500 - val_loss: 2.2591 - val_acc: 0.5467\n",
      "Epoch 1898/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4555 - acc: 0.8514 - val_loss: 2.2645 - val_acc: 0.5400\n",
      "Epoch 1899/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4573 - acc: 0.8514 - val_loss: 2.2477 - val_acc: 0.5400\n",
      "Epoch 1900/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4572 - acc: 0.8514 - val_loss: 2.2631 - val_acc: 0.5467\n",
      "Epoch 1901/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.4560 - acc: 0.8514 - val_loss: 2.2587 - val_acc: 0.5333\n",
      "Epoch 1902/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4563 - acc: 0.8500 - val_loss: 2.2628 - val_acc: 0.5400\n",
      "Epoch 1903/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4565 - acc: 0.8514 - val_loss: 2.2782 - val_acc: 0.5400\n",
      "Epoch 1904/3000\n",
      "700/700 [==============================] - 0s 211us/step - loss: 0.4572 - acc: 0.8529 - val_loss: 2.2612 - val_acc: 0.5267\n",
      "Epoch 1905/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4571 - acc: 0.8514 - val_loss: 2.2556 - val_acc: 0.5400\n",
      "Epoch 1906/3000\n",
      "700/700 [==============================] - 0s 179us/step - loss: 0.4560 - acc: 0.8529 - val_loss: 2.2467 - val_acc: 0.5333\n",
      "Epoch 1907/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4562 - acc: 0.8529 - val_loss: 2.2710 - val_acc: 0.5367\n",
      "Epoch 1908/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4555 - acc: 0.8543 - val_loss: 2.2450 - val_acc: 0.5367\n",
      "Epoch 1909/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.4562 - acc: 0.8529 - val_loss: 2.2521 - val_acc: 0.5333\n",
      "Epoch 1910/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4560 - acc: 0.8514 - val_loss: 2.2665 - val_acc: 0.5433\n",
      "Epoch 1911/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4572 - acc: 0.8529 - val_loss: 2.2751 - val_acc: 0.5400\n",
      "Epoch 1912/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4550 - acc: 0.8529 - val_loss: 2.2549 - val_acc: 0.5333\n",
      "Epoch 1913/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4549 - acc: 0.8514 - val_loss: 2.2701 - val_acc: 0.5333\n",
      "Epoch 1914/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4549 - acc: 0.8529 - val_loss: 2.2501 - val_acc: 0.5400\n",
      "Epoch 1915/3000\n",
      "700/700 [==============================] - 0s 176us/step - loss: 0.4550 - acc: 0.8529 - val_loss: 2.2628 - val_acc: 0.5367\n",
      "Epoch 1916/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4561 - acc: 0.8529 - val_loss: 2.2722 - val_acc: 0.5433\n",
      "Epoch 1917/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.4568 - acc: 0.8529 - val_loss: 2.2597 - val_acc: 0.5400\n",
      "Epoch 1918/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.4562 - acc: 0.8529 - val_loss: 2.2656 - val_acc: 0.5367\n",
      "Epoch 1919/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4550 - acc: 0.8543 - val_loss: 2.2740 - val_acc: 0.5400\n",
      "Epoch 1920/3000\n",
      "700/700 [==============================] - 0s 180us/step - loss: 0.4552 - acc: 0.8529 - val_loss: 2.2543 - val_acc: 0.5333\n",
      "Epoch 1921/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4556 - acc: 0.8514 - val_loss: 2.2569 - val_acc: 0.5333\n",
      "Epoch 1922/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4550 - acc: 0.8514 - val_loss: 2.2818 - val_acc: 0.5400\n",
      "Epoch 1923/3000\n",
      "700/700 [==============================] - 0s 180us/step - loss: 0.4562 - acc: 0.8529 - val_loss: 2.2698 - val_acc: 0.5400\n",
      "Epoch 1924/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4552 - acc: 0.8529 - val_loss: 2.2687 - val_acc: 0.5400\n",
      "Epoch 1925/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.4540 - acc: 0.8529 - val_loss: 2.2579 - val_acc: 0.5367\n",
      "Epoch 1926/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.4553 - acc: 0.8543 - val_loss: 2.2717 - val_acc: 0.5333\n",
      "Epoch 1927/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4557 - acc: 0.8543 - val_loss: 2.2651 - val_acc: 0.5400\n",
      "Epoch 1928/3000\n",
      "700/700 [==============================] - 0s 178us/step - loss: 0.4544 - acc: 0.8529 - val_loss: 2.2729 - val_acc: 0.5400\n",
      "Epoch 1929/3000\n",
      "700/700 [==============================] - 0s 184us/step - loss: 0.4549 - acc: 0.8543 - val_loss: 2.2744 - val_acc: 0.5433\n",
      "Epoch 1930/3000\n",
      "700/700 [==============================] - 0s 182us/step - loss: 0.4542 - acc: 0.8543 - val_loss: 2.2648 - val_acc: 0.5300\n",
      "Epoch 1931/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.4543 - acc: 0.8529 - val_loss: 2.2684 - val_acc: 0.5467\n",
      "Epoch 1932/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4549 - acc: 0.8529 - val_loss: 2.2661 - val_acc: 0.5367\n",
      "Epoch 1933/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4549 - acc: 0.8529 - val_loss: 2.2940 - val_acc: 0.5367\n",
      "Epoch 1934/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4537 - acc: 0.8514 - val_loss: 2.2857 - val_acc: 0.5367\n",
      "Epoch 1935/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.4547 - acc: 0.8514 - val_loss: 2.2747 - val_acc: 0.5333\n",
      "Epoch 1936/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4540 - acc: 0.8543 - val_loss: 2.2738 - val_acc: 0.5433\n",
      "Epoch 1937/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4536 - acc: 0.8543 - val_loss: 2.2808 - val_acc: 0.5400\n",
      "Epoch 1938/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4541 - acc: 0.8543 - val_loss: 2.2763 - val_acc: 0.5433\n",
      "Epoch 1939/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.4532 - acc: 0.8543 - val_loss: 2.2796 - val_acc: 0.5367\n",
      "Epoch 1940/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4530 - acc: 0.8543 - val_loss: 2.2604 - val_acc: 0.5433\n",
      "Epoch 1941/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4548 - acc: 0.8529 - val_loss: 2.2715 - val_acc: 0.5400\n",
      "Epoch 1942/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4541 - acc: 0.8514 - val_loss: 2.2826 - val_acc: 0.5433\n",
      "Epoch 1943/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.4537 - acc: 0.8514 - val_loss: 2.2738 - val_acc: 0.5433\n",
      "Epoch 1944/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4542 - acc: 0.8529 - val_loss: 2.2669 - val_acc: 0.5400\n",
      "Epoch 1945/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4540 - acc: 0.8529 - val_loss: 2.2817 - val_acc: 0.5367\n",
      "Epoch 1946/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4533 - acc: 0.8500 - val_loss: 2.2715 - val_acc: 0.5367\n",
      "Epoch 1947/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4531 - acc: 0.8529 - val_loss: 2.2815 - val_acc: 0.5400\n",
      "Epoch 1948/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4528 - acc: 0.8529 - val_loss: 2.2813 - val_acc: 0.5433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1949/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.4522 - acc: 0.8529 - val_loss: 2.2780 - val_acc: 0.5400\n",
      "Epoch 1950/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4532 - acc: 0.8529 - val_loss: 2.2763 - val_acc: 0.5433\n",
      "Epoch 1951/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4531 - acc: 0.8543 - val_loss: 2.2839 - val_acc: 0.5367\n",
      "Epoch 1952/3000\n",
      "700/700 [==============================] - 0s 184us/step - loss: 0.4523 - acc: 0.8500 - val_loss: 2.2890 - val_acc: 0.5400\n",
      "Epoch 1953/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.4526 - acc: 0.8543 - val_loss: 2.2690 - val_acc: 0.5367\n",
      "Epoch 1954/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4528 - acc: 0.8543 - val_loss: 2.2883 - val_acc: 0.5367\n",
      "Epoch 1955/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4532 - acc: 0.8514 - val_loss: 2.2838 - val_acc: 0.5433\n",
      "Epoch 1956/3000\n",
      "700/700 [==============================] - 0s 176us/step - loss: 0.4527 - acc: 0.8514 - val_loss: 2.2774 - val_acc: 0.5333\n",
      "Epoch 1957/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4528 - acc: 0.8543 - val_loss: 2.2952 - val_acc: 0.5400\n",
      "Epoch 1958/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4534 - acc: 0.8514 - val_loss: 2.2876 - val_acc: 0.5333\n",
      "Epoch 1959/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4534 - acc: 0.8529 - val_loss: 2.2788 - val_acc: 0.5367\n",
      "Epoch 1960/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4523 - acc: 0.8514 - val_loss: 2.2836 - val_acc: 0.5367\n",
      "Epoch 1961/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4520 - acc: 0.8543 - val_loss: 2.2835 - val_acc: 0.5367\n",
      "Epoch 1962/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4525 - acc: 0.8529 - val_loss: 2.2904 - val_acc: 0.5367\n",
      "Epoch 1963/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4518 - acc: 0.8529 - val_loss: 2.2885 - val_acc: 0.5400\n",
      "Epoch 1964/3000\n",
      "700/700 [==============================] - 0s 176us/step - loss: 0.4526 - acc: 0.8529 - val_loss: 2.2867 - val_acc: 0.5367\n",
      "Epoch 1965/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.4524 - acc: 0.8529 - val_loss: 2.3006 - val_acc: 0.5367\n",
      "Epoch 1966/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4520 - acc: 0.8529 - val_loss: 2.2875 - val_acc: 0.5367\n",
      "Epoch 1967/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4515 - acc: 0.8529 - val_loss: 2.2713 - val_acc: 0.5367\n",
      "Epoch 1968/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4512 - acc: 0.8543 - val_loss: 2.3051 - val_acc: 0.5400\n",
      "Epoch 1969/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4523 - acc: 0.8529 - val_loss: 2.2795 - val_acc: 0.5333\n",
      "Epoch 1970/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4531 - acc: 0.8557 - val_loss: 2.3009 - val_acc: 0.5367\n",
      "Epoch 1971/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4518 - acc: 0.8529 - val_loss: 2.2964 - val_acc: 0.5333\n",
      "Epoch 1972/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4513 - acc: 0.8500 - val_loss: 2.2923 - val_acc: 0.5367\n",
      "Epoch 1973/3000\n",
      "700/700 [==============================] - 0s 182us/step - loss: 0.4517 - acc: 0.8543 - val_loss: 2.2982 - val_acc: 0.5300\n",
      "Epoch 1974/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4502 - acc: 0.8543 - val_loss: 2.2853 - val_acc: 0.5367\n",
      "Epoch 1975/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4507 - acc: 0.8543 - val_loss: 2.3071 - val_acc: 0.5367\n",
      "Epoch 1976/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4510 - acc: 0.8529 - val_loss: 2.3003 - val_acc: 0.5333\n",
      "Epoch 1977/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4508 - acc: 0.8529 - val_loss: 2.2960 - val_acc: 0.5300\n",
      "Epoch 1978/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4522 - acc: 0.8543 - val_loss: 2.3047 - val_acc: 0.5333\n",
      "Epoch 1979/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4510 - acc: 0.8543 - val_loss: 2.3009 - val_acc: 0.5300\n",
      "Epoch 1980/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4503 - acc: 0.8529 - val_loss: 2.3017 - val_acc: 0.5367\n",
      "Epoch 1981/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4517 - acc: 0.8529 - val_loss: 2.2977 - val_acc: 0.5367\n",
      "Epoch 1982/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4510 - acc: 0.8529 - val_loss: 2.2952 - val_acc: 0.5333\n",
      "Epoch 1983/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4499 - acc: 0.8543 - val_loss: 2.2994 - val_acc: 0.5300\n",
      "Epoch 1984/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4504 - acc: 0.8529 - val_loss: 2.3031 - val_acc: 0.5333\n",
      "Epoch 1985/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4500 - acc: 0.8543 - val_loss: 2.2864 - val_acc: 0.5367\n",
      "Epoch 1986/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4502 - acc: 0.8557 - val_loss: 2.3075 - val_acc: 0.5367\n",
      "Epoch 1987/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4498 - acc: 0.8529 - val_loss: 2.3135 - val_acc: 0.5333\n",
      "Epoch 1988/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4507 - acc: 0.8514 - val_loss: 2.3045 - val_acc: 0.5333\n",
      "Epoch 1989/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4494 - acc: 0.8543 - val_loss: 2.2983 - val_acc: 0.5333\n",
      "Epoch 1990/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.4505 - acc: 0.8543 - val_loss: 2.3124 - val_acc: 0.5400\n",
      "Epoch 1991/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4505 - acc: 0.8514 - val_loss: 2.3156 - val_acc: 0.5367\n",
      "Epoch 1992/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4502 - acc: 0.8529 - val_loss: 2.3096 - val_acc: 0.5333\n",
      "Epoch 1993/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4501 - acc: 0.8514 - val_loss: 2.3106 - val_acc: 0.5333\n",
      "Epoch 1994/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.4503 - acc: 0.8529 - val_loss: 2.3258 - val_acc: 0.5367\n",
      "Epoch 1995/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4500 - acc: 0.8543 - val_loss: 2.3058 - val_acc: 0.5300\n",
      "Epoch 1996/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4488 - acc: 0.8529 - val_loss: 2.3132 - val_acc: 0.5367\n",
      "Epoch 1997/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4491 - acc: 0.8543 - val_loss: 2.3042 - val_acc: 0.5333\n",
      "Epoch 1998/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4492 - acc: 0.8529 - val_loss: 2.2965 - val_acc: 0.5367\n",
      "Epoch 1999/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.4490 - acc: 0.8557 - val_loss: 2.3212 - val_acc: 0.5367\n",
      "Epoch 2000/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4499 - acc: 0.8543 - val_loss: 2.3205 - val_acc: 0.5333\n",
      "Epoch 2001/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4500 - acc: 0.8529 - val_loss: 2.3119 - val_acc: 0.5333\n",
      "Epoch 2002/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4485 - acc: 0.8543 - val_loss: 2.3254 - val_acc: 0.5333\n",
      "Epoch 2003/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.4482 - acc: 0.8543 - val_loss: 2.3053 - val_acc: 0.5267\n",
      "Epoch 2004/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4486 - acc: 0.8543 - val_loss: 2.3237 - val_acc: 0.5367\n",
      "Epoch 2005/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4488 - acc: 0.8514 - val_loss: 2.3032 - val_acc: 0.5300\n",
      "Epoch 2006/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4489 - acc: 0.8543 - val_loss: 2.3247 - val_acc: 0.5367\n",
      "Epoch 2007/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4493 - acc: 0.8543 - val_loss: 2.3169 - val_acc: 0.5367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2008/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.4486 - acc: 0.8543 - val_loss: 2.3141 - val_acc: 0.5267\n",
      "Epoch 2009/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4486 - acc: 0.8529 - val_loss: 2.3169 - val_acc: 0.5300\n",
      "Epoch 2010/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4485 - acc: 0.8514 - val_loss: 2.3143 - val_acc: 0.5300\n",
      "Epoch 2011/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4488 - acc: 0.8543 - val_loss: 2.3165 - val_acc: 0.5333\n",
      "Epoch 2012/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4488 - acc: 0.8543 - val_loss: 2.3184 - val_acc: 0.5333\n",
      "Epoch 2013/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4484 - acc: 0.8543 - val_loss: 2.3046 - val_acc: 0.5267\n",
      "Epoch 2014/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4486 - acc: 0.8529 - val_loss: 2.3181 - val_acc: 0.5333\n",
      "Epoch 2015/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4483 - acc: 0.8557 - val_loss: 2.3215 - val_acc: 0.5333\n",
      "Epoch 2016/3000\n",
      "700/700 [==============================] - 0s 178us/step - loss: 0.4488 - acc: 0.8543 - val_loss: 2.3237 - val_acc: 0.5333\n",
      "Epoch 2017/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4481 - acc: 0.8529 - val_loss: 2.3133 - val_acc: 0.5333\n",
      "Epoch 2018/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4489 - acc: 0.8543 - val_loss: 2.3077 - val_acc: 0.5267\n",
      "Epoch 2019/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.4478 - acc: 0.8529 - val_loss: 2.3346 - val_acc: 0.5333\n",
      "Epoch 2020/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4479 - acc: 0.8529 - val_loss: 2.3179 - val_acc: 0.5333\n",
      "Epoch 2021/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4487 - acc: 0.8543 - val_loss: 2.3217 - val_acc: 0.5300\n",
      "Epoch 2022/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4486 - acc: 0.8529 - val_loss: 2.3312 - val_acc: 0.5333\n",
      "Epoch 2023/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4484 - acc: 0.8543 - val_loss: 2.3118 - val_acc: 0.5333\n",
      "Epoch 2024/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4481 - acc: 0.8543 - val_loss: 2.3304 - val_acc: 0.5300\n",
      "Epoch 2025/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4480 - acc: 0.8529 - val_loss: 2.3244 - val_acc: 0.5333\n",
      "Epoch 2026/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4484 - acc: 0.8543 - val_loss: 2.3211 - val_acc: 0.5333\n",
      "Epoch 2027/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4474 - acc: 0.8543 - val_loss: 2.3187 - val_acc: 0.5267\n",
      "Epoch 2028/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.4474 - acc: 0.8529 - val_loss: 2.3295 - val_acc: 0.5333\n",
      "Epoch 2029/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4488 - acc: 0.8529 - val_loss: 2.3170 - val_acc: 0.5300\n",
      "Epoch 2030/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4474 - acc: 0.8543 - val_loss: 2.3266 - val_acc: 0.5333\n",
      "Epoch 2031/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4472 - acc: 0.8543 - val_loss: 2.3164 - val_acc: 0.5267\n",
      "Epoch 2032/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4480 - acc: 0.8543 - val_loss: 2.3095 - val_acc: 0.5367\n",
      "Epoch 2033/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4478 - acc: 0.8543 - val_loss: 2.3330 - val_acc: 0.5333\n",
      "Epoch 2034/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4480 - acc: 0.8529 - val_loss: 2.3311 - val_acc: 0.5400\n",
      "Epoch 2035/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4464 - acc: 0.8543 - val_loss: 2.3266 - val_acc: 0.5300\n",
      "Epoch 2036/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4477 - acc: 0.8543 - val_loss: 2.3237 - val_acc: 0.5267\n",
      "Epoch 2037/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4471 - acc: 0.8543 - val_loss: 2.3360 - val_acc: 0.5333\n",
      "Epoch 2038/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4473 - acc: 0.8543 - val_loss: 2.3248 - val_acc: 0.5267\n",
      "Epoch 2039/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4467 - acc: 0.8543 - val_loss: 2.3288 - val_acc: 0.5300\n",
      "Epoch 2040/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4464 - acc: 0.8543 - val_loss: 2.3375 - val_acc: 0.5333\n",
      "Epoch 2041/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4473 - acc: 0.8543 - val_loss: 2.3273 - val_acc: 0.5333\n",
      "Epoch 2042/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4463 - acc: 0.8514 - val_loss: 2.3104 - val_acc: 0.5267\n",
      "Epoch 2043/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4467 - acc: 0.8543 - val_loss: 2.3327 - val_acc: 0.5333\n",
      "Epoch 2044/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4461 - acc: 0.8543 - val_loss: 2.3416 - val_acc: 0.5300\n",
      "Epoch 2045/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4471 - acc: 0.8529 - val_loss: 2.3406 - val_acc: 0.5367\n",
      "Epoch 2046/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4472 - acc: 0.8543 - val_loss: 2.3327 - val_acc: 0.5333\n",
      "Epoch 2047/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4460 - acc: 0.8529 - val_loss: 2.3392 - val_acc: 0.5333\n",
      "Epoch 2048/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4464 - acc: 0.8529 - val_loss: 2.3473 - val_acc: 0.5267\n",
      "Epoch 2049/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4468 - acc: 0.8543 - val_loss: 2.3298 - val_acc: 0.5300\n",
      "Epoch 2050/3000\n",
      "700/700 [==============================] - 0s 185us/step - loss: 0.4457 - acc: 0.8557 - val_loss: 2.3387 - val_acc: 0.5267\n",
      "Epoch 2051/3000\n",
      "700/700 [==============================] - 0s 228us/step - loss: 0.4463 - acc: 0.8543 - val_loss: 2.3376 - val_acc: 0.5267\n",
      "Epoch 2052/3000\n",
      "700/700 [==============================] - 0s 209us/step - loss: 0.4468 - acc: 0.8543 - val_loss: 2.3366 - val_acc: 0.5333\n",
      "Epoch 2053/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4463 - acc: 0.8543 - val_loss: 2.3404 - val_acc: 0.5300\n",
      "Epoch 2054/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4450 - acc: 0.8543 - val_loss: 2.3301 - val_acc: 0.5267\n",
      "Epoch 2055/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4458 - acc: 0.8557 - val_loss: 2.3370 - val_acc: 0.5333\n",
      "Epoch 2056/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4455 - acc: 0.8529 - val_loss: 2.3443 - val_acc: 0.5333\n",
      "Epoch 2057/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4467 - acc: 0.8529 - val_loss: 2.3381 - val_acc: 0.5333\n",
      "Epoch 2058/3000\n",
      "700/700 [==============================] - 0s 176us/step - loss: 0.4457 - acc: 0.8529 - val_loss: 2.3463 - val_acc: 0.5267\n",
      "Epoch 2059/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4451 - acc: 0.8543 - val_loss: 2.3416 - val_acc: 0.5367\n",
      "Epoch 2060/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4458 - acc: 0.8514 - val_loss: 2.3583 - val_acc: 0.5400\n",
      "Epoch 2061/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4454 - acc: 0.8529 - val_loss: 2.3446 - val_acc: 0.5333\n",
      "Epoch 2062/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4448 - acc: 0.8571 - val_loss: 2.3443 - val_acc: 0.5300\n",
      "Epoch 2063/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4457 - acc: 0.8543 - val_loss: 2.3429 - val_acc: 0.5300\n",
      "Epoch 2064/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4453 - acc: 0.8557 - val_loss: 2.3482 - val_acc: 0.5300\n",
      "Epoch 2065/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4445 - acc: 0.8557 - val_loss: 2.3569 - val_acc: 0.5300\n",
      "Epoch 2066/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4450 - acc: 0.8543 - val_loss: 2.3294 - val_acc: 0.5300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2067/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4457 - acc: 0.8529 - val_loss: 2.3399 - val_acc: 0.5267\n",
      "Epoch 2068/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.4453 - acc: 0.8571 - val_loss: 2.3516 - val_acc: 0.5300\n",
      "Epoch 2069/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4450 - acc: 0.8543 - val_loss: 2.3440 - val_acc: 0.5300\n",
      "Epoch 2070/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4452 - acc: 0.8529 - val_loss: 2.3394 - val_acc: 0.5333\n",
      "Epoch 2071/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4450 - acc: 0.8557 - val_loss: 2.3453 - val_acc: 0.5300\n",
      "Epoch 2072/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4444 - acc: 0.8543 - val_loss: 2.3611 - val_acc: 0.5300\n",
      "Epoch 2073/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4443 - acc: 0.8529 - val_loss: 2.3495 - val_acc: 0.5233\n",
      "Epoch 2074/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4435 - acc: 0.8571 - val_loss: 2.3473 - val_acc: 0.5300\n",
      "Epoch 2075/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4450 - acc: 0.8543 - val_loss: 2.3492 - val_acc: 0.5267\n",
      "Epoch 2076/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4447 - acc: 0.8557 - val_loss: 2.3418 - val_acc: 0.5233\n",
      "Epoch 2077/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4442 - acc: 0.8543 - val_loss: 2.3598 - val_acc: 0.5333\n",
      "Epoch 2078/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4442 - acc: 0.8543 - val_loss: 2.3596 - val_acc: 0.5300\n",
      "Epoch 2079/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4443 - acc: 0.8529 - val_loss: 2.3564 - val_acc: 0.5300\n",
      "Epoch 2080/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.4443 - acc: 0.8543 - val_loss: 2.3625 - val_acc: 0.5300\n",
      "Epoch 2081/3000\n",
      "700/700 [==============================] - 0s 182us/step - loss: 0.4433 - acc: 0.8571 - val_loss: 2.3527 - val_acc: 0.5300\n",
      "Epoch 2082/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4439 - acc: 0.8543 - val_loss: 2.3433 - val_acc: 0.5300\n",
      "Epoch 2083/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.4440 - acc: 0.8529 - val_loss: 2.3635 - val_acc: 0.5300\n",
      "Epoch 2084/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4446 - acc: 0.8543 - val_loss: 2.3620 - val_acc: 0.5300\n",
      "Epoch 2085/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4441 - acc: 0.8543 - val_loss: 2.3562 - val_acc: 0.5233\n",
      "Epoch 2086/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4439 - acc: 0.8557 - val_loss: 2.3692 - val_acc: 0.5267\n",
      "Epoch 2087/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.4434 - acc: 0.8600 - val_loss: 2.3629 - val_acc: 0.5300\n",
      "Epoch 2088/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4435 - acc: 0.8543 - val_loss: 2.3556 - val_acc: 0.5333\n",
      "Epoch 2089/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4435 - acc: 0.8557 - val_loss: 2.3577 - val_acc: 0.5367\n",
      "Epoch 2090/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4435 - acc: 0.8543 - val_loss: 2.3502 - val_acc: 0.5233\n",
      "Epoch 2091/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4439 - acc: 0.8529 - val_loss: 2.3514 - val_acc: 0.5300\n",
      "Epoch 2092/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4429 - acc: 0.8543 - val_loss: 2.3671 - val_acc: 0.5267\n",
      "Epoch 2093/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4439 - acc: 0.8529 - val_loss: 2.3609 - val_acc: 0.5267\n",
      "Epoch 2094/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4431 - acc: 0.8557 - val_loss: 2.3615 - val_acc: 0.5300\n",
      "Epoch 2095/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.4430 - acc: 0.8557 - val_loss: 2.3631 - val_acc: 0.5333\n",
      "Epoch 2096/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4428 - acc: 0.8543 - val_loss: 2.3603 - val_acc: 0.5300\n",
      "Epoch 2097/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4432 - acc: 0.8586 - val_loss: 2.3558 - val_acc: 0.5267\n",
      "Epoch 2098/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4425 - acc: 0.8543 - val_loss: 2.3657 - val_acc: 0.5300\n",
      "Epoch 2099/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4431 - acc: 0.8543 - val_loss: 2.3605 - val_acc: 0.5267\n",
      "Epoch 2100/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4426 - acc: 0.8543 - val_loss: 2.3554 - val_acc: 0.5233\n",
      "Epoch 2101/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.4432 - acc: 0.8543 - val_loss: 2.3711 - val_acc: 0.5300\n",
      "Epoch 2102/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4434 - acc: 0.8557 - val_loss: 2.3679 - val_acc: 0.5300\n",
      "Epoch 2103/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4428 - acc: 0.8571 - val_loss: 2.3688 - val_acc: 0.5300\n",
      "Epoch 2104/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4426 - acc: 0.8543 - val_loss: 2.3590 - val_acc: 0.5267\n",
      "Epoch 2105/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.4430 - acc: 0.8543 - val_loss: 2.3599 - val_acc: 0.5300\n",
      "Epoch 2106/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4416 - acc: 0.8571 - val_loss: 2.3592 - val_acc: 0.5233\n",
      "Epoch 2107/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4425 - acc: 0.8543 - val_loss: 2.3665 - val_acc: 0.5300\n",
      "Epoch 2108/3000\n",
      "700/700 [==============================] - 0s 181us/step - loss: 0.4433 - acc: 0.8543 - val_loss: 2.3537 - val_acc: 0.5300\n",
      "Epoch 2109/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4421 - acc: 0.8557 - val_loss: 2.3701 - val_acc: 0.5300\n",
      "Epoch 2110/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4426 - acc: 0.8586 - val_loss: 2.3578 - val_acc: 0.5300\n",
      "Epoch 2111/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4426 - acc: 0.8543 - val_loss: 2.3754 - val_acc: 0.5300\n",
      "Epoch 2112/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4428 - acc: 0.8543 - val_loss: 2.3608 - val_acc: 0.5267\n",
      "Epoch 2113/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4417 - acc: 0.8543 - val_loss: 2.3540 - val_acc: 0.5267\n",
      "Epoch 2114/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4423 - acc: 0.8557 - val_loss: 2.3609 - val_acc: 0.5267\n",
      "Epoch 2115/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4421 - acc: 0.8586 - val_loss: 2.3691 - val_acc: 0.5333\n",
      "Epoch 2116/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4429 - acc: 0.8529 - val_loss: 2.3675 - val_acc: 0.5267\n",
      "Epoch 2117/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4414 - acc: 0.8557 - val_loss: 2.3684 - val_acc: 0.5333\n",
      "Epoch 2118/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4422 - acc: 0.8557 - val_loss: 2.3730 - val_acc: 0.5300\n",
      "Epoch 2119/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4414 - acc: 0.8557 - val_loss: 2.3725 - val_acc: 0.5267\n",
      "Epoch 2120/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4418 - acc: 0.8529 - val_loss: 2.3794 - val_acc: 0.5300\n",
      "Epoch 2121/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4416 - acc: 0.8557 - val_loss: 2.3610 - val_acc: 0.5233\n",
      "Epoch 2122/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4423 - acc: 0.8514 - val_loss: 2.3726 - val_acc: 0.5300\n",
      "Epoch 2123/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4410 - acc: 0.8586 - val_loss: 2.3605 - val_acc: 0.5233\n",
      "Epoch 2124/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4410 - acc: 0.8557 - val_loss: 2.3668 - val_acc: 0.5300\n",
      "Epoch 2125/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4409 - acc: 0.8543 - val_loss: 2.3855 - val_acc: 0.5300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2126/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4414 - acc: 0.8543 - val_loss: 2.3791 - val_acc: 0.5267\n",
      "Epoch 2127/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4412 - acc: 0.8543 - val_loss: 2.3780 - val_acc: 0.5300\n",
      "Epoch 2128/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4415 - acc: 0.8571 - val_loss: 2.3870 - val_acc: 0.5300\n",
      "Epoch 2129/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4410 - acc: 0.8571 - val_loss: 2.3706 - val_acc: 0.5267\n",
      "Epoch 2130/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4404 - acc: 0.8557 - val_loss: 2.3760 - val_acc: 0.5333\n",
      "Epoch 2131/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4411 - acc: 0.8600 - val_loss: 2.3810 - val_acc: 0.5300\n",
      "Epoch 2132/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4417 - acc: 0.8543 - val_loss: 2.3813 - val_acc: 0.5300\n",
      "Epoch 2133/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4406 - acc: 0.8557 - val_loss: 2.3724 - val_acc: 0.5300\n",
      "Epoch 2134/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4408 - acc: 0.8571 - val_loss: 2.3787 - val_acc: 0.5267\n",
      "Epoch 2135/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4411 - acc: 0.8557 - val_loss: 2.3686 - val_acc: 0.5267\n",
      "Epoch 2136/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4408 - acc: 0.8557 - val_loss: 2.3819 - val_acc: 0.5300\n",
      "Epoch 2137/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4410 - acc: 0.8557 - val_loss: 2.3867 - val_acc: 0.5300\n",
      "Epoch 2138/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4406 - acc: 0.8543 - val_loss: 2.3754 - val_acc: 0.5300\n",
      "Epoch 2139/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4404 - acc: 0.8571 - val_loss: 2.3796 - val_acc: 0.5300\n",
      "Epoch 2140/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4413 - acc: 0.8557 - val_loss: 2.3780 - val_acc: 0.5267\n",
      "Epoch 2141/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4401 - acc: 0.8557 - val_loss: 2.3774 - val_acc: 0.5300\n",
      "Epoch 2142/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4405 - acc: 0.8543 - val_loss: 2.3810 - val_acc: 0.5300\n",
      "Epoch 2143/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4403 - acc: 0.8557 - val_loss: 2.3711 - val_acc: 0.5233\n",
      "Epoch 2144/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4399 - acc: 0.8586 - val_loss: 2.3721 - val_acc: 0.5233\n",
      "Epoch 2145/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4399 - acc: 0.8557 - val_loss: 2.3825 - val_acc: 0.5300\n",
      "Epoch 2146/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4397 - acc: 0.8571 - val_loss: 2.3817 - val_acc: 0.5233\n",
      "Epoch 2147/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4401 - acc: 0.8557 - val_loss: 2.3952 - val_acc: 0.5300\n",
      "Epoch 2148/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4396 - acc: 0.8557 - val_loss: 2.3891 - val_acc: 0.5267\n",
      "Epoch 2149/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4402 - acc: 0.8557 - val_loss: 2.3970 - val_acc: 0.5267\n",
      "Epoch 2150/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4397 - acc: 0.8586 - val_loss: 2.3768 - val_acc: 0.5233\n",
      "Epoch 2151/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4400 - acc: 0.8543 - val_loss: 2.3969 - val_acc: 0.5267\n",
      "Epoch 2152/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4402 - acc: 0.8571 - val_loss: 2.3916 - val_acc: 0.5267\n",
      "Epoch 2153/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4395 - acc: 0.8557 - val_loss: 2.3884 - val_acc: 0.5300\n",
      "Epoch 2154/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4398 - acc: 0.8543 - val_loss: 2.4039 - val_acc: 0.5267\n",
      "Epoch 2155/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4400 - acc: 0.8571 - val_loss: 2.3874 - val_acc: 0.5267\n",
      "Epoch 2156/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4397 - acc: 0.8586 - val_loss: 2.3783 - val_acc: 0.5267\n",
      "Epoch 2157/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4400 - acc: 0.8571 - val_loss: 2.3895 - val_acc: 0.5233\n",
      "Epoch 2158/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4392 - acc: 0.8543 - val_loss: 2.3956 - val_acc: 0.5300\n",
      "Epoch 2159/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4398 - acc: 0.8557 - val_loss: 2.3857 - val_acc: 0.5233\n",
      "Epoch 2160/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4392 - acc: 0.8529 - val_loss: 2.4053 - val_acc: 0.5300\n",
      "Epoch 2161/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4390 - acc: 0.8571 - val_loss: 2.3978 - val_acc: 0.5267\n",
      "Epoch 2162/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4393 - acc: 0.8600 - val_loss: 2.4234 - val_acc: 0.5367\n",
      "Epoch 2163/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4396 - acc: 0.8571 - val_loss: 2.3835 - val_acc: 0.5233\n",
      "Epoch 2164/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4393 - acc: 0.8600 - val_loss: 2.3893 - val_acc: 0.5267\n",
      "Epoch 2165/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4388 - acc: 0.8557 - val_loss: 2.3983 - val_acc: 0.5300\n",
      "Epoch 2166/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4389 - acc: 0.8557 - val_loss: 2.3930 - val_acc: 0.5267\n",
      "Epoch 2167/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4393 - acc: 0.8529 - val_loss: 2.4173 - val_acc: 0.5267\n",
      "Epoch 2168/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4382 - acc: 0.8571 - val_loss: 2.4139 - val_acc: 0.5333\n",
      "Epoch 2169/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4383 - acc: 0.8557 - val_loss: 2.3875 - val_acc: 0.5233\n",
      "Epoch 2170/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4382 - acc: 0.8557 - val_loss: 2.4030 - val_acc: 0.5267\n",
      "Epoch 2171/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4385 - acc: 0.8600 - val_loss: 2.4062 - val_acc: 0.5300\n",
      "Epoch 2172/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4394 - acc: 0.8571 - val_loss: 2.4020 - val_acc: 0.5300\n",
      "Epoch 2173/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4375 - acc: 0.8557 - val_loss: 2.3947 - val_acc: 0.5300\n",
      "Epoch 2174/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4385 - acc: 0.8586 - val_loss: 2.3932 - val_acc: 0.5233\n",
      "Epoch 2175/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4381 - acc: 0.8557 - val_loss: 2.3852 - val_acc: 0.5267\n",
      "Epoch 2176/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4383 - acc: 0.8543 - val_loss: 2.3965 - val_acc: 0.5300\n",
      "Epoch 2177/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4391 - acc: 0.8571 - val_loss: 2.4051 - val_acc: 0.5300\n",
      "Epoch 2178/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4382 - acc: 0.8557 - val_loss: 2.4076 - val_acc: 0.5267\n",
      "Epoch 2179/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4378 - acc: 0.8557 - val_loss: 2.4024 - val_acc: 0.5267\n",
      "Epoch 2180/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4378 - acc: 0.8571 - val_loss: 2.4024 - val_acc: 0.5300\n",
      "Epoch 2181/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4382 - acc: 0.8557 - val_loss: 2.3977 - val_acc: 0.5267\n",
      "Epoch 2182/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4383 - acc: 0.8557 - val_loss: 2.3895 - val_acc: 0.5267\n",
      "Epoch 2183/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4381 - acc: 0.8557 - val_loss: 2.3964 - val_acc: 0.5267\n",
      "Epoch 2184/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4375 - acc: 0.8557 - val_loss: 2.3909 - val_acc: 0.5233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2185/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4398 - acc: 0.8600 - val_loss: 2.4223 - val_acc: 0.5300\n",
      "Epoch 2186/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4385 - acc: 0.8557 - val_loss: 2.4099 - val_acc: 0.5300\n",
      "Epoch 2187/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4373 - acc: 0.8557 - val_loss: 2.4207 - val_acc: 0.5267\n",
      "Epoch 2188/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4370 - acc: 0.8586 - val_loss: 2.3949 - val_acc: 0.5267\n",
      "Epoch 2189/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4376 - acc: 0.8571 - val_loss: 2.3959 - val_acc: 0.5233\n",
      "Epoch 2190/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4372 - acc: 0.8586 - val_loss: 2.4016 - val_acc: 0.5267\n",
      "Epoch 2191/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4370 - acc: 0.8586 - val_loss: 2.4075 - val_acc: 0.5267\n",
      "Epoch 2192/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4375 - acc: 0.8586 - val_loss: 2.3937 - val_acc: 0.5233\n",
      "Epoch 2193/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4375 - acc: 0.8557 - val_loss: 2.3955 - val_acc: 0.5233\n",
      "Epoch 2194/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4375 - acc: 0.8586 - val_loss: 2.3830 - val_acc: 0.5267\n",
      "Epoch 2195/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4371 - acc: 0.8600 - val_loss: 2.3993 - val_acc: 0.5233\n",
      "Epoch 2196/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4377 - acc: 0.8571 - val_loss: 2.4071 - val_acc: 0.5300\n",
      "Epoch 2197/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4364 - acc: 0.8571 - val_loss: 2.4045 - val_acc: 0.5267\n",
      "Epoch 2198/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4375 - acc: 0.8557 - val_loss: 2.4202 - val_acc: 0.5300\n",
      "Epoch 2199/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4370 - acc: 0.8600 - val_loss: 2.4162 - val_acc: 0.5300\n",
      "Epoch 2200/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4367 - acc: 0.8586 - val_loss: 2.4092 - val_acc: 0.5233\n",
      "Epoch 2201/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4369 - acc: 0.8571 - val_loss: 2.4079 - val_acc: 0.5267\n",
      "Epoch 2202/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4361 - acc: 0.8586 - val_loss: 2.4139 - val_acc: 0.5267\n",
      "Epoch 2203/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4365 - acc: 0.8586 - val_loss: 2.4216 - val_acc: 0.5300\n",
      "Epoch 2204/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4362 - acc: 0.8571 - val_loss: 2.4166 - val_acc: 0.5267\n",
      "Epoch 2205/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4358 - acc: 0.8586 - val_loss: 2.4169 - val_acc: 0.5267\n",
      "Epoch 2206/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4367 - acc: 0.8571 - val_loss: 2.4139 - val_acc: 0.5267\n",
      "Epoch 2207/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4361 - acc: 0.8557 - val_loss: 2.4245 - val_acc: 0.5300\n",
      "Epoch 2208/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4361 - acc: 0.8586 - val_loss: 2.4270 - val_acc: 0.5300\n",
      "Epoch 2209/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4363 - acc: 0.8557 - val_loss: 2.3943 - val_acc: 0.5267\n",
      "Epoch 2210/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4368 - acc: 0.8529 - val_loss: 2.4131 - val_acc: 0.5300\n",
      "Epoch 2211/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4365 - acc: 0.8586 - val_loss: 2.4030 - val_acc: 0.5233\n",
      "Epoch 2212/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4365 - acc: 0.8586 - val_loss: 2.4202 - val_acc: 0.5267\n",
      "Epoch 2213/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4355 - acc: 0.8571 - val_loss: 2.4239 - val_acc: 0.5300\n",
      "Epoch 2214/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4359 - acc: 0.8557 - val_loss: 2.4120 - val_acc: 0.5267\n",
      "Epoch 2215/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4356 - acc: 0.8557 - val_loss: 2.4012 - val_acc: 0.5267\n",
      "Epoch 2216/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4354 - acc: 0.8571 - val_loss: 2.4304 - val_acc: 0.5267\n",
      "Epoch 2217/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4364 - acc: 0.8543 - val_loss: 2.4187 - val_acc: 0.5267\n",
      "Epoch 2218/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4357 - acc: 0.8600 - val_loss: 2.4124 - val_acc: 0.5300\n",
      "Epoch 2219/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4362 - acc: 0.8586 - val_loss: 2.3947 - val_acc: 0.5267\n",
      "Epoch 2220/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4356 - acc: 0.8543 - val_loss: 2.4183 - val_acc: 0.5267\n",
      "Epoch 2221/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4351 - acc: 0.8586 - val_loss: 2.4334 - val_acc: 0.5267\n",
      "Epoch 2222/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4350 - acc: 0.8586 - val_loss: 2.4270 - val_acc: 0.5267\n",
      "Epoch 2223/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4363 - acc: 0.8557 - val_loss: 2.4241 - val_acc: 0.5300\n",
      "Epoch 2224/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4352 - acc: 0.8557 - val_loss: 2.4225 - val_acc: 0.5300\n",
      "Epoch 2225/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4347 - acc: 0.8557 - val_loss: 2.4181 - val_acc: 0.5267\n",
      "Epoch 2226/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4350 - acc: 0.8571 - val_loss: 2.4143 - val_acc: 0.5267\n",
      "Epoch 2227/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4347 - acc: 0.8586 - val_loss: 2.4265 - val_acc: 0.5267\n",
      "Epoch 2228/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4357 - acc: 0.8571 - val_loss: 2.4252 - val_acc: 0.5267\n",
      "Epoch 2229/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4358 - acc: 0.8571 - val_loss: 2.4278 - val_acc: 0.5267\n",
      "Epoch 2230/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4354 - acc: 0.8586 - val_loss: 2.4443 - val_acc: 0.5267\n",
      "Epoch 2231/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4351 - acc: 0.8571 - val_loss: 2.4230 - val_acc: 0.5300\n",
      "Epoch 2232/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4346 - acc: 0.8586 - val_loss: 2.4283 - val_acc: 0.5233\n",
      "Epoch 2233/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4341 - acc: 0.8586 - val_loss: 2.4243 - val_acc: 0.5267\n",
      "Epoch 2234/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.4345 - acc: 0.8614 - val_loss: 2.4290 - val_acc: 0.5300\n",
      "Epoch 2235/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4348 - acc: 0.8571 - val_loss: 2.4471 - val_acc: 0.5267\n",
      "Epoch 2236/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4348 - acc: 0.8557 - val_loss: 2.4255 - val_acc: 0.5233\n",
      "Epoch 2237/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4345 - acc: 0.8600 - val_loss: 2.4258 - val_acc: 0.5300\n",
      "Epoch 2238/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.4340 - acc: 0.8543 - val_loss: 2.4403 - val_acc: 0.5233\n",
      "Epoch 2239/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4351 - acc: 0.8571 - val_loss: 2.4322 - val_acc: 0.5267\n",
      "Epoch 2240/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4350 - acc: 0.8614 - val_loss: 2.4431 - val_acc: 0.5300\n",
      "Epoch 2241/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4339 - acc: 0.8586 - val_loss: 2.4359 - val_acc: 0.5267\n",
      "Epoch 2242/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4333 - acc: 0.8571 - val_loss: 2.4261 - val_acc: 0.5267\n",
      "Epoch 2243/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4342 - acc: 0.8600 - val_loss: 2.4380 - val_acc: 0.5300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2244/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4350 - acc: 0.8600 - val_loss: 2.4412 - val_acc: 0.5300\n",
      "Epoch 2245/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4348 - acc: 0.8557 - val_loss: 2.4327 - val_acc: 0.5267\n",
      "Epoch 2246/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4342 - acc: 0.8600 - val_loss: 2.4337 - val_acc: 0.5267\n",
      "Epoch 2247/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4337 - acc: 0.8614 - val_loss: 2.4354 - val_acc: 0.5267\n",
      "Epoch 2248/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4344 - acc: 0.8586 - val_loss: 2.4226 - val_acc: 0.5233\n",
      "Epoch 2249/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4337 - acc: 0.8586 - val_loss: 2.4462 - val_acc: 0.5300\n",
      "Epoch 2250/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4331 - acc: 0.8643 - val_loss: 2.4567 - val_acc: 0.5267\n",
      "Epoch 2251/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4344 - acc: 0.8586 - val_loss: 2.4390 - val_acc: 0.5267\n",
      "Epoch 2252/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4335 - acc: 0.8614 - val_loss: 2.4361 - val_acc: 0.5267\n",
      "Epoch 2253/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4335 - acc: 0.8557 - val_loss: 2.4577 - val_acc: 0.5267\n",
      "Epoch 2254/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4335 - acc: 0.8600 - val_loss: 2.4400 - val_acc: 0.5300\n",
      "Epoch 2255/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4338 - acc: 0.8586 - val_loss: 2.4389 - val_acc: 0.5300\n",
      "Epoch 2256/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4338 - acc: 0.8571 - val_loss: 2.4294 - val_acc: 0.5267\n",
      "Epoch 2257/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4336 - acc: 0.8571 - val_loss: 2.4432 - val_acc: 0.5300\n",
      "Epoch 2258/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4333 - acc: 0.8614 - val_loss: 2.4184 - val_acc: 0.5333\n",
      "Epoch 2259/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4335 - acc: 0.8600 - val_loss: 2.4402 - val_acc: 0.5300\n",
      "Epoch 2260/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4340 - acc: 0.8600 - val_loss: 2.4358 - val_acc: 0.5233\n",
      "Epoch 2261/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.4333 - acc: 0.8571 - val_loss: 2.4422 - val_acc: 0.5300\n",
      "Epoch 2262/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.4328 - acc: 0.8571 - val_loss: 2.4413 - val_acc: 0.5233\n",
      "Epoch 2263/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4333 - acc: 0.8571 - val_loss: 2.4351 - val_acc: 0.5233\n",
      "Epoch 2264/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4326 - acc: 0.8600 - val_loss: 2.4369 - val_acc: 0.5267\n",
      "Epoch 2265/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4325 - acc: 0.8586 - val_loss: 2.4588 - val_acc: 0.5300\n",
      "Epoch 2266/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4332 - acc: 0.8600 - val_loss: 2.4245 - val_acc: 0.5267\n",
      "Epoch 2267/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4326 - acc: 0.8600 - val_loss: 2.4418 - val_acc: 0.5267\n",
      "Epoch 2268/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4327 - acc: 0.8586 - val_loss: 2.4559 - val_acc: 0.5267\n",
      "Epoch 2269/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4329 - acc: 0.8600 - val_loss: 2.4122 - val_acc: 0.5267\n",
      "Epoch 2270/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4334 - acc: 0.8586 - val_loss: 2.4427 - val_acc: 0.5267\n",
      "Epoch 2271/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4331 - acc: 0.8571 - val_loss: 2.4429 - val_acc: 0.5267\n",
      "Epoch 2272/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4322 - acc: 0.8600 - val_loss: 2.4374 - val_acc: 0.5300\n",
      "Epoch 2273/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4326 - acc: 0.8571 - val_loss: 2.4528 - val_acc: 0.5267\n",
      "Epoch 2274/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4326 - acc: 0.8614 - val_loss: 2.4317 - val_acc: 0.5267\n",
      "Epoch 2275/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.4322 - acc: 0.8586 - val_loss: 2.4562 - val_acc: 0.5300\n",
      "Epoch 2276/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4317 - acc: 0.8586 - val_loss: 2.4341 - val_acc: 0.5267\n",
      "Epoch 2277/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4322 - acc: 0.8571 - val_loss: 2.4461 - val_acc: 0.5300\n",
      "Epoch 2278/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4319 - acc: 0.8614 - val_loss: 2.4383 - val_acc: 0.5300\n",
      "Epoch 2279/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4317 - acc: 0.8614 - val_loss: 2.4630 - val_acc: 0.5267\n",
      "Epoch 2280/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4336 - acc: 0.8586 - val_loss: 2.4505 - val_acc: 0.5267\n",
      "Epoch 2281/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4320 - acc: 0.8600 - val_loss: 2.4568 - val_acc: 0.5300\n",
      "Epoch 2282/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4318 - acc: 0.8600 - val_loss: 2.4508 - val_acc: 0.5233\n",
      "Epoch 2283/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4320 - acc: 0.8614 - val_loss: 2.4470 - val_acc: 0.5233\n",
      "Epoch 2284/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4322 - acc: 0.8586 - val_loss: 2.4560 - val_acc: 0.5267\n",
      "Epoch 2285/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4312 - acc: 0.8600 - val_loss: 2.4489 - val_acc: 0.5300\n",
      "Epoch 2286/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4325 - acc: 0.8643 - val_loss: 2.4650 - val_acc: 0.5267\n",
      "Epoch 2287/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4327 - acc: 0.8600 - val_loss: 2.4495 - val_acc: 0.5233\n",
      "Epoch 2288/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4326 - acc: 0.8600 - val_loss: 2.4403 - val_acc: 0.5233\n",
      "Epoch 2289/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4313 - acc: 0.8600 - val_loss: 2.4442 - val_acc: 0.5233\n",
      "Epoch 2290/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4317 - acc: 0.8571 - val_loss: 2.4558 - val_acc: 0.5267\n",
      "Epoch 2291/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4322 - acc: 0.8586 - val_loss: 2.4523 - val_acc: 0.5233\n",
      "Epoch 2292/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4321 - acc: 0.8600 - val_loss: 2.4556 - val_acc: 0.5267\n",
      "Epoch 2293/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4314 - acc: 0.8571 - val_loss: 2.4559 - val_acc: 0.5267\n",
      "Epoch 2294/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4310 - acc: 0.8600 - val_loss: 2.4565 - val_acc: 0.5267\n",
      "Epoch 2295/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4313 - acc: 0.8600 - val_loss: 2.4503 - val_acc: 0.5267\n",
      "Epoch 2296/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4317 - acc: 0.8586 - val_loss: 2.4608 - val_acc: 0.5300\n",
      "Epoch 2297/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4313 - acc: 0.8571 - val_loss: 2.4459 - val_acc: 0.5300\n",
      "Epoch 2298/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4310 - acc: 0.8600 - val_loss: 2.4559 - val_acc: 0.5267\n",
      "Epoch 2299/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.4312 - acc: 0.8600 - val_loss: 2.4599 - val_acc: 0.5300\n",
      "Epoch 2300/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4305 - acc: 0.8586 - val_loss: 2.4467 - val_acc: 0.5267\n",
      "Epoch 2301/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4314 - acc: 0.8600 - val_loss: 2.4470 - val_acc: 0.5267\n",
      "Epoch 2302/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4310 - acc: 0.8600 - val_loss: 2.4436 - val_acc: 0.5233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2303/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4315 - acc: 0.8586 - val_loss: 2.4814 - val_acc: 0.5267\n",
      "Epoch 2304/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4313 - acc: 0.8557 - val_loss: 2.4516 - val_acc: 0.5233\n",
      "Epoch 2305/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4312 - acc: 0.8600 - val_loss: 2.4542 - val_acc: 0.5267\n",
      "Epoch 2306/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4303 - acc: 0.8600 - val_loss: 2.4712 - val_acc: 0.5267\n",
      "Epoch 2307/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.4304 - acc: 0.8600 - val_loss: 2.4780 - val_acc: 0.5267\n",
      "Epoch 2308/3000\n",
      "700/700 [==============================] - 0s 180us/step - loss: 0.4309 - acc: 0.8629 - val_loss: 2.4865 - val_acc: 0.5267\n",
      "Epoch 2309/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4308 - acc: 0.8614 - val_loss: 2.4387 - val_acc: 0.5233\n",
      "Epoch 2310/3000\n",
      "700/700 [==============================] - 0s 176us/step - loss: 0.4306 - acc: 0.8571 - val_loss: 2.4597 - val_acc: 0.5267\n",
      "Epoch 2311/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4324 - acc: 0.8600 - val_loss: 2.4522 - val_acc: 0.5300\n",
      "Epoch 2312/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.4304 - acc: 0.8629 - val_loss: 2.4589 - val_acc: 0.5267\n",
      "Epoch 2313/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4304 - acc: 0.8571 - val_loss: 2.4645 - val_acc: 0.5267\n",
      "Epoch 2314/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.4304 - acc: 0.8629 - val_loss: 2.4595 - val_acc: 0.5267\n",
      "Epoch 2315/3000\n",
      "700/700 [==============================] - 0s 189us/step - loss: 0.4307 - acc: 0.8600 - val_loss: 2.4583 - val_acc: 0.5233\n",
      "Epoch 2316/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.4310 - acc: 0.8571 - val_loss: 2.4732 - val_acc: 0.5267\n",
      "Epoch 2317/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4294 - acc: 0.8629 - val_loss: 2.4846 - val_acc: 0.5267\n",
      "Epoch 2318/3000\n",
      "700/700 [==============================] - 0s 193us/step - loss: 0.4299 - acc: 0.8586 - val_loss: 2.4743 - val_acc: 0.5267\n",
      "Epoch 2319/3000\n",
      "700/700 [==============================] - 0s 169us/step - loss: 0.4301 - acc: 0.8600 - val_loss: 2.4612 - val_acc: 0.5233\n",
      "Epoch 2320/3000\n",
      "700/700 [==============================] - 0s 271us/step - loss: 0.4298 - acc: 0.8571 - val_loss: 2.4559 - val_acc: 0.5233\n",
      "Epoch 2321/3000\n",
      "700/700 [==============================] - 0s 181us/step - loss: 0.4299 - acc: 0.8643 - val_loss: 2.4619 - val_acc: 0.5267\n",
      "Epoch 2322/3000\n",
      "700/700 [==============================] - 0s 217us/step - loss: 0.4305 - acc: 0.8586 - val_loss: 2.4876 - val_acc: 0.5267\n",
      "Epoch 2323/3000\n",
      "700/700 [==============================] - 0s 204us/step - loss: 0.4291 - acc: 0.8571 - val_loss: 2.4552 - val_acc: 0.5233\n",
      "Epoch 2324/3000\n",
      "700/700 [==============================] - 0s 199us/step - loss: 0.4293 - acc: 0.8557 - val_loss: 2.4788 - val_acc: 0.5267\n",
      "Epoch 2325/3000\n",
      "700/700 [==============================] - 0s 213us/step - loss: 0.4296 - acc: 0.8614 - val_loss: 2.4687 - val_acc: 0.5200\n",
      "Epoch 2326/3000\n",
      "700/700 [==============================] - 0s 209us/step - loss: 0.4301 - acc: 0.8629 - val_loss: 2.4560 - val_acc: 0.5300\n",
      "Epoch 2327/3000\n",
      "700/700 [==============================] - 0s 217us/step - loss: 0.4300 - acc: 0.8571 - val_loss: 2.4635 - val_acc: 0.5233\n",
      "Epoch 2328/3000\n",
      "700/700 [==============================] - 0s 201us/step - loss: 0.4297 - acc: 0.8586 - val_loss: 2.4818 - val_acc: 0.5267\n",
      "Epoch 2329/3000\n",
      "700/700 [==============================] - 0s 221us/step - loss: 0.4303 - acc: 0.8614 - val_loss: 2.4754 - val_acc: 0.5267\n",
      "Epoch 2330/3000\n",
      "700/700 [==============================] - 0s 216us/step - loss: 0.4293 - acc: 0.8614 - val_loss: 2.4732 - val_acc: 0.5233\n",
      "Epoch 2331/3000\n",
      "700/700 [==============================] - 0s 219us/step - loss: 0.4289 - acc: 0.8600 - val_loss: 2.4724 - val_acc: 0.5233\n",
      "Epoch 2332/3000\n",
      "700/700 [==============================] - 0s 237us/step - loss: 0.4287 - acc: 0.8614 - val_loss: 2.4763 - val_acc: 0.5233\n",
      "Epoch 2333/3000\n",
      "700/700 [==============================] - 0s 212us/step - loss: 0.4294 - acc: 0.8586 - val_loss: 2.4745 - val_acc: 0.5267\n",
      "Epoch 2334/3000\n",
      "700/700 [==============================] - 0s 208us/step - loss: 0.4294 - acc: 0.8571 - val_loss: 2.4838 - val_acc: 0.5267\n",
      "Epoch 2335/3000\n",
      "700/700 [==============================] - 0s 233us/step - loss: 0.4289 - acc: 0.8586 - val_loss: 2.4540 - val_acc: 0.5233\n",
      "Epoch 2336/3000\n",
      "700/700 [==============================] - 0s 235us/step - loss: 0.4295 - acc: 0.8643 - val_loss: 2.4677 - val_acc: 0.5233\n",
      "Epoch 2337/3000\n",
      "700/700 [==============================] - 0s 232us/step - loss: 0.4288 - acc: 0.8629 - val_loss: 2.4794 - val_acc: 0.5267\n",
      "Epoch 2338/3000\n",
      "700/700 [==============================] - 0s 250us/step - loss: 0.4297 - acc: 0.8614 - val_loss: 2.4766 - val_acc: 0.5300\n",
      "Epoch 2339/3000\n",
      "700/700 [==============================] - 0s 254us/step - loss: 0.4291 - acc: 0.8614 - val_loss: 2.4731 - val_acc: 0.5267\n",
      "Epoch 2340/3000\n",
      "700/700 [==============================] - 0s 289us/step - loss: 0.4285 - acc: 0.8586 - val_loss: 2.4749 - val_acc: 0.5267\n",
      "Epoch 2341/3000\n",
      "700/700 [==============================] - 0s 228us/step - loss: 0.4288 - acc: 0.8657 - val_loss: 2.4833 - val_acc: 0.5267\n",
      "Epoch 2342/3000\n",
      "700/700 [==============================] - 0s 247us/step - loss: 0.4290 - acc: 0.8571 - val_loss: 2.4736 - val_acc: 0.5267\n",
      "Epoch 2343/3000\n",
      "700/700 [==============================] - 0s 231us/step - loss: 0.4297 - acc: 0.8571 - val_loss: 2.4749 - val_acc: 0.5267\n",
      "Epoch 2344/3000\n",
      "700/700 [==============================] - 0s 254us/step - loss: 0.4284 - acc: 0.8629 - val_loss: 2.4728 - val_acc: 0.5267\n",
      "Epoch 2345/3000\n",
      "700/700 [==============================] - 0s 241us/step - loss: 0.4283 - acc: 0.8571 - val_loss: 2.4665 - val_acc: 0.5233\n",
      "Epoch 2346/3000\n",
      "700/700 [==============================] - 0s 248us/step - loss: 0.4282 - acc: 0.8614 - val_loss: 2.4725 - val_acc: 0.5267\n",
      "Epoch 2347/3000\n",
      "700/700 [==============================] - 0s 246us/step - loss: 0.4286 - acc: 0.8643 - val_loss: 2.4855 - val_acc: 0.5267\n",
      "Epoch 2348/3000\n",
      "700/700 [==============================] - 0s 241us/step - loss: 0.4284 - acc: 0.8600 - val_loss: 2.4717 - val_acc: 0.5267\n",
      "Epoch 2349/3000\n",
      "700/700 [==============================] - 0s 287us/step - loss: 0.4283 - acc: 0.8600 - val_loss: 2.4802 - val_acc: 0.5233\n",
      "Epoch 2350/3000\n",
      "700/700 [==============================] - 0s 245us/step - loss: 0.4279 - acc: 0.8643 - val_loss: 2.4671 - val_acc: 0.5233\n",
      "Epoch 2351/3000\n",
      "700/700 [==============================] - 0s 243us/step - loss: 0.4281 - acc: 0.8614 - val_loss: 2.4704 - val_acc: 0.5233\n",
      "Epoch 2352/3000\n",
      "700/700 [==============================] - 0s 265us/step - loss: 0.4292 - acc: 0.8657 - val_loss: 2.4821 - val_acc: 0.5233\n",
      "Epoch 2353/3000\n",
      "700/700 [==============================] - 0s 291us/step - loss: 0.4283 - acc: 0.8629 - val_loss: 2.4841 - val_acc: 0.5267\n",
      "Epoch 2354/3000\n",
      "700/700 [==============================] - 0s 245us/step - loss: 0.4275 - acc: 0.8614 - val_loss: 2.4789 - val_acc: 0.5233\n",
      "Epoch 2355/3000\n",
      "700/700 [==============================] - 0s 260us/step - loss: 0.4285 - acc: 0.8600 - val_loss: 2.4791 - val_acc: 0.5233\n",
      "Epoch 2356/3000\n",
      "700/700 [==============================] - 0s 247us/step - loss: 0.4284 - acc: 0.8614 - val_loss: 2.4668 - val_acc: 0.5233\n",
      "Epoch 2357/3000\n",
      "700/700 [==============================] - 0s 240us/step - loss: 0.4280 - acc: 0.8614 - val_loss: 2.4976 - val_acc: 0.5233\n",
      "Epoch 2358/3000\n",
      "700/700 [==============================] - 0s 273us/step - loss: 0.4278 - acc: 0.8629 - val_loss: 2.4805 - val_acc: 0.5233\n",
      "Epoch 2359/3000\n",
      "700/700 [==============================] - 0s 296us/step - loss: 0.4283 - acc: 0.8614 - val_loss: 2.4778 - val_acc: 0.5233\n",
      "Epoch 2360/3000\n",
      "700/700 [==============================] - 0s 305us/step - loss: 0.4274 - acc: 0.8614 - val_loss: 2.4913 - val_acc: 0.5233\n",
      "Epoch 2361/3000\n",
      "700/700 [==============================] - 0s 283us/step - loss: 0.4278 - acc: 0.8614 - val_loss: 2.4805 - val_acc: 0.5267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2362/3000\n",
      "700/700 [==============================] - 0s 266us/step - loss: 0.4275 - acc: 0.8629 - val_loss: 2.4808 - val_acc: 0.5233\n",
      "Epoch 2363/3000\n",
      "700/700 [==============================] - 0s 253us/step - loss: 0.4282 - acc: 0.8614 - val_loss: 2.4975 - val_acc: 0.5267\n",
      "Epoch 2364/3000\n",
      "700/700 [==============================] - 0s 268us/step - loss: 0.4277 - acc: 0.8571 - val_loss: 2.4862 - val_acc: 0.5233\n",
      "Epoch 2365/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 0.4303 - acc: 0.861 - 0s 287us/step - loss: 0.4276 - acc: 0.8614 - val_loss: 2.4784 - val_acc: 0.5233\n",
      "Epoch 2366/3000\n",
      "700/700 [==============================] - 0s 269us/step - loss: 0.4275 - acc: 0.8586 - val_loss: 2.4757 - val_acc: 0.5233\n",
      "Epoch 2367/3000\n",
      "700/700 [==============================] - 0s 264us/step - loss: 0.4275 - acc: 0.8614 - val_loss: 2.4981 - val_acc: 0.5267\n",
      "Epoch 2368/3000\n",
      "700/700 [==============================] - 0s 245us/step - loss: 0.4275 - acc: 0.8629 - val_loss: 2.4862 - val_acc: 0.5267\n",
      "Epoch 2369/3000\n",
      "700/700 [==============================] - 0s 266us/step - loss: 0.4271 - acc: 0.8643 - val_loss: 2.4769 - val_acc: 0.5233\n",
      "Epoch 2370/3000\n",
      "700/700 [==============================] - 0s 288us/step - loss: 0.4273 - acc: 0.8614 - val_loss: 2.4900 - val_acc: 0.5267\n",
      "Epoch 2371/3000\n",
      "700/700 [==============================] - 0s 283us/step - loss: 0.4278 - acc: 0.8643 - val_loss: 2.5052 - val_acc: 0.5233\n",
      "Epoch 2372/3000\n",
      "700/700 [==============================] - 0s 265us/step - loss: 0.4268 - acc: 0.8629 - val_loss: 2.4887 - val_acc: 0.5267\n",
      "Epoch 2373/3000\n",
      "700/700 [==============================] - 0s 274us/step - loss: 0.4279 - acc: 0.8586 - val_loss: 2.4750 - val_acc: 0.5267\n",
      "Epoch 2374/3000\n",
      "700/700 [==============================] - 0s 297us/step - loss: 0.4277 - acc: 0.8614 - val_loss: 2.5075 - val_acc: 0.5267\n",
      "Epoch 2375/3000\n",
      "700/700 [==============================] - 0s 269us/step - loss: 0.4273 - acc: 0.8600 - val_loss: 2.5023 - val_acc: 0.5300\n",
      "Epoch 2376/3000\n",
      "700/700 [==============================] - 0s 292us/step - loss: 0.4275 - acc: 0.8614 - val_loss: 2.5025 - val_acc: 0.5267\n",
      "Epoch 2377/3000\n",
      "700/700 [==============================] - 0s 287us/step - loss: 0.4267 - acc: 0.8629 - val_loss: 2.4842 - val_acc: 0.5233\n",
      "Epoch 2378/3000\n",
      "700/700 [==============================] - 0s 271us/step - loss: 0.4270 - acc: 0.8600 - val_loss: 2.4931 - val_acc: 0.5267\n",
      "Epoch 2379/3000\n",
      "700/700 [==============================] - 0s 270us/step - loss: 0.4268 - acc: 0.8614 - val_loss: 2.4754 - val_acc: 0.5267\n",
      "Epoch 2380/3000\n",
      "700/700 [==============================] - 0s 286us/step - loss: 0.4281 - acc: 0.8600 - val_loss: 2.4891 - val_acc: 0.5267\n",
      "Epoch 2381/3000\n",
      "700/700 [==============================] - 0s 284us/step - loss: 0.4268 - acc: 0.8614 - val_loss: 2.5096 - val_acc: 0.5233\n",
      "Epoch 2382/3000\n",
      "700/700 [==============================] - 0s 265us/step - loss: 0.4268 - acc: 0.8600 - val_loss: 2.5051 - val_acc: 0.5267\n",
      "Epoch 2383/3000\n",
      "700/700 [==============================] - 0s 276us/step - loss: 0.4262 - acc: 0.8643 - val_loss: 2.5009 - val_acc: 0.5233\n",
      "Epoch 2384/3000\n",
      "700/700 [==============================] - 0s 278us/step - loss: 0.4265 - acc: 0.8643 - val_loss: 2.5085 - val_acc: 0.5267\n",
      "Epoch 2385/3000\n",
      "700/700 [==============================] - 0s 286us/step - loss: 0.4263 - acc: 0.8629 - val_loss: 2.4925 - val_acc: 0.5267\n",
      "Epoch 2386/3000\n",
      "700/700 [==============================] - 0s 304us/step - loss: 0.4264 - acc: 0.8643 - val_loss: 2.5168 - val_acc: 0.5267\n",
      "Epoch 2387/3000\n",
      "700/700 [==============================] - 0s 262us/step - loss: 0.4266 - acc: 0.8629 - val_loss: 2.4964 - val_acc: 0.5267\n",
      "Epoch 2388/3000\n",
      "700/700 [==============================] - 0s 284us/step - loss: 0.4261 - acc: 0.8600 - val_loss: 2.4978 - val_acc: 0.5267\n",
      "Epoch 2389/3000\n",
      "700/700 [==============================] - 0s 262us/step - loss: 0.4262 - acc: 0.8600 - val_loss: 2.4668 - val_acc: 0.5233\n",
      "Epoch 2390/3000\n",
      "700/700 [==============================] - 0s 269us/step - loss: 0.4272 - acc: 0.8629 - val_loss: 2.5090 - val_acc: 0.5233\n",
      "Epoch 2391/3000\n",
      "700/700 [==============================] - 0s 297us/step - loss: 0.4259 - acc: 0.8671 - val_loss: 2.4760 - val_acc: 0.5233\n",
      "Epoch 2392/3000\n",
      "700/700 [==============================] - 0s 289us/step - loss: 0.4261 - acc: 0.8629 - val_loss: 2.5051 - val_acc: 0.5300\n",
      "Epoch 2393/3000\n",
      "700/700 [==============================] - 0s 284us/step - loss: 0.4261 - acc: 0.8586 - val_loss: 2.4843 - val_acc: 0.5267\n",
      "Epoch 2394/3000\n",
      "700/700 [==============================] - 0s 286us/step - loss: 0.4260 - acc: 0.8614 - val_loss: 2.5091 - val_acc: 0.5267\n",
      "Epoch 2395/3000\n",
      "700/700 [==============================] - 0s 306us/step - loss: 0.4266 - acc: 0.8614 - val_loss: 2.5000 - val_acc: 0.5233\n",
      "Epoch 2396/3000\n",
      "700/700 [==============================] - 0s 308us/step - loss: 0.4264 - acc: 0.8643 - val_loss: 2.4748 - val_acc: 0.5233\n",
      "Epoch 2397/3000\n",
      "700/700 [==============================] - 0s 289us/step - loss: 0.4261 - acc: 0.8643 - val_loss: 2.4936 - val_acc: 0.5233\n",
      "Epoch 2398/3000\n",
      "700/700 [==============================] - 0s 334us/step - loss: 0.4253 - acc: 0.8643 - val_loss: 2.4794 - val_acc: 0.5233\n",
      "Epoch 2399/3000\n",
      "700/700 [==============================] - 0s 329us/step - loss: 0.4259 - acc: 0.8586 - val_loss: 2.4945 - val_acc: 0.5233\n",
      "Epoch 2400/3000\n",
      "700/700 [==============================] - 0s 349us/step - loss: 0.4256 - acc: 0.8686 - val_loss: 2.5082 - val_acc: 0.5300\n",
      "Epoch 2401/3000\n",
      "700/700 [==============================] - 0s 347us/step - loss: 0.4259 - acc: 0.8600 - val_loss: 2.5106 - val_acc: 0.5267\n",
      "Epoch 2402/3000\n",
      "700/700 [==============================] - 0s 359us/step - loss: 0.4260 - acc: 0.8643 - val_loss: 2.4976 - val_acc: 0.5267\n",
      "Epoch 2403/3000\n",
      "700/700 [==============================] - 0s 346us/step - loss: 0.4259 - acc: 0.8643 - val_loss: 2.5045 - val_acc: 0.5267\n",
      "Epoch 2404/3000\n",
      "700/700 [==============================] - 0s 404us/step - loss: 0.4249 - acc: 0.8671 - val_loss: 2.4977 - val_acc: 0.5267\n",
      "Epoch 2405/3000\n",
      "700/700 [==============================] - 0s 346us/step - loss: 0.4251 - acc: 0.8614 - val_loss: 2.4728 - val_acc: 0.5267\n",
      "Epoch 2406/3000\n",
      "700/700 [==============================] - 0s 341us/step - loss: 0.4255 - acc: 0.8614 - val_loss: 2.5081 - val_acc: 0.5267\n",
      "Epoch 2407/3000\n",
      "700/700 [==============================] - 0s 342us/step - loss: 0.4253 - acc: 0.8614 - val_loss: 2.5002 - val_acc: 0.5300\n",
      "Epoch 2408/3000\n",
      "700/700 [==============================] - 0s 398us/step - loss: 0.4250 - acc: 0.8643 - val_loss: 2.5062 - val_acc: 0.5300\n",
      "Epoch 2409/3000\n",
      "700/700 [==============================] - 0s 341us/step - loss: 0.4265 - acc: 0.8614 - val_loss: 2.5012 - val_acc: 0.5300\n",
      "Epoch 2410/3000\n",
      "700/700 [==============================] - 0s 374us/step - loss: 0.4260 - acc: 0.8600 - val_loss: 2.5139 - val_acc: 0.5267\n",
      "Epoch 2411/3000\n",
      "700/700 [==============================] - 0s 372us/step - loss: 0.4249 - acc: 0.8657 - val_loss: 2.5112 - val_acc: 0.5267\n",
      "Epoch 2412/3000\n",
      "700/700 [==============================] - 0s 385us/step - loss: 0.4251 - acc: 0.8614 - val_loss: 2.5008 - val_acc: 0.5233\n",
      "Epoch 2413/3000\n",
      "700/700 [==============================] - 0s 401us/step - loss: 0.4253 - acc: 0.8614 - val_loss: 2.5037 - val_acc: 0.5267\n",
      "Epoch 2414/3000\n",
      "700/700 [==============================] - 0s 362us/step - loss: 0.4245 - acc: 0.8643 - val_loss: 2.5051 - val_acc: 0.5267\n",
      "Epoch 2415/3000\n",
      "700/700 [==============================] - 0s 400us/step - loss: 0.4249 - acc: 0.8614 - val_loss: 2.5100 - val_acc: 0.5267\n",
      "Epoch 2416/3000\n",
      "700/700 [==============================] - 0s 386us/step - loss: 0.4245 - acc: 0.8629 - val_loss: 2.4861 - val_acc: 0.5233\n",
      "Epoch 2417/3000\n",
      "700/700 [==============================] - 0s 362us/step - loss: 0.4245 - acc: 0.8614 - val_loss: 2.5195 - val_acc: 0.5267\n",
      "Epoch 2418/3000\n",
      "700/700 [==============================] - 0s 391us/step - loss: 0.4247 - acc: 0.8614 - val_loss: 2.5157 - val_acc: 0.5233\n",
      "Epoch 2419/3000\n",
      "700/700 [==============================] - 0s 408us/step - loss: 0.4246 - acc: 0.8629 - val_loss: 2.5012 - val_acc: 0.5233\n",
      "Epoch 2420/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 337us/step - loss: 0.4261 - acc: 0.8643 - val_loss: 2.5004 - val_acc: 0.5267\n",
      "Epoch 2421/3000\n",
      "700/700 [==============================] - 0s 266us/step - loss: 0.4252 - acc: 0.8657 - val_loss: 2.5128 - val_acc: 0.5267\n",
      "Epoch 2422/3000\n",
      "700/700 [==============================] - 0s 255us/step - loss: 0.4244 - acc: 0.8600 - val_loss: 2.5102 - val_acc: 0.5267\n",
      "Epoch 2423/3000\n",
      "700/700 [==============================] - 0s 282us/step - loss: 0.4243 - acc: 0.8586 - val_loss: 2.4977 - val_acc: 0.5233\n",
      "Epoch 2424/3000\n",
      "700/700 [==============================] - 0s 305us/step - loss: 0.4244 - acc: 0.8614 - val_loss: 2.5181 - val_acc: 0.5267\n",
      "Epoch 2425/3000\n",
      "700/700 [==============================] - 0s 264us/step - loss: 0.4240 - acc: 0.8629 - val_loss: 2.5121 - val_acc: 0.5233\n",
      "Epoch 2426/3000\n",
      "700/700 [==============================] - 0s 272us/step - loss: 0.4239 - acc: 0.8643 - val_loss: 2.5049 - val_acc: 0.5233\n",
      "Epoch 2427/3000\n",
      "700/700 [==============================] - 0s 276us/step - loss: 0.4241 - acc: 0.8629 - val_loss: 2.5087 - val_acc: 0.5267\n",
      "Epoch 2428/3000\n",
      "700/700 [==============================] - 0s 248us/step - loss: 0.4242 - acc: 0.8600 - val_loss: 2.5062 - val_acc: 0.5267\n",
      "Epoch 2429/3000\n",
      "700/700 [==============================] - 0s 276us/step - loss: 0.4250 - acc: 0.8643 - val_loss: 2.4872 - val_acc: 0.5167\n",
      "Epoch 2430/3000\n",
      "700/700 [==============================] - 0s 278us/step - loss: 0.4248 - acc: 0.8614 - val_loss: 2.5094 - val_acc: 0.5233\n",
      "Epoch 2431/3000\n",
      "700/700 [==============================] - 0s 259us/step - loss: 0.4245 - acc: 0.8614 - val_loss: 2.5029 - val_acc: 0.5200\n",
      "Epoch 2432/3000\n",
      "700/700 [==============================] - 0s 252us/step - loss: 0.4238 - acc: 0.8571 - val_loss: 2.5040 - val_acc: 0.5233\n",
      "Epoch 2433/3000\n",
      "700/700 [==============================] - 0s 293us/step - loss: 0.4246 - acc: 0.8671 - val_loss: 2.5016 - val_acc: 0.5267\n",
      "Epoch 2434/3000\n",
      "700/700 [==============================] - 0s 268us/step - loss: 0.4242 - acc: 0.8629 - val_loss: 2.5109 - val_acc: 0.5233\n",
      "Epoch 2435/3000\n",
      "700/700 [==============================] - 0s 240us/step - loss: 0.4239 - acc: 0.8657 - val_loss: 2.5072 - val_acc: 0.5267\n",
      "Epoch 2436/3000\n",
      "700/700 [==============================] - 0s 247us/step - loss: 0.4234 - acc: 0.8600 - val_loss: 2.5144 - val_acc: 0.5267\n",
      "Epoch 2437/3000\n",
      "700/700 [==============================] - 0s 252us/step - loss: 0.4234 - acc: 0.8657 - val_loss: 2.5127 - val_acc: 0.5267\n",
      "Epoch 2438/3000\n",
      "700/700 [==============================] - 0s 231us/step - loss: 0.4243 - acc: 0.8629 - val_loss: 2.5143 - val_acc: 0.5267\n",
      "Epoch 2439/3000\n",
      "700/700 [==============================] - 0s 241us/step - loss: 0.4235 - acc: 0.8629 - val_loss: 2.5205 - val_acc: 0.5233\n",
      "Epoch 2440/3000\n",
      "700/700 [==============================] - 0s 249us/step - loss: 0.4235 - acc: 0.8643 - val_loss: 2.5354 - val_acc: 0.5300\n",
      "Epoch 2441/3000\n",
      "700/700 [==============================] - 0s 226us/step - loss: 0.4234 - acc: 0.8657 - val_loss: 2.5216 - val_acc: 0.5267\n",
      "Epoch 2442/3000\n",
      "700/700 [==============================] - 0s 239us/step - loss: 0.4241 - acc: 0.8629 - val_loss: 2.5193 - val_acc: 0.5267\n",
      "Epoch 2443/3000\n",
      "700/700 [==============================] - 0s 224us/step - loss: 0.4239 - acc: 0.8614 - val_loss: 2.5031 - val_acc: 0.5200\n",
      "Epoch 2444/3000\n",
      "700/700 [==============================] - 0s 226us/step - loss: 0.4237 - acc: 0.8643 - val_loss: 2.5080 - val_acc: 0.5200\n",
      "Epoch 2445/3000\n",
      "700/700 [==============================] - 0s 228us/step - loss: 0.4228 - acc: 0.8629 - val_loss: 2.5286 - val_acc: 0.5233\n",
      "Epoch 2446/3000\n",
      "700/700 [==============================] - 0s 245us/step - loss: 0.4238 - acc: 0.8643 - val_loss: 2.5271 - val_acc: 0.5267\n",
      "Epoch 2447/3000\n",
      "700/700 [==============================] - 0s 236us/step - loss: 0.4227 - acc: 0.8614 - val_loss: 2.5177 - val_acc: 0.5200\n",
      "Epoch 2448/3000\n",
      "700/700 [==============================] - 0s 234us/step - loss: 0.4235 - acc: 0.8629 - val_loss: 2.5254 - val_acc: 0.5233\n",
      "Epoch 2449/3000\n",
      "700/700 [==============================] - 0s 212us/step - loss: 0.4230 - acc: 0.8629 - val_loss: 2.5094 - val_acc: 0.5200\n",
      "Epoch 2450/3000\n",
      "700/700 [==============================] - 0s 243us/step - loss: 0.4235 - acc: 0.8657 - val_loss: 2.5171 - val_acc: 0.5233\n",
      "Epoch 2451/3000\n",
      "700/700 [==============================] - 0s 217us/step - loss: 0.4231 - acc: 0.8614 - val_loss: 2.5248 - val_acc: 0.5233\n",
      "Epoch 2452/3000\n",
      "700/700 [==============================] - 0s 225us/step - loss: 0.4234 - acc: 0.8643 - val_loss: 2.5186 - val_acc: 0.5200\n",
      "Epoch 2453/3000\n",
      "700/700 [==============================] - 0s 208us/step - loss: 0.4235 - acc: 0.8614 - val_loss: 2.5182 - val_acc: 0.5200\n",
      "Epoch 2454/3000\n",
      "700/700 [==============================] - 0s 204us/step - loss: 0.4228 - acc: 0.8657 - val_loss: 2.5351 - val_acc: 0.5233\n",
      "Epoch 2455/3000\n",
      "700/700 [==============================] - 0s 227us/step - loss: 0.4234 - acc: 0.8629 - val_loss: 2.5171 - val_acc: 0.5200\n",
      "Epoch 2456/3000\n",
      "700/700 [==============================] - 0s 220us/step - loss: 0.4228 - acc: 0.8657 - val_loss: 2.5330 - val_acc: 0.5233\n",
      "Epoch 2457/3000\n",
      "700/700 [==============================] - 0s 220us/step - loss: 0.4230 - acc: 0.8614 - val_loss: 2.5195 - val_acc: 0.5267\n",
      "Epoch 2458/3000\n",
      "700/700 [==============================] - 0s 221us/step - loss: 0.4228 - acc: 0.8629 - val_loss: 2.5153 - val_acc: 0.5267\n",
      "Epoch 2459/3000\n",
      "700/700 [==============================] - 0s 213us/step - loss: 0.4233 - acc: 0.8671 - val_loss: 2.5098 - val_acc: 0.5267\n",
      "Epoch 2460/3000\n",
      "700/700 [==============================] - 0s 221us/step - loss: 0.4225 - acc: 0.8600 - val_loss: 2.5170 - val_acc: 0.5267\n",
      "Epoch 2461/3000\n",
      "700/700 [==============================] - 0s 205us/step - loss: 0.4221 - acc: 0.8600 - val_loss: 2.5189 - val_acc: 0.5233\n",
      "Epoch 2462/3000\n",
      "700/700 [==============================] - 0s 215us/step - loss: 0.4229 - acc: 0.8629 - val_loss: 2.5184 - val_acc: 0.5233\n",
      "Epoch 2463/3000\n",
      "700/700 [==============================] - 0s 210us/step - loss: 0.4236 - acc: 0.8643 - val_loss: 2.5240 - val_acc: 0.5233\n",
      "Epoch 2464/3000\n",
      "700/700 [==============================] - 0s 205us/step - loss: 0.4224 - acc: 0.8643 - val_loss: 2.5340 - val_acc: 0.5233\n",
      "Epoch 2465/3000\n",
      "700/700 [==============================] - 0s 205us/step - loss: 0.4217 - acc: 0.8629 - val_loss: 2.5401 - val_acc: 0.5267\n",
      "Epoch 2466/3000\n",
      "700/700 [==============================] - 0s 206us/step - loss: 0.4219 - acc: 0.8614 - val_loss: 2.5053 - val_acc: 0.5200\n",
      "Epoch 2467/3000\n",
      "700/700 [==============================] - 0s 198us/step - loss: 0.4225 - acc: 0.8629 - val_loss: 2.5191 - val_acc: 0.5200\n",
      "Epoch 2468/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.4219 - acc: 0.8629 - val_loss: 2.5256 - val_acc: 0.5200\n",
      "Epoch 2469/3000\n",
      "700/700 [==============================] - 0s 199us/step - loss: 0.4224 - acc: 0.8643 - val_loss: 2.5279 - val_acc: 0.5200\n",
      "Epoch 2470/3000\n",
      "700/700 [==============================] - 0s 196us/step - loss: 0.4220 - acc: 0.8629 - val_loss: 2.5095 - val_acc: 0.5233\n",
      "Epoch 2471/3000\n",
      "700/700 [==============================] - 0s 199us/step - loss: 0.4216 - acc: 0.8600 - val_loss: 2.5445 - val_acc: 0.5267\n",
      "Epoch 2472/3000\n",
      "700/700 [==============================] - 0s 202us/step - loss: 0.4222 - acc: 0.8643 - val_loss: 2.5298 - val_acc: 0.5233\n",
      "Epoch 2473/3000\n",
      "700/700 [==============================] - 0s 201us/step - loss: 0.4216 - acc: 0.8629 - val_loss: 2.5305 - val_acc: 0.5267\n",
      "Epoch 2474/3000\n",
      "700/700 [==============================] - 0s 209us/step - loss: 0.4221 - acc: 0.8614 - val_loss: 2.5413 - val_acc: 0.5267\n",
      "Epoch 2475/3000\n",
      "700/700 [==============================] - 0s 199us/step - loss: 0.4226 - acc: 0.8657 - val_loss: 2.5286 - val_acc: 0.5233\n",
      "Epoch 2476/3000\n",
      "700/700 [==============================] - 0s 188us/step - loss: 0.4224 - acc: 0.8657 - val_loss: 2.5366 - val_acc: 0.5267\n",
      "Epoch 2477/3000\n",
      "700/700 [==============================] - 0s 192us/step - loss: 0.4218 - acc: 0.8629 - val_loss: 2.5261 - val_acc: 0.5233\n",
      "Epoch 2478/3000\n",
      "700/700 [==============================] - 0s 179us/step - loss: 0.4215 - acc: 0.8629 - val_loss: 2.5381 - val_acc: 0.5233\n",
      "Epoch 2479/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 175us/step - loss: 0.4219 - acc: 0.8614 - val_loss: 2.5382 - val_acc: 0.5233\n",
      "Epoch 2480/3000\n",
      "700/700 [==============================] - 0s 189us/step - loss: 0.4217 - acc: 0.8657 - val_loss: 2.5419 - val_acc: 0.5233\n",
      "Epoch 2481/3000\n",
      "700/700 [==============================] - 0s 267us/step - loss: 0.4218 - acc: 0.8629 - val_loss: 2.5298 - val_acc: 0.5233\n",
      "Epoch 2482/3000\n",
      "700/700 [==============================] - 0s 202us/step - loss: 0.4210 - acc: 0.8629 - val_loss: 2.5290 - val_acc: 0.5233\n",
      "Epoch 2483/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 0.4216 - acc: 0.8657 - val_loss: 2.5316 - val_acc: 0.5233\n",
      "Epoch 2484/3000\n",
      "700/700 [==============================] - 0s 179us/step - loss: 0.4215 - acc: 0.8657 - val_loss: 2.5281 - val_acc: 0.5233\n",
      "Epoch 2485/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4211 - acc: 0.8600 - val_loss: 2.5391 - val_acc: 0.5233\n",
      "Epoch 2486/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4212 - acc: 0.8643 - val_loss: 2.5215 - val_acc: 0.5267\n",
      "Epoch 2487/3000\n",
      "700/700 [==============================] - 0s 183us/step - loss: 0.4215 - acc: 0.8657 - val_loss: 2.5292 - val_acc: 0.5267\n",
      "Epoch 2488/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4213 - acc: 0.8614 - val_loss: 2.5486 - val_acc: 0.5233\n",
      "Epoch 2489/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4213 - acc: 0.8657 - val_loss: 2.5323 - val_acc: 0.5267\n",
      "Epoch 2490/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4216 - acc: 0.8614 - val_loss: 2.5453 - val_acc: 0.5267\n",
      "Epoch 2491/3000\n",
      "700/700 [==============================] - 0s 172us/step - loss: 0.4212 - acc: 0.8643 - val_loss: 2.5418 - val_acc: 0.5267\n",
      "Epoch 2492/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4209 - acc: 0.8586 - val_loss: 2.5470 - val_acc: 0.5333\n",
      "Epoch 2493/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4216 - acc: 0.8657 - val_loss: 2.5462 - val_acc: 0.5233\n",
      "Epoch 2494/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4202 - acc: 0.8629 - val_loss: 2.5444 - val_acc: 0.5233\n",
      "Epoch 2495/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4219 - acc: 0.8671 - val_loss: 2.5302 - val_acc: 0.5233\n",
      "Epoch 2496/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4204 - acc: 0.8614 - val_loss: 2.5375 - val_acc: 0.5233\n",
      "Epoch 2497/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4202 - acc: 0.8671 - val_loss: 2.5401 - val_acc: 0.5233\n",
      "Epoch 2498/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.4206 - acc: 0.8643 - val_loss: 2.5476 - val_acc: 0.5267\n",
      "Epoch 2499/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4209 - acc: 0.8614 - val_loss: 2.5325 - val_acc: 0.5200\n",
      "Epoch 2500/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4205 - acc: 0.8657 - val_loss: 2.5322 - val_acc: 0.5233\n",
      "Epoch 2501/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.4203 - acc: 0.8686 - val_loss: 2.5445 - val_acc: 0.5233\n",
      "Epoch 2502/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4209 - acc: 0.8671 - val_loss: 2.5357 - val_acc: 0.5233\n",
      "Epoch 2503/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4201 - acc: 0.8629 - val_loss: 2.5359 - val_acc: 0.5233\n",
      "Epoch 2504/3000\n",
      "700/700 [==============================] - 0s 180us/step - loss: 0.4208 - acc: 0.8629 - val_loss: 2.5545 - val_acc: 0.5267\n",
      "Epoch 2505/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4209 - acc: 0.8614 - val_loss: 2.5568 - val_acc: 0.5267\n",
      "Epoch 2506/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4197 - acc: 0.8629 - val_loss: 2.5269 - val_acc: 0.5233\n",
      "Epoch 2507/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4208 - acc: 0.8629 - val_loss: 2.5473 - val_acc: 0.5233\n",
      "Epoch 2508/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4208 - acc: 0.8671 - val_loss: 2.5444 - val_acc: 0.5233\n",
      "Epoch 2509/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4197 - acc: 0.8600 - val_loss: 2.5314 - val_acc: 0.5233\n",
      "Epoch 2510/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4198 - acc: 0.8586 - val_loss: 2.5307 - val_acc: 0.5233\n",
      "Epoch 2511/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4213 - acc: 0.8643 - val_loss: 2.5394 - val_acc: 0.5267\n",
      "Epoch 2512/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.4199 - acc: 0.8614 - val_loss: 2.5431 - val_acc: 0.5267\n",
      "Epoch 2513/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4203 - acc: 0.8643 - val_loss: 2.5332 - val_acc: 0.5200\n",
      "Epoch 2514/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4205 - acc: 0.8657 - val_loss: 2.5512 - val_acc: 0.5233\n",
      "Epoch 2515/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4205 - acc: 0.8657 - val_loss: 2.5399 - val_acc: 0.5200\n",
      "Epoch 2516/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4201 - acc: 0.8643 - val_loss: 2.5455 - val_acc: 0.5233\n",
      "Epoch 2517/3000\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.4196 - acc: 0.8629 - val_loss: 2.5509 - val_acc: 0.5233\n",
      "Epoch 2518/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.4195 - acc: 0.8629 - val_loss: 2.5515 - val_acc: 0.5233\n",
      "Epoch 2519/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4196 - acc: 0.8629 - val_loss: 2.5449 - val_acc: 0.5167\n",
      "Epoch 2520/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4198 - acc: 0.8657 - val_loss: 2.5358 - val_acc: 0.5200\n",
      "Epoch 2521/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4196 - acc: 0.8657 - val_loss: 2.5393 - val_acc: 0.5233\n",
      "Epoch 2522/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4196 - acc: 0.8671 - val_loss: 2.5805 - val_acc: 0.5367\n",
      "Epoch 2523/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4201 - acc: 0.8629 - val_loss: 2.5462 - val_acc: 0.5267\n",
      "Epoch 2524/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4197 - acc: 0.8643 - val_loss: 2.5612 - val_acc: 0.5267\n",
      "Epoch 2525/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4192 - acc: 0.8657 - val_loss: 2.5405 - val_acc: 0.5167\n",
      "Epoch 2526/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4195 - acc: 0.8700 - val_loss: 2.5396 - val_acc: 0.5233\n",
      "Epoch 2527/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4195 - acc: 0.8614 - val_loss: 2.5474 - val_acc: 0.5233\n",
      "Epoch 2528/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4195 - acc: 0.8700 - val_loss: 2.5512 - val_acc: 0.5233\n",
      "Epoch 2529/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4194 - acc: 0.8629 - val_loss: 2.5490 - val_acc: 0.5233\n",
      "Epoch 2530/3000\n",
      "700/700 [==============================] - 0s 168us/step - loss: 0.4196 - acc: 0.8600 - val_loss: 2.5512 - val_acc: 0.5200\n",
      "Epoch 2531/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4193 - acc: 0.8671 - val_loss: 2.5631 - val_acc: 0.5333\n",
      "Epoch 2532/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4190 - acc: 0.8657 - val_loss: 2.5549 - val_acc: 0.5233\n",
      "Epoch 2533/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4189 - acc: 0.8643 - val_loss: 2.5701 - val_acc: 0.5267\n",
      "Epoch 2534/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4194 - acc: 0.8629 - val_loss: 2.5641 - val_acc: 0.5233\n",
      "Epoch 2535/3000\n",
      "700/700 [==============================] - 0s 166us/step - loss: 0.4188 - acc: 0.8629 - val_loss: 2.5707 - val_acc: 0.5300\n",
      "Epoch 2536/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4188 - acc: 0.8657 - val_loss: 2.5427 - val_acc: 0.5200\n",
      "Epoch 2537/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4182 - acc: 0.8629 - val_loss: 2.5529 - val_acc: 0.5233\n",
      "Epoch 2538/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 170us/step - loss: 0.4195 - acc: 0.8643 - val_loss: 2.5605 - val_acc: 0.5200\n",
      "Epoch 2539/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4183 - acc: 0.8657 - val_loss: 2.5470 - val_acc: 0.5233\n",
      "Epoch 2540/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4185 - acc: 0.8629 - val_loss: 2.5459 - val_acc: 0.5233\n",
      "Epoch 2541/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4188 - acc: 0.8629 - val_loss: 2.5542 - val_acc: 0.5233\n",
      "Epoch 2542/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4186 - acc: 0.8614 - val_loss: 2.5500 - val_acc: 0.5233\n",
      "Epoch 2543/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4190 - acc: 0.8700 - val_loss: 2.5569 - val_acc: 0.5200\n",
      "Epoch 2544/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4183 - acc: 0.8657 - val_loss: 2.5410 - val_acc: 0.5200\n",
      "Epoch 2545/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4183 - acc: 0.8643 - val_loss: 2.5571 - val_acc: 0.5267\n",
      "Epoch 2546/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4182 - acc: 0.8671 - val_loss: 2.5419 - val_acc: 0.5233\n",
      "Epoch 2547/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4185 - acc: 0.8671 - val_loss: 2.5768 - val_acc: 0.5333\n",
      "Epoch 2548/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4187 - acc: 0.8657 - val_loss: 2.5709 - val_acc: 0.5233\n",
      "Epoch 2549/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4183 - acc: 0.8643 - val_loss: 2.5770 - val_acc: 0.5300\n",
      "Epoch 2550/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4186 - acc: 0.8657 - val_loss: 2.5646 - val_acc: 0.5267\n",
      "Epoch 2551/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4182 - acc: 0.8657 - val_loss: 2.5596 - val_acc: 0.5233\n",
      "Epoch 2552/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4182 - acc: 0.8686 - val_loss: 2.5425 - val_acc: 0.5233\n",
      "Epoch 2553/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4182 - acc: 0.8657 - val_loss: 2.5533 - val_acc: 0.5200\n",
      "Epoch 2554/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4181 - acc: 0.8657 - val_loss: 2.5709 - val_acc: 0.5233\n",
      "Epoch 2555/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4186 - acc: 0.8643 - val_loss: 2.5641 - val_acc: 0.5200\n",
      "Epoch 2556/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4185 - acc: 0.8657 - val_loss: 2.5692 - val_acc: 0.5233\n",
      "Epoch 2557/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4184 - acc: 0.8657 - val_loss: 2.5499 - val_acc: 0.5167\n",
      "Epoch 2558/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4173 - acc: 0.8671 - val_loss: 2.5594 - val_acc: 0.5233\n",
      "Epoch 2559/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4185 - acc: 0.8686 - val_loss: 2.5688 - val_acc: 0.5233\n",
      "Epoch 2560/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4179 - acc: 0.8657 - val_loss: 2.5617 - val_acc: 0.5200\n",
      "Epoch 2561/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4178 - acc: 0.8657 - val_loss: 2.5585 - val_acc: 0.5233\n",
      "Epoch 2562/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4180 - acc: 0.8600 - val_loss: 2.5531 - val_acc: 0.5200\n",
      "Epoch 2563/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4174 - acc: 0.8671 - val_loss: 2.5602 - val_acc: 0.5233\n",
      "Epoch 2564/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4171 - acc: 0.8629 - val_loss: 2.5647 - val_acc: 0.5200\n",
      "Epoch 2565/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4179 - acc: 0.8686 - val_loss: 2.5723 - val_acc: 0.5267\n",
      "Epoch 2566/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4166 - acc: 0.8643 - val_loss: 2.5597 - val_acc: 0.5267\n",
      "Epoch 2567/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4174 - acc: 0.8671 - val_loss: 2.5719 - val_acc: 0.5233\n",
      "Epoch 2568/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4175 - acc: 0.8629 - val_loss: 2.5632 - val_acc: 0.5233\n",
      "Epoch 2569/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4169 - acc: 0.8657 - val_loss: 2.5700 - val_acc: 0.5267\n",
      "Epoch 2570/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4173 - acc: 0.8643 - val_loss: 2.5674 - val_acc: 0.5200\n",
      "Epoch 2571/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4180 - acc: 0.8657 - val_loss: 2.5657 - val_acc: 0.5200\n",
      "Epoch 2572/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4169 - acc: 0.8657 - val_loss: 2.5700 - val_acc: 0.5233\n",
      "Epoch 2573/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4173 - acc: 0.8657 - val_loss: 2.5732 - val_acc: 0.5200\n",
      "Epoch 2574/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4172 - acc: 0.8629 - val_loss: 2.5668 - val_acc: 0.5200\n",
      "Epoch 2575/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4173 - acc: 0.8671 - val_loss: 2.5730 - val_acc: 0.5233\n",
      "Epoch 2576/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4166 - acc: 0.8614 - val_loss: 2.5655 - val_acc: 0.5200\n",
      "Epoch 2577/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4169 - acc: 0.8657 - val_loss: 2.5722 - val_acc: 0.5233\n",
      "Epoch 2578/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4169 - acc: 0.8657 - val_loss: 2.5595 - val_acc: 0.5200\n",
      "Epoch 2579/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4179 - acc: 0.8671 - val_loss: 2.5628 - val_acc: 0.5233\n",
      "Epoch 2580/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4165 - acc: 0.8686 - val_loss: 2.5939 - val_acc: 0.5233\n",
      "Epoch 2581/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4170 - acc: 0.8586 - val_loss: 2.5881 - val_acc: 0.5267\n",
      "Epoch 2582/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4164 - acc: 0.8671 - val_loss: 2.6006 - val_acc: 0.5367\n",
      "Epoch 2583/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4168 - acc: 0.8643 - val_loss: 2.5801 - val_acc: 0.5233\n",
      "Epoch 2584/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4170 - acc: 0.8643 - val_loss: 2.5739 - val_acc: 0.5233\n",
      "Epoch 2585/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4164 - acc: 0.8657 - val_loss: 2.5846 - val_acc: 0.5267\n",
      "Epoch 2586/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4170 - acc: 0.8671 - val_loss: 2.5727 - val_acc: 0.5200\n",
      "Epoch 2587/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4164 - acc: 0.8657 - val_loss: 2.5864 - val_acc: 0.5267\n",
      "Epoch 2588/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4165 - acc: 0.8643 - val_loss: 2.5782 - val_acc: 0.5233\n",
      "Epoch 2589/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4165 - acc: 0.8629 - val_loss: 2.5786 - val_acc: 0.5233\n",
      "Epoch 2590/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4162 - acc: 0.8614 - val_loss: 2.5788 - val_acc: 0.5267\n",
      "Epoch 2591/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4164 - acc: 0.8671 - val_loss: 2.5673 - val_acc: 0.5233\n",
      "Epoch 2592/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4168 - acc: 0.8629 - val_loss: 2.5821 - val_acc: 0.5233\n",
      "Epoch 2593/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4159 - acc: 0.8657 - val_loss: 2.5816 - val_acc: 0.5233\n",
      "Epoch 2594/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4157 - acc: 0.8657 - val_loss: 2.5857 - val_acc: 0.5200\n",
      "Epoch 2595/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4154 - acc: 0.8657 - val_loss: 2.5952 - val_acc: 0.5267\n",
      "Epoch 2596/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4164 - acc: 0.8686 - val_loss: 2.5749 - val_acc: 0.5233\n",
      "Epoch 2597/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 156us/step - loss: 0.4173 - acc: 0.8671 - val_loss: 2.5700 - val_acc: 0.5200\n",
      "Epoch 2598/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4157 - acc: 0.8657 - val_loss: 2.5636 - val_acc: 0.5200\n",
      "Epoch 2599/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4161 - acc: 0.8657 - val_loss: 2.5621 - val_acc: 0.5233\n",
      "Epoch 2600/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4153 - acc: 0.8643 - val_loss: 2.5991 - val_acc: 0.5333\n",
      "Epoch 2601/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4166 - acc: 0.8643 - val_loss: 2.5843 - val_acc: 0.5233\n",
      "Epoch 2602/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4162 - acc: 0.8657 - val_loss: 2.5684 - val_acc: 0.5233\n",
      "Epoch 2603/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4156 - acc: 0.8729 - val_loss: 2.5790 - val_acc: 0.5233\n",
      "Epoch 2604/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4167 - acc: 0.8671 - val_loss: 2.5934 - val_acc: 0.5267\n",
      "Epoch 2605/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4155 - acc: 0.8643 - val_loss: 2.5802 - val_acc: 0.5233\n",
      "Epoch 2606/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4158 - acc: 0.8657 - val_loss: 2.5814 - val_acc: 0.5200\n",
      "Epoch 2607/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4155 - acc: 0.8657 - val_loss: 2.5822 - val_acc: 0.5233\n",
      "Epoch 2608/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4172 - acc: 0.8629 - val_loss: 2.5867 - val_acc: 0.5233\n",
      "Epoch 2609/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4171 - acc: 0.8686 - val_loss: 2.5929 - val_acc: 0.5267\n",
      "Epoch 2610/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4149 - acc: 0.8671 - val_loss: 2.5802 - val_acc: 0.5233\n",
      "Epoch 2611/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4151 - acc: 0.8729 - val_loss: 2.5898 - val_acc: 0.5233\n",
      "Epoch 2612/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4150 - acc: 0.8657 - val_loss: 2.5890 - val_acc: 0.5233\n",
      "Epoch 2613/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4151 - acc: 0.8671 - val_loss: 2.5949 - val_acc: 0.5300\n",
      "Epoch 2614/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4150 - acc: 0.8671 - val_loss: 2.5973 - val_acc: 0.5267\n",
      "Epoch 2615/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4158 - acc: 0.8686 - val_loss: 2.5828 - val_acc: 0.5233\n",
      "Epoch 2616/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4156 - acc: 0.8657 - val_loss: 2.5894 - val_acc: 0.5200\n",
      "Epoch 2617/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.4153 - acc: 0.8686 - val_loss: 2.5834 - val_acc: 0.5233\n",
      "Epoch 2618/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4155 - acc: 0.8657 - val_loss: 2.5798 - val_acc: 0.5200\n",
      "Epoch 2619/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4148 - acc: 0.8714 - val_loss: 2.5913 - val_acc: 0.5233\n",
      "Epoch 2620/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4148 - acc: 0.8671 - val_loss: 2.5931 - val_acc: 0.5233\n",
      "Epoch 2621/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4149 - acc: 0.8629 - val_loss: 2.5717 - val_acc: 0.5200\n",
      "Epoch 2622/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.4151 - acc: 0.8686 - val_loss: 2.5868 - val_acc: 0.5233\n",
      "Epoch 2623/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4150 - acc: 0.8671 - val_loss: 2.6048 - val_acc: 0.5233\n",
      "Epoch 2624/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4149 - acc: 0.8686 - val_loss: 2.5885 - val_acc: 0.5233\n",
      "Epoch 2625/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4145 - acc: 0.8671 - val_loss: 2.5910 - val_acc: 0.5233\n",
      "Epoch 2626/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4147 - acc: 0.8671 - val_loss: 2.5903 - val_acc: 0.5200\n",
      "Epoch 2627/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.4147 - acc: 0.8686 - val_loss: 2.5864 - val_acc: 0.5200\n",
      "Epoch 2628/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4144 - acc: 0.8671 - val_loss: 2.5992 - val_acc: 0.5267\n",
      "Epoch 2629/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4146 - acc: 0.8643 - val_loss: 2.5966 - val_acc: 0.5233\n",
      "Epoch 2630/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4150 - acc: 0.8657 - val_loss: 2.5988 - val_acc: 0.5267\n",
      "Epoch 2631/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4152 - acc: 0.8643 - val_loss: 2.5945 - val_acc: 0.5233\n",
      "Epoch 2632/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.4159 - acc: 0.8686 - val_loss: 2.5883 - val_acc: 0.5200\n",
      "Epoch 2633/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4144 - acc: 0.8671 - val_loss: 2.5956 - val_acc: 0.5267\n",
      "Epoch 2634/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4145 - acc: 0.8671 - val_loss: 2.5978 - val_acc: 0.5233\n",
      "Epoch 2635/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4139 - acc: 0.8657 - val_loss: 2.6004 - val_acc: 0.5233\n",
      "Epoch 2636/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4141 - acc: 0.8671 - val_loss: 2.5877 - val_acc: 0.5267\n",
      "Epoch 2637/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4152 - acc: 0.8700 - val_loss: 2.5901 - val_acc: 0.5200\n",
      "Epoch 2638/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4140 - acc: 0.8671 - val_loss: 2.5844 - val_acc: 0.5200\n",
      "Epoch 2639/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4142 - acc: 0.8657 - val_loss: 2.6032 - val_acc: 0.5233\n",
      "Epoch 2640/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4141 - acc: 0.8700 - val_loss: 2.5888 - val_acc: 0.5233\n",
      "Epoch 2641/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4148 - acc: 0.8657 - val_loss: 2.5891 - val_acc: 0.5167\n",
      "Epoch 2642/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4150 - acc: 0.8686 - val_loss: 2.6013 - val_acc: 0.5233\n",
      "Epoch 2643/3000\n",
      "700/700 [==============================] - 0s 142us/step - loss: 0.4146 - acc: 0.8643 - val_loss: 2.5932 - val_acc: 0.5233\n",
      "Epoch 2644/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4148 - acc: 0.8700 - val_loss: 2.5968 - val_acc: 0.5267\n",
      "Epoch 2645/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4137 - acc: 0.8671 - val_loss: 2.6080 - val_acc: 0.5233\n",
      "Epoch 2646/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4137 - acc: 0.8714 - val_loss: 2.5869 - val_acc: 0.5233\n",
      "Epoch 2647/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4142 - acc: 0.8657 - val_loss: 2.5921 - val_acc: 0.5267\n",
      "Epoch 2648/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4142 - acc: 0.8671 - val_loss: 2.5952 - val_acc: 0.5233\n",
      "Epoch 2649/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4136 - acc: 0.8686 - val_loss: 2.5976 - val_acc: 0.5200\n",
      "Epoch 2650/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4137 - acc: 0.8671 - val_loss: 2.6039 - val_acc: 0.5267\n",
      "Epoch 2651/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.4132 - acc: 0.8714 - val_loss: 2.6136 - val_acc: 0.5267\n",
      "Epoch 2652/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4143 - acc: 0.8643 - val_loss: 2.5955 - val_acc: 0.5233\n",
      "Epoch 2653/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4132 - acc: 0.8700 - val_loss: 2.5856 - val_acc: 0.5200\n",
      "Epoch 2654/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4140 - acc: 0.8686 - val_loss: 2.6084 - val_acc: 0.5267\n",
      "Epoch 2655/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4138 - acc: 0.8686 - val_loss: 2.5927 - val_acc: 0.5233\n",
      "Epoch 2656/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 147us/step - loss: 0.4138 - acc: 0.8671 - val_loss: 2.6055 - val_acc: 0.5267\n",
      "Epoch 2657/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4130 - acc: 0.8686 - val_loss: 2.5932 - val_acc: 0.5267\n",
      "Epoch 2658/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4133 - acc: 0.8686 - val_loss: 2.6087 - val_acc: 0.5267\n",
      "Epoch 2659/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4134 - acc: 0.8700 - val_loss: 2.6162 - val_acc: 0.5233\n",
      "Epoch 2660/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.4129 - acc: 0.8700 - val_loss: 2.6130 - val_acc: 0.5267\n",
      "Epoch 2661/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4130 - acc: 0.8686 - val_loss: 2.6234 - val_acc: 0.5267\n",
      "Epoch 2662/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4130 - acc: 0.8657 - val_loss: 2.6087 - val_acc: 0.5267\n",
      "Epoch 2663/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4129 - acc: 0.8714 - val_loss: 2.6172 - val_acc: 0.5267\n",
      "Epoch 2664/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4131 - acc: 0.8700 - val_loss: 2.6143 - val_acc: 0.5233\n",
      "Epoch 2665/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.4132 - acc: 0.8643 - val_loss: 2.6195 - val_acc: 0.5233\n",
      "Epoch 2666/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4127 - acc: 0.8671 - val_loss: 2.6021 - val_acc: 0.5267\n",
      "Epoch 2667/3000\n",
      "700/700 [==============================] - 0s 143us/step - loss: 0.4136 - acc: 0.8671 - val_loss: 2.6054 - val_acc: 0.5233\n",
      "Epoch 2668/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4127 - acc: 0.8714 - val_loss: 2.5927 - val_acc: 0.5233\n",
      "Epoch 2669/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.4133 - acc: 0.8700 - val_loss: 2.6028 - val_acc: 0.5267\n",
      "Epoch 2670/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4131 - acc: 0.8686 - val_loss: 2.6117 - val_acc: 0.5233\n",
      "Epoch 2671/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4137 - acc: 0.8700 - val_loss: 2.6001 - val_acc: 0.5267\n",
      "Epoch 2672/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4126 - acc: 0.8671 - val_loss: 2.6017 - val_acc: 0.5200\n",
      "Epoch 2673/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4122 - acc: 0.8671 - val_loss: 2.6082 - val_acc: 0.5267\n",
      "Epoch 2674/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.4131 - acc: 0.8671 - val_loss: 2.6150 - val_acc: 0.5233\n",
      "Epoch 2675/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4128 - acc: 0.8657 - val_loss: 2.6051 - val_acc: 0.5233\n",
      "Epoch 2676/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4132 - acc: 0.8714 - val_loss: 2.6139 - val_acc: 0.5233\n",
      "Epoch 2677/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4121 - acc: 0.8700 - val_loss: 2.6214 - val_acc: 0.5267\n",
      "Epoch 2678/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4129 - acc: 0.8700 - val_loss: 2.6067 - val_acc: 0.5233\n",
      "Epoch 2679/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4127 - acc: 0.8700 - val_loss: 2.6007 - val_acc: 0.5233\n",
      "Epoch 2680/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4121 - acc: 0.8686 - val_loss: 2.6290 - val_acc: 0.5300\n",
      "Epoch 2681/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4122 - acc: 0.8657 - val_loss: 2.5950 - val_acc: 0.5200\n",
      "Epoch 2682/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4121 - acc: 0.8686 - val_loss: 2.6209 - val_acc: 0.5267\n",
      "Epoch 2683/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4121 - acc: 0.8700 - val_loss: 2.6121 - val_acc: 0.5233\n",
      "Epoch 2684/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4119 - acc: 0.8671 - val_loss: 2.6146 - val_acc: 0.5233\n",
      "Epoch 2685/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4121 - acc: 0.8686 - val_loss: 2.6302 - val_acc: 0.5267\n",
      "Epoch 2686/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4120 - acc: 0.8714 - val_loss: 2.6279 - val_acc: 0.5233\n",
      "Epoch 2687/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4118 - acc: 0.8643 - val_loss: 2.6189 - val_acc: 0.5267\n",
      "Epoch 2688/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4119 - acc: 0.8657 - val_loss: 2.6137 - val_acc: 0.5300\n",
      "Epoch 2689/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4114 - acc: 0.8657 - val_loss: 2.6230 - val_acc: 0.5267\n",
      "Epoch 2690/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4129 - acc: 0.8686 - val_loss: 2.6094 - val_acc: 0.5267\n",
      "Epoch 2691/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4121 - acc: 0.8686 - val_loss: 2.6077 - val_acc: 0.5233\n",
      "Epoch 2692/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4118 - acc: 0.8700 - val_loss: 2.6164 - val_acc: 0.5267\n",
      "Epoch 2693/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4113 - acc: 0.8686 - val_loss: 2.6348 - val_acc: 0.5267\n",
      "Epoch 2694/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4122 - acc: 0.8671 - val_loss: 2.6292 - val_acc: 0.5267\n",
      "Epoch 2695/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4122 - acc: 0.8686 - val_loss: 2.6226 - val_acc: 0.5233\n",
      "Epoch 2696/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4114 - acc: 0.8671 - val_loss: 2.6268 - val_acc: 0.5233\n",
      "Epoch 2697/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4108 - acc: 0.8729 - val_loss: 2.6373 - val_acc: 0.5367\n",
      "Epoch 2698/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4118 - acc: 0.8714 - val_loss: 2.6211 - val_acc: 0.5233\n",
      "Epoch 2699/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4111 - acc: 0.8671 - val_loss: 2.6042 - val_acc: 0.5233\n",
      "Epoch 2700/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4116 - acc: 0.8686 - val_loss: 2.6352 - val_acc: 0.5333\n",
      "Epoch 2701/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4120 - acc: 0.8700 - val_loss: 2.6293 - val_acc: 0.5233\n",
      "Epoch 2702/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4109 - acc: 0.8671 - val_loss: 2.6311 - val_acc: 0.5233\n",
      "Epoch 2703/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4113 - acc: 0.8657 - val_loss: 2.6228 - val_acc: 0.5233\n",
      "Epoch 2704/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4110 - acc: 0.8686 - val_loss: 2.6326 - val_acc: 0.5300\n",
      "Epoch 2705/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4113 - acc: 0.8714 - val_loss: 2.6255 - val_acc: 0.5233\n",
      "Epoch 2706/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4114 - acc: 0.8671 - val_loss: 2.6162 - val_acc: 0.5267\n",
      "Epoch 2707/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4116 - acc: 0.8657 - val_loss: 2.6160 - val_acc: 0.5233\n",
      "Epoch 2708/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4106 - acc: 0.8671 - val_loss: 2.6259 - val_acc: 0.5267\n",
      "Epoch 2709/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.4122 - acc: 0.8700 - val_loss: 2.6257 - val_acc: 0.5267\n",
      "Epoch 2710/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4111 - acc: 0.8700 - val_loss: 2.6284 - val_acc: 0.5267\n",
      "Epoch 2711/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4110 - acc: 0.8671 - val_loss: 2.6174 - val_acc: 0.5200\n",
      "Epoch 2712/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4108 - acc: 0.8714 - val_loss: 2.6226 - val_acc: 0.5267\n",
      "Epoch 2713/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4110 - acc: 0.8643 - val_loss: 2.6377 - val_acc: 0.5267\n",
      "Epoch 2714/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4112 - acc: 0.8714 - val_loss: 2.6363 - val_acc: 0.5233\n",
      "Epoch 2715/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 156us/step - loss: 0.4106 - acc: 0.8714 - val_loss: 2.6165 - val_acc: 0.5200\n",
      "Epoch 2716/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4106 - acc: 0.8686 - val_loss: 2.6231 - val_acc: 0.5267\n",
      "Epoch 2717/3000\n",
      "700/700 [==============================] - 0s 171us/step - loss: 0.4108 - acc: 0.8671 - val_loss: 2.6183 - val_acc: 0.5267\n",
      "Epoch 2718/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4110 - acc: 0.8671 - val_loss: 2.6322 - val_acc: 0.5233\n",
      "Epoch 2719/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4108 - acc: 0.8686 - val_loss: 2.6227 - val_acc: 0.5200\n",
      "Epoch 2720/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4105 - acc: 0.8686 - val_loss: 2.6324 - val_acc: 0.5233\n",
      "Epoch 2721/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4109 - acc: 0.8686 - val_loss: 2.6246 - val_acc: 0.5233\n",
      "Epoch 2722/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4110 - acc: 0.8700 - val_loss: 2.6127 - val_acc: 0.5267\n",
      "Epoch 2723/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4104 - acc: 0.8671 - val_loss: 2.6422 - val_acc: 0.5267\n",
      "Epoch 2724/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4104 - acc: 0.8686 - val_loss: 2.6351 - val_acc: 0.5233\n",
      "Epoch 2725/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4102 - acc: 0.8700 - val_loss: 2.6337 - val_acc: 0.5233\n",
      "Epoch 2726/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4109 - acc: 0.8700 - val_loss: 2.6278 - val_acc: 0.5267\n",
      "Epoch 2727/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4104 - acc: 0.8657 - val_loss: 2.6249 - val_acc: 0.5267\n",
      "Epoch 2728/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4108 - acc: 0.8700 - val_loss: 2.6396 - val_acc: 0.5233\n",
      "Epoch 2729/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4102 - acc: 0.8700 - val_loss: 2.6294 - val_acc: 0.5233\n",
      "Epoch 2730/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4101 - acc: 0.8743 - val_loss: 2.6496 - val_acc: 0.5300\n",
      "Epoch 2731/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4103 - acc: 0.8714 - val_loss: 2.6218 - val_acc: 0.5267\n",
      "Epoch 2732/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4099 - acc: 0.8686 - val_loss: 2.6366 - val_acc: 0.5267\n",
      "Epoch 2733/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4098 - acc: 0.8714 - val_loss: 2.6236 - val_acc: 0.5267\n",
      "Epoch 2734/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4104 - acc: 0.8729 - val_loss: 2.6255 - val_acc: 0.5233\n",
      "Epoch 2735/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4101 - acc: 0.8714 - val_loss: 2.6269 - val_acc: 0.5267\n",
      "Epoch 2736/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4103 - acc: 0.8671 - val_loss: 2.6378 - val_acc: 0.5233\n",
      "Epoch 2737/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4100 - acc: 0.8700 - val_loss: 2.6321 - val_acc: 0.5233\n",
      "Epoch 2738/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4095 - acc: 0.8671 - val_loss: 2.6329 - val_acc: 0.5233\n",
      "Epoch 2739/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4103 - acc: 0.8714 - val_loss: 2.6425 - val_acc: 0.5267\n",
      "Epoch 2740/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4103 - acc: 0.8686 - val_loss: 2.6357 - val_acc: 0.5233\n",
      "Epoch 2741/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4098 - acc: 0.8729 - val_loss: 2.6404 - val_acc: 0.5267\n",
      "Epoch 2742/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4099 - acc: 0.8686 - val_loss: 2.6354 - val_acc: 0.5200\n",
      "Epoch 2743/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4095 - acc: 0.8729 - val_loss: 2.6361 - val_acc: 0.5233\n",
      "Epoch 2744/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4097 - acc: 0.8643 - val_loss: 2.6306 - val_acc: 0.5233\n",
      "Epoch 2745/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4101 - acc: 0.8700 - val_loss: 2.6340 - val_acc: 0.5233\n",
      "Epoch 2746/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4102 - acc: 0.8714 - val_loss: 2.6368 - val_acc: 0.5267\n",
      "Epoch 2747/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4092 - acc: 0.8657 - val_loss: 2.6437 - val_acc: 0.5267\n",
      "Epoch 2748/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4102 - acc: 0.8729 - val_loss: 2.6456 - val_acc: 0.5233\n",
      "Epoch 2749/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4095 - acc: 0.8657 - val_loss: 2.6448 - val_acc: 0.5267\n",
      "Epoch 2750/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4096 - acc: 0.8700 - val_loss: 2.6463 - val_acc: 0.5233\n",
      "Epoch 2751/3000\n",
      "700/700 [==============================] - 0s 146us/step - loss: 0.4093 - acc: 0.8700 - val_loss: 2.6202 - val_acc: 0.5200\n",
      "Epoch 2752/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4098 - acc: 0.8686 - val_loss: 2.6524 - val_acc: 0.5300\n",
      "Epoch 2753/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4098 - acc: 0.8657 - val_loss: 2.6464 - val_acc: 0.5267\n",
      "Epoch 2754/3000\n",
      "700/700 [==============================] - 0s 229us/step - loss: 0.4095 - acc: 0.8671 - val_loss: 2.6371 - val_acc: 0.5267\n",
      "Epoch 2755/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4093 - acc: 0.8700 - val_loss: 2.6473 - val_acc: 0.5267\n",
      "Epoch 2756/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4090 - acc: 0.8686 - val_loss: 2.6323 - val_acc: 0.5200\n",
      "Epoch 2757/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4097 - acc: 0.8700 - val_loss: 2.6345 - val_acc: 0.5267\n",
      "Epoch 2758/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4094 - acc: 0.8643 - val_loss: 2.6502 - val_acc: 0.5267\n",
      "Epoch 2759/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4088 - acc: 0.8700 - val_loss: 2.6515 - val_acc: 0.5267\n",
      "Epoch 2760/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4103 - acc: 0.8671 - val_loss: 2.6405 - val_acc: 0.5233\n",
      "Epoch 2761/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4089 - acc: 0.8714 - val_loss: 2.6210 - val_acc: 0.5167\n",
      "Epoch 2762/3000\n",
      "700/700 [==============================] - 0s 147us/step - loss: 0.4095 - acc: 0.8671 - val_loss: 2.6489 - val_acc: 0.5267\n",
      "Epoch 2763/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4086 - acc: 0.8714 - val_loss: 2.6509 - val_acc: 0.5267\n",
      "Epoch 2764/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4095 - acc: 0.8729 - val_loss: 2.6321 - val_acc: 0.5233\n",
      "Epoch 2765/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4098 - acc: 0.8714 - val_loss: 2.6519 - val_acc: 0.5267\n",
      "Epoch 2766/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4088 - acc: 0.8729 - val_loss: 2.6388 - val_acc: 0.5233\n",
      "Epoch 2767/3000\n",
      "700/700 [==============================] - 0s 150us/step - loss: 0.4083 - acc: 0.8743 - val_loss: 2.6479 - val_acc: 0.5267\n",
      "Epoch 2768/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4084 - acc: 0.8686 - val_loss: 2.6382 - val_acc: 0.5267\n",
      "Epoch 2769/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4090 - acc: 0.8686 - val_loss: 2.6327 - val_acc: 0.5167\n",
      "Epoch 2770/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4088 - acc: 0.8700 - val_loss: 2.6705 - val_acc: 0.5300\n",
      "Epoch 2771/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4090 - acc: 0.8729 - val_loss: 2.6541 - val_acc: 0.5267\n",
      "Epoch 2772/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4086 - acc: 0.8700 - val_loss: 2.6660 - val_acc: 0.5300\n",
      "Epoch 2773/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4087 - acc: 0.8643 - val_loss: 2.6602 - val_acc: 0.5300\n",
      "Epoch 2774/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 147us/step - loss: 0.4086 - acc: 0.8700 - val_loss: 2.6735 - val_acc: 0.5267\n",
      "Epoch 2775/3000\n",
      "700/700 [==============================] - 0s 159us/step - loss: 0.4084 - acc: 0.8686 - val_loss: 2.6580 - val_acc: 0.5267\n",
      "Epoch 2776/3000\n",
      "700/700 [==============================] - 0s 148us/step - loss: 0.4085 - acc: 0.8700 - val_loss: 2.6528 - val_acc: 0.5267\n",
      "Epoch 2777/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4079 - acc: 0.8657 - val_loss: 2.6548 - val_acc: 0.5300\n",
      "Epoch 2778/3000\n",
      "700/700 [==============================] - 0s 145us/step - loss: 0.4084 - acc: 0.8729 - val_loss: 2.6575 - val_acc: 0.5267\n",
      "Epoch 2779/3000\n",
      "700/700 [==============================] - 0s 152us/step - loss: 0.4082 - acc: 0.8686 - val_loss: 2.6496 - val_acc: 0.5267\n",
      "Epoch 2780/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4082 - acc: 0.8714 - val_loss: 2.6511 - val_acc: 0.5233\n",
      "Epoch 2781/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4085 - acc: 0.8686 - val_loss: 2.6449 - val_acc: 0.5267\n",
      "Epoch 2782/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4076 - acc: 0.8714 - val_loss: 2.6572 - val_acc: 0.5300\n",
      "Epoch 2783/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4090 - acc: 0.8743 - val_loss: 2.6659 - val_acc: 0.5300\n",
      "Epoch 2784/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4079 - acc: 0.8671 - val_loss: 2.6534 - val_acc: 0.5300\n",
      "Epoch 2785/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4075 - acc: 0.8743 - val_loss: 2.6598 - val_acc: 0.5300\n",
      "Epoch 2786/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4082 - acc: 0.8714 - val_loss: 2.6606 - val_acc: 0.5300\n",
      "Epoch 2787/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4079 - acc: 0.8729 - val_loss: 2.6523 - val_acc: 0.5233\n",
      "Epoch 2788/3000\n",
      "700/700 [==============================] - 0s 155us/step - loss: 0.4078 - acc: 0.8714 - val_loss: 2.6744 - val_acc: 0.5267\n",
      "Epoch 2789/3000\n",
      "700/700 [==============================] - 0s 179us/step - loss: 0.4080 - acc: 0.8686 - val_loss: 2.6707 - val_acc: 0.5300\n",
      "Epoch 2790/3000\n",
      "700/700 [==============================] - 0s 158us/step - loss: 0.4076 - acc: 0.8671 - val_loss: 2.6509 - val_acc: 0.5200\n",
      "Epoch 2791/3000\n",
      "700/700 [==============================] - 0s 144us/step - loss: 0.4075 - acc: 0.8729 - val_loss: 2.6620 - val_acc: 0.5267\n",
      "Epoch 2792/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4076 - acc: 0.8714 - val_loss: 2.6383 - val_acc: 0.5167\n",
      "Epoch 2793/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4084 - acc: 0.8671 - val_loss: 2.6520 - val_acc: 0.5233\n",
      "Epoch 2794/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4079 - acc: 0.8729 - val_loss: 2.6256 - val_acc: 0.5167\n",
      "Epoch 2795/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4076 - acc: 0.8686 - val_loss: 2.6576 - val_acc: 0.5267\n",
      "Epoch 2796/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4073 - acc: 0.8729 - val_loss: 2.6593 - val_acc: 0.5300\n",
      "Epoch 2797/3000\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.4075 - acc: 0.8743 - val_loss: 2.6674 - val_acc: 0.5300\n",
      "Epoch 2798/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4081 - acc: 0.8729 - val_loss: 2.6651 - val_acc: 0.5267\n",
      "Epoch 2799/3000\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.4073 - acc: 0.8686 - val_loss: 2.6481 - val_acc: 0.5167\n",
      "Epoch 2800/3000\n",
      "700/700 [==============================] - 0s 153us/step - loss: 0.4075 - acc: 0.8714 - val_loss: 2.6596 - val_acc: 0.5233\n",
      "Epoch 2801/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4076 - acc: 0.8714 - val_loss: 2.6655 - val_acc: 0.5300\n",
      "Epoch 2802/3000\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.4074 - acc: 0.8700 - val_loss: 2.6708 - val_acc: 0.5300\n",
      "Epoch 2803/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4072 - acc: 0.8714 - val_loss: 2.6695 - val_acc: 0.5300\n",
      "Epoch 2804/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4072 - acc: 0.8686 - val_loss: 2.6637 - val_acc: 0.5267\n",
      "Epoch 2805/3000\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.4063 - acc: 0.8671 - val_loss: 2.6349 - val_acc: 0.5133\n",
      "Epoch 2806/3000\n",
      "700/700 [==============================] - 0s 151us/step - loss: 0.4076 - acc: 0.8686 - val_loss: 2.6502 - val_acc: 0.5267\n",
      "Epoch 2807/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4070 - acc: 0.8714 - val_loss: 2.6676 - val_acc: 0.5267\n",
      "Epoch 2808/3000\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.4072 - acc: 0.8729 - val_loss: 2.6594 - val_acc: 0.5233\n",
      "Epoch 2809/3000\n",
      "700/700 [==============================] - 0s 163us/step - loss: 0.4074 - acc: 0.8700 - val_loss: 2.6566 - val_acc: 0.5300\n",
      "Epoch 2810/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4073 - acc: 0.8714 - val_loss: 2.6686 - val_acc: 0.5267\n",
      "Epoch 2811/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4069 - acc: 0.8700 - val_loss: 2.6583 - val_acc: 0.5200\n",
      "Epoch 2812/3000\n",
      "700/700 [==============================] - 0s 165us/step - loss: 0.4072 - acc: 0.8729 - val_loss: 2.6643 - val_acc: 0.5267\n",
      "Epoch 2813/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4072 - acc: 0.8700 - val_loss: 2.6797 - val_acc: 0.5233\n",
      "Epoch 2814/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4072 - acc: 0.8729 - val_loss: 2.6604 - val_acc: 0.5300\n",
      "Epoch 2815/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4064 - acc: 0.8757 - val_loss: 2.6637 - val_acc: 0.5267\n",
      "Epoch 2816/3000\n",
      "700/700 [==============================] - 0s 161us/step - loss: 0.4070 - acc: 0.8686 - val_loss: 2.6653 - val_acc: 0.5233\n",
      "Epoch 2817/3000\n",
      "700/700 [==============================] - 0s 162us/step - loss: 0.4066 - acc: 0.8700 - val_loss: 2.6527 - val_acc: 0.5300\n",
      "Epoch 2818/3000\n",
      "700/700 [==============================] - 0s 180us/step - loss: 0.4069 - acc: 0.8714 - val_loss: 2.6681 - val_acc: 0.5300\n",
      "Epoch 2819/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4066 - acc: 0.8714 - val_loss: 2.6687 - val_acc: 0.5267\n",
      "Epoch 2820/3000\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.4063 - acc: 0.8700 - val_loss: 2.6595 - val_acc: 0.5233\n",
      "Epoch 2821/3000\n",
      "700/700 [==============================] - 0s 186us/step - loss: 0.4074 - acc: 0.8729 - val_loss: 2.6711 - val_acc: 0.5233\n",
      "Epoch 2822/3000\n",
      "700/700 [==============================] - 0s 173us/step - loss: 0.4065 - acc: 0.8729 - val_loss: 2.6794 - val_acc: 0.5267\n",
      "Epoch 2823/3000\n",
      "700/700 [==============================] - 0s 176us/step - loss: 0.4067 - acc: 0.8714 - val_loss: 2.6601 - val_acc: 0.5267\n",
      "Epoch 2824/3000\n",
      "700/700 [==============================] - 0s 184us/step - loss: 0.4065 - acc: 0.8729 - val_loss: 2.6760 - val_acc: 0.5300\n",
      "Epoch 2825/3000\n",
      "700/700 [==============================] - 0s 180us/step - loss: 0.4064 - acc: 0.8700 - val_loss: 2.6642 - val_acc: 0.5300\n",
      "Epoch 2826/3000\n",
      "700/700 [==============================] - 0s 196us/step - loss: 0.4059 - acc: 0.8714 - val_loss: 2.6721 - val_acc: 0.5267\n",
      "Epoch 2827/3000\n",
      "700/700 [==============================] - 0s 174us/step - loss: 0.4062 - acc: 0.8700 - val_loss: 2.6622 - val_acc: 0.5267\n",
      "Epoch 2828/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4069 - acc: 0.8729 - val_loss: 2.6739 - val_acc: 0.5333\n",
      "Epoch 2829/3000\n",
      "700/700 [==============================] - 0s 183us/step - loss: 0.4066 - acc: 0.8757 - val_loss: 2.6880 - val_acc: 0.5300\n",
      "Epoch 2830/3000\n",
      "700/700 [==============================] - 0s 193us/step - loss: 0.4064 - acc: 0.8743 - val_loss: 2.6586 - val_acc: 0.5267\n",
      "Epoch 2831/3000\n",
      "700/700 [==============================] - 0s 222us/step - loss: 0.4068 - acc: 0.8686 - val_loss: 2.6731 - val_acc: 0.5300\n",
      "Epoch 2832/3000\n",
      "700/700 [==============================] - 0s 190us/step - loss: 0.4057 - acc: 0.8700 - val_loss: 2.6620 - val_acc: 0.5300\n",
      "Epoch 2833/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 192us/step - loss: 0.4058 - acc: 0.8729 - val_loss: 2.6829 - val_acc: 0.5267\n",
      "Epoch 2834/3000\n",
      "700/700 [==============================] - 0s 198us/step - loss: 0.4060 - acc: 0.8714 - val_loss: 2.6714 - val_acc: 0.5267\n",
      "Epoch 2835/3000\n",
      "700/700 [==============================] - 0s 205us/step - loss: 0.4054 - acc: 0.8729 - val_loss: 2.6675 - val_acc: 0.5233\n",
      "Epoch 2836/3000\n",
      "700/700 [==============================] - 0s 177us/step - loss: 0.4064 - acc: 0.8743 - val_loss: 2.6797 - val_acc: 0.5267\n",
      "Epoch 2837/3000\n",
      "700/700 [==============================] - 0s 181us/step - loss: 0.4064 - acc: 0.8729 - val_loss: 2.6773 - val_acc: 0.5267\n",
      "Epoch 2838/3000\n",
      "700/700 [==============================] - 0s 196us/step - loss: 0.4062 - acc: 0.8714 - val_loss: 2.6819 - val_acc: 0.5300\n",
      "Epoch 2839/3000\n",
      "700/700 [==============================] - 0s 175us/step - loss: 0.4057 - acc: 0.8686 - val_loss: 2.6721 - val_acc: 0.5267\n",
      "Epoch 2840/3000\n",
      "700/700 [==============================] - 0s 211us/step - loss: 0.4057 - acc: 0.8743 - val_loss: 2.6594 - val_acc: 0.5167\n",
      "Epoch 2841/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4052 - acc: 0.8700 - val_loss: 2.6701 - val_acc: 0.5267\n",
      "Epoch 2842/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4057 - acc: 0.8743 - val_loss: 2.6758 - val_acc: 0.5267\n",
      "Epoch 2843/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4062 - acc: 0.8686 - val_loss: 2.6673 - val_acc: 0.5233\n",
      "Epoch 2844/3000\n",
      "700/700 [==============================] - 0s 195us/step - loss: 0.4056 - acc: 0.8729 - val_loss: 2.6877 - val_acc: 0.5300\n",
      "Epoch 2845/3000\n",
      "700/700 [==============================] - 0s 190us/step - loss: 0.4056 - acc: 0.8729 - val_loss: 2.6698 - val_acc: 0.5300\n",
      "Epoch 2846/3000\n",
      "700/700 [==============================] - 0s 201us/step - loss: 0.4054 - acc: 0.8671 - val_loss: 2.6898 - val_acc: 0.5300\n",
      "Epoch 2847/3000\n",
      "700/700 [==============================] - 0s 189us/step - loss: 0.4067 - acc: 0.8771 - val_loss: 2.6767 - val_acc: 0.5267\n",
      "Epoch 2848/3000\n",
      "700/700 [==============================] - 0s 194us/step - loss: 0.4063 - acc: 0.8729 - val_loss: 2.6675 - val_acc: 0.5233\n",
      "Epoch 2849/3000\n",
      "700/700 [==============================] - 0s 191us/step - loss: 0.4053 - acc: 0.8700 - val_loss: 2.7024 - val_acc: 0.5333\n",
      "Epoch 2850/3000\n",
      "700/700 [==============================] - 0s 200us/step - loss: 0.4057 - acc: 0.8714 - val_loss: 2.6772 - val_acc: 0.5233\n",
      "Epoch 2851/3000\n",
      "700/700 [==============================] - 0s 197us/step - loss: 0.4055 - acc: 0.8714 - val_loss: 2.6782 - val_acc: 0.5267\n",
      "Epoch 2852/3000\n",
      "700/700 [==============================] - 0s 201us/step - loss: 0.4053 - acc: 0.8729 - val_loss: 2.6942 - val_acc: 0.5300\n",
      "Epoch 2853/3000\n",
      "700/700 [==============================] - 0s 236us/step - loss: 0.4054 - acc: 0.8743 - val_loss: 2.6821 - val_acc: 0.5300\n",
      "Epoch 2854/3000\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.4056 - acc: 0.8743 - val_loss: 2.6692 - val_acc: 0.5233\n",
      "Epoch 2855/3000\n",
      "700/700 [==============================] - 0s 186us/step - loss: 0.4057 - acc: 0.8700 - val_loss: 2.7091 - val_acc: 0.5267\n",
      "Epoch 2856/3000\n",
      "700/700 [==============================] - 0s 189us/step - loss: 0.4053 - acc: 0.8757 - val_loss: 2.6876 - val_acc: 0.5300\n",
      "Epoch 2857/3000\n",
      "700/700 [==============================] - 0s 193us/step - loss: 0.4057 - acc: 0.8743 - val_loss: 2.6738 - val_acc: 0.5300\n",
      "Epoch 2858/3000\n",
      "700/700 [==============================] - 0s 217us/step - loss: 0.4052 - acc: 0.8729 - val_loss: 2.6963 - val_acc: 0.5233\n",
      "Epoch 2859/3000\n",
      "700/700 [==============================] - 0s 200us/step - loss: 0.4050 - acc: 0.8714 - val_loss: 2.6663 - val_acc: 0.5233\n",
      "Epoch 2860/3000\n",
      "700/700 [==============================] - 0s 212us/step - loss: 0.4053 - acc: 0.8729 - val_loss: 2.6770 - val_acc: 0.5233\n",
      "Epoch 2861/3000\n",
      "700/700 [==============================] - 0s 212us/step - loss: 0.4048 - acc: 0.8714 - val_loss: 2.6844 - val_acc: 0.5267\n",
      "Epoch 2862/3000\n",
      "700/700 [==============================] - 0s 212us/step - loss: 0.4055 - acc: 0.8743 - val_loss: 2.6863 - val_acc: 0.5267\n",
      "Epoch 2863/3000\n",
      "700/700 [==============================] - 0s 214us/step - loss: 0.4054 - acc: 0.8743 - val_loss: 2.6970 - val_acc: 0.5267\n",
      "Epoch 2864/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 0.4048 - acc: 0.8743 - val_loss: 2.6943 - val_acc: 0.5300\n",
      "Epoch 2865/3000\n",
      "700/700 [==============================] - 0s 210us/step - loss: 0.4051 - acc: 0.8686 - val_loss: 2.6929 - val_acc: 0.5300\n",
      "Epoch 2866/3000\n",
      "700/700 [==============================] - 0s 228us/step - loss: 0.4047 - acc: 0.8700 - val_loss: 2.7043 - val_acc: 0.5267\n",
      "Epoch 2867/3000\n",
      "700/700 [==============================] - 0s 217us/step - loss: 0.4047 - acc: 0.8743 - val_loss: 2.6793 - val_acc: 0.5267\n",
      "Epoch 2868/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 0.4046 - acc: 0.8729 - val_loss: 2.7060 - val_acc: 0.5300\n",
      "Epoch 2869/3000\n",
      "700/700 [==============================] - 0s 220us/step - loss: 0.4053 - acc: 0.8743 - val_loss: 2.6880 - val_acc: 0.5300\n",
      "Epoch 2870/3000\n",
      "700/700 [==============================] - 0s 212us/step - loss: 0.4050 - acc: 0.8757 - val_loss: 2.6887 - val_acc: 0.5300\n",
      "Epoch 2871/3000\n",
      "700/700 [==============================] - 0s 211us/step - loss: 0.4044 - acc: 0.8714 - val_loss: 2.6945 - val_acc: 0.5300\n",
      "Epoch 2872/3000\n",
      "700/700 [==============================] - 0s 236us/step - loss: 0.4044 - acc: 0.8743 - val_loss: 2.6956 - val_acc: 0.5267\n",
      "Epoch 2873/3000\n",
      "700/700 [==============================] - 0s 251us/step - loss: 0.4051 - acc: 0.8700 - val_loss: 2.6850 - val_acc: 0.5300\n",
      "Epoch 2874/3000\n",
      "700/700 [==============================] - 0s 211us/step - loss: 0.4049 - acc: 0.8714 - val_loss: 2.6874 - val_acc: 0.5300\n",
      "Epoch 2875/3000\n",
      "700/700 [==============================] - 0s 206us/step - loss: 0.4045 - acc: 0.8729 - val_loss: 2.6874 - val_acc: 0.5300\n",
      "Epoch 2876/3000\n",
      "700/700 [==============================] - 0s 203us/step - loss: 0.4046 - acc: 0.8771 - val_loss: 2.6937 - val_acc: 0.5267\n",
      "Epoch 2877/3000\n",
      "700/700 [==============================] - 0s 213us/step - loss: 0.4044 - acc: 0.8743 - val_loss: 2.6984 - val_acc: 0.5267\n",
      "Epoch 2878/3000\n",
      "700/700 [==============================] - 0s 234us/step - loss: 0.4045 - acc: 0.8743 - val_loss: 2.6888 - val_acc: 0.5300\n",
      "Epoch 2879/3000\n",
      "700/700 [==============================] - 0s 258us/step - loss: 0.4045 - acc: 0.8743 - val_loss: 2.7077 - val_acc: 0.5267\n",
      "Epoch 2880/3000\n",
      "700/700 [==============================] - 0s 243us/step - loss: 0.4041 - acc: 0.8700 - val_loss: 2.6768 - val_acc: 0.5200\n",
      "Epoch 2881/3000\n",
      "700/700 [==============================] - 0s 241us/step - loss: 0.4045 - acc: 0.8729 - val_loss: 2.6939 - val_acc: 0.5233\n",
      "Epoch 2882/3000\n",
      "700/700 [==============================] - 0s 218us/step - loss: 0.4043 - acc: 0.8714 - val_loss: 2.6931 - val_acc: 0.5267\n",
      "Epoch 2883/3000\n",
      "700/700 [==============================] - 0s 229us/step - loss: 0.4041 - acc: 0.8743 - val_loss: 2.6903 - val_acc: 0.5267\n",
      "Epoch 2884/3000\n",
      "700/700 [==============================] - 0s 250us/step - loss: 0.4038 - acc: 0.8757 - val_loss: 2.6986 - val_acc: 0.5267\n",
      "Epoch 2885/3000\n",
      "700/700 [==============================] - 0s 271us/step - loss: 0.4043 - acc: 0.8714 - val_loss: 2.6847 - val_acc: 0.5267\n",
      "Epoch 2886/3000\n",
      "700/700 [==============================] - 0s 226us/step - loss: 0.4045 - acc: 0.8729 - val_loss: 2.6866 - val_acc: 0.5233\n",
      "Epoch 2887/3000\n",
      "700/700 [==============================] - 0s 221us/step - loss: 0.4044 - acc: 0.8729 - val_loss: 2.7048 - val_acc: 0.5267\n",
      "Epoch 2888/3000\n",
      "700/700 [==============================] - 0s 244us/step - loss: 0.4035 - acc: 0.8729 - val_loss: 2.6875 - val_acc: 0.5300\n",
      "Epoch 2889/3000\n",
      "700/700 [==============================] - 0s 226us/step - loss: 0.4035 - acc: 0.8729 - val_loss: 2.6977 - val_acc: 0.5333\n",
      "Epoch 2890/3000\n",
      "700/700 [==============================] - 0s 254us/step - loss: 0.4041 - acc: 0.8771 - val_loss: 2.6865 - val_acc: 0.5300\n",
      "Epoch 2891/3000\n",
      "700/700 [==============================] - 0s 242us/step - loss: 0.4045 - acc: 0.8714 - val_loss: 2.7015 - val_acc: 0.5267\n",
      "Epoch 2892/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 255us/step - loss: 0.4039 - acc: 0.8757 - val_loss: 2.7013 - val_acc: 0.5300\n",
      "Epoch 2893/3000\n",
      "700/700 [==============================] - 0s 246us/step - loss: 0.4034 - acc: 0.8729 - val_loss: 2.6805 - val_acc: 0.5233\n",
      "Epoch 2894/3000\n",
      "700/700 [==============================] - 0s 243us/step - loss: 0.4038 - acc: 0.8743 - val_loss: 2.6992 - val_acc: 0.5300\n",
      "Epoch 2895/3000\n",
      "700/700 [==============================] - 0s 247us/step - loss: 0.4037 - acc: 0.8743 - val_loss: 2.6957 - val_acc: 0.5267\n",
      "Epoch 2896/3000\n",
      "700/700 [==============================] - 0s 252us/step - loss: 0.4042 - acc: 0.8729 - val_loss: 2.6955 - val_acc: 0.5267\n",
      "Epoch 2897/3000\n",
      "700/700 [==============================] - 0s 251us/step - loss: 0.4031 - acc: 0.8700 - val_loss: 2.7135 - val_acc: 0.5300\n",
      "Epoch 2898/3000\n",
      "700/700 [==============================] - 0s 247us/step - loss: 0.4045 - acc: 0.8771 - val_loss: 2.7099 - val_acc: 0.5233\n",
      "Epoch 2899/3000\n",
      "700/700 [==============================] - 0s 240us/step - loss: 0.4042 - acc: 0.8743 - val_loss: 2.7026 - val_acc: 0.5200\n",
      "Epoch 2900/3000\n",
      "700/700 [==============================] - 0s 245us/step - loss: 0.4035 - acc: 0.8729 - val_loss: 2.7093 - val_acc: 0.5233\n",
      "Epoch 2901/3000\n",
      "700/700 [==============================] - 0s 244us/step - loss: 0.4033 - acc: 0.8686 - val_loss: 2.6975 - val_acc: 0.5200\n",
      "Epoch 2902/3000\n",
      "700/700 [==============================] - 0s 246us/step - loss: 0.4039 - acc: 0.8786 - val_loss: 2.6916 - val_acc: 0.5200\n",
      "Epoch 2903/3000\n",
      "700/700 [==============================] - 0s 250us/step - loss: 0.4034 - acc: 0.8686 - val_loss: 2.6989 - val_acc: 0.5233\n",
      "Epoch 2904/3000\n",
      "700/700 [==============================] - 0s 242us/step - loss: 0.4036 - acc: 0.8686 - val_loss: 2.6945 - val_acc: 0.5267\n",
      "Epoch 2905/3000\n",
      "700/700 [==============================] - 0s 236us/step - loss: 0.4031 - acc: 0.8700 - val_loss: 2.7132 - val_acc: 0.5267\n",
      "Epoch 2906/3000\n",
      "700/700 [==============================] - 0s 249us/step - loss: 0.4030 - acc: 0.8743 - val_loss: 2.7000 - val_acc: 0.5267\n",
      "Epoch 2907/3000\n",
      "700/700 [==============================] - 0s 242us/step - loss: 0.4036 - acc: 0.8714 - val_loss: 2.7097 - val_acc: 0.5267\n",
      "Epoch 2908/3000\n",
      "700/700 [==============================] - 0s 240us/step - loss: 0.4033 - acc: 0.8757 - val_loss: 2.7174 - val_acc: 0.5267\n",
      "Epoch 2909/3000\n",
      "700/700 [==============================] - 0s 245us/step - loss: 0.4047 - acc: 0.8729 - val_loss: 2.6940 - val_acc: 0.5233\n",
      "Epoch 2910/3000\n",
      "700/700 [==============================] - 0s 232us/step - loss: 0.4027 - acc: 0.8729 - val_loss: 2.6942 - val_acc: 0.5267\n",
      "Epoch 2911/3000\n",
      "700/700 [==============================] - 0s 249us/step - loss: 0.4031 - acc: 0.8714 - val_loss: 2.7118 - val_acc: 0.5300\n",
      "Epoch 2912/3000\n",
      "700/700 [==============================] - 0s 236us/step - loss: 0.4026 - acc: 0.8743 - val_loss: 2.7098 - val_acc: 0.5267\n",
      "Epoch 2913/3000\n",
      "700/700 [==============================] - 0s 271us/step - loss: 0.4030 - acc: 0.8700 - val_loss: 2.7037 - val_acc: 0.5233\n",
      "Epoch 2914/3000\n",
      "700/700 [==============================] - 0s 257us/step - loss: 0.4033 - acc: 0.8743 - val_loss: 2.6972 - val_acc: 0.5267\n",
      "Epoch 2915/3000\n",
      "700/700 [==============================] - 0s 249us/step - loss: 0.4025 - acc: 0.8757 - val_loss: 2.7054 - val_acc: 0.5267\n",
      "Epoch 2916/3000\n",
      "700/700 [==============================] - 0s 231us/step - loss: 0.4031 - acc: 0.8729 - val_loss: 2.7036 - val_acc: 0.5267\n",
      "Epoch 2917/3000\n",
      "700/700 [==============================] - 0s 242us/step - loss: 0.4029 - acc: 0.8757 - val_loss: 2.7175 - val_acc: 0.5267\n",
      "Epoch 2918/3000\n",
      "700/700 [==============================] - 0s 263us/step - loss: 0.4027 - acc: 0.8714 - val_loss: 2.7117 - val_acc: 0.5300\n",
      "Epoch 2919/3000\n",
      "700/700 [==============================] - 0s 236us/step - loss: 0.4027 - acc: 0.8729 - val_loss: 2.6954 - val_acc: 0.5267\n",
      "Epoch 2920/3000\n",
      "700/700 [==============================] - 0s 251us/step - loss: 0.4021 - acc: 0.8743 - val_loss: 2.7131 - val_acc: 0.5300\n",
      "Epoch 2921/3000\n",
      "700/700 [==============================] - 0s 251us/step - loss: 0.4027 - acc: 0.8743 - val_loss: 2.7063 - val_acc: 0.5300\n",
      "Epoch 2922/3000\n",
      "700/700 [==============================] - 0s 229us/step - loss: 0.4030 - acc: 0.8757 - val_loss: 2.7046 - val_acc: 0.5267\n",
      "Epoch 2923/3000\n",
      "700/700 [==============================] - 0s 226us/step - loss: 0.4031 - acc: 0.8729 - val_loss: 2.7088 - val_acc: 0.5267\n",
      "Epoch 2924/3000\n",
      "700/700 [==============================] - 0s 240us/step - loss: 0.4029 - acc: 0.8700 - val_loss: 2.7092 - val_acc: 0.5200\n",
      "Epoch 2925/3000\n",
      "700/700 [==============================] - 0s 219us/step - loss: 0.4023 - acc: 0.8771 - val_loss: 2.7065 - val_acc: 0.5300\n",
      "Epoch 2926/3000\n",
      "700/700 [==============================] - 0s 239us/step - loss: 0.4027 - acc: 0.8743 - val_loss: 2.7212 - val_acc: 0.5267\n",
      "Epoch 2927/3000\n",
      "700/700 [==============================] - 0s 236us/step - loss: 0.4023 - acc: 0.8757 - val_loss: 2.7166 - val_acc: 0.5300\n",
      "Epoch 2928/3000\n",
      "700/700 [==============================] - 0s 242us/step - loss: 0.4025 - acc: 0.8757 - val_loss: 2.7145 - val_acc: 0.5267\n",
      "Epoch 2929/3000\n",
      "700/700 [==============================] - 0s 268us/step - loss: 0.4023 - acc: 0.8757 - val_loss: 2.6940 - val_acc: 0.5300\n",
      "Epoch 2930/3000\n",
      "700/700 [==============================] - 0s 261us/step - loss: 0.4026 - acc: 0.8700 - val_loss: 2.7132 - val_acc: 0.5300\n",
      "Epoch 2931/3000\n",
      "700/700 [==============================] - 0s 288us/step - loss: 0.4021 - acc: 0.8729 - val_loss: 2.7237 - val_acc: 0.5300\n",
      "Epoch 2932/3000\n",
      "700/700 [==============================] - 0s 242us/step - loss: 0.4020 - acc: 0.8757 - val_loss: 2.7295 - val_acc: 0.5267\n",
      "Epoch 2933/3000\n",
      "700/700 [==============================] - 0s 269us/step - loss: 0.4020 - acc: 0.8743 - val_loss: 2.7099 - val_acc: 0.5267\n",
      "Epoch 2934/3000\n",
      "700/700 [==============================] - 0s 303us/step - loss: 0.4021 - acc: 0.8757 - val_loss: 2.6974 - val_acc: 0.5200\n",
      "Epoch 2935/3000\n",
      "700/700 [==============================] - 0s 288us/step - loss: 0.4027 - acc: 0.8743 - val_loss: 2.7186 - val_acc: 0.5267\n",
      "Epoch 2936/3000\n",
      "700/700 [==============================] - 0s 290us/step - loss: 0.4020 - acc: 0.8729 - val_loss: 2.7311 - val_acc: 0.5300\n",
      "Epoch 2937/3000\n",
      "700/700 [==============================] - 0s 268us/step - loss: 0.4021 - acc: 0.8714 - val_loss: 2.7225 - val_acc: 0.5267\n",
      "Epoch 2938/3000\n",
      "700/700 [==============================] - 0s 307us/step - loss: 0.4023 - acc: 0.8757 - val_loss: 2.7290 - val_acc: 0.5267\n",
      "Epoch 2939/3000\n",
      "700/700 [==============================] - 0s 303us/step - loss: 0.4015 - acc: 0.8686 - val_loss: 2.7250 - val_acc: 0.5300\n",
      "Epoch 2940/3000\n",
      "700/700 [==============================] - 0s 291us/step - loss: 0.4028 - acc: 0.8729 - val_loss: 2.7106 - val_acc: 0.5200\n",
      "Epoch 2941/3000\n",
      "700/700 [==============================] - 0s 277us/step - loss: 0.4019 - acc: 0.8729 - val_loss: 2.7065 - val_acc: 0.5300\n",
      "Epoch 2942/3000\n",
      "700/700 [==============================] - 0s 341us/step - loss: 0.4026 - acc: 0.8743 - val_loss: 2.7229 - val_acc: 0.5267\n",
      "Epoch 2943/3000\n",
      "700/700 [==============================] - 0s 291us/step - loss: 0.4012 - acc: 0.8757 - val_loss: 2.7294 - val_acc: 0.5267\n",
      "Epoch 2944/3000\n",
      "700/700 [==============================] - 0s 326us/step - loss: 0.4014 - acc: 0.8729 - val_loss: 2.7105 - val_acc: 0.5333\n",
      "Epoch 2945/3000\n",
      "700/700 [==============================] - 0s 284us/step - loss: 0.4015 - acc: 0.8714 - val_loss: 2.7269 - val_acc: 0.5300\n",
      "Epoch 2946/3000\n",
      "700/700 [==============================] - 0s 326us/step - loss: 0.4015 - acc: 0.8729 - val_loss: 2.7114 - val_acc: 0.5233\n",
      "Epoch 2947/3000\n",
      "700/700 [==============================] - 0s 312us/step - loss: 0.4024 - acc: 0.8757 - val_loss: 2.7071 - val_acc: 0.5233\n",
      "Epoch 2948/3000\n",
      "700/700 [==============================] - 0s 344us/step - loss: 0.4015 - acc: 0.8757 - val_loss: 2.7328 - val_acc: 0.5300\n",
      "Epoch 2949/3000\n",
      "700/700 [==============================] - 0s 459us/step - loss: 0.4015 - acc: 0.8743 - val_loss: 2.7360 - val_acc: 0.5300\n",
      "Epoch 2950/3000\n",
      "700/700 [==============================] - 0s 512us/step - loss: 0.4015 - acc: 0.8743 - val_loss: 2.7201 - val_acc: 0.5267\n",
      "Epoch 2951/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 402us/step - loss: 0.4014 - acc: 0.8714 - val_loss: 2.7273 - val_acc: 0.5267\n",
      "Epoch 2952/3000\n",
      "700/700 [==============================] - 0s 330us/step - loss: 0.4020 - acc: 0.8743 - val_loss: 2.7129 - val_acc: 0.5233\n",
      "Epoch 2953/3000\n",
      "700/700 [==============================] - 0s 352us/step - loss: 0.4020 - acc: 0.8757 - val_loss: 2.7213 - val_acc: 0.5267\n",
      "Epoch 2954/3000\n",
      "700/700 [==============================] - 0s 464us/step - loss: 0.4014 - acc: 0.8743 - val_loss: 2.7186 - val_acc: 0.5267\n",
      "Epoch 2955/3000\n",
      "700/700 [==============================] - 0s 345us/step - loss: 0.4014 - acc: 0.8700 - val_loss: 2.7363 - val_acc: 0.5300\n",
      "Epoch 2956/3000\n",
      "700/700 [==============================] - 0s 336us/step - loss: 0.4011 - acc: 0.8743 - val_loss: 2.7250 - val_acc: 0.5300\n",
      "Epoch 2957/3000\n",
      "700/700 [==============================] - 0s 334us/step - loss: 0.4019 - acc: 0.8757 - val_loss: 2.7318 - val_acc: 0.5333\n",
      "Epoch 2958/3000\n",
      "700/700 [==============================] - 0s 337us/step - loss: 0.4017 - acc: 0.8771 - val_loss: 2.7359 - val_acc: 0.5300\n",
      "Epoch 2959/3000\n",
      "700/700 [==============================] - 0s 330us/step - loss: 0.4015 - acc: 0.8729 - val_loss: 2.7329 - val_acc: 0.5300\n",
      "Epoch 2960/3000\n",
      "700/700 [==============================] - 0s 344us/step - loss: 0.4011 - acc: 0.8743 - val_loss: 2.7311 - val_acc: 0.5300\n",
      "Epoch 2961/3000\n",
      "700/700 [==============================] - 0s 349us/step - loss: 0.4016 - acc: 0.8743 - val_loss: 2.7224 - val_acc: 0.5233\n",
      "Epoch 2962/3000\n",
      "700/700 [==============================] - 0s 321us/step - loss: 0.4012 - acc: 0.8743 - val_loss: 2.7164 - val_acc: 0.5200\n",
      "Epoch 2963/3000\n",
      "700/700 [==============================] - 0s 382us/step - loss: 0.4010 - acc: 0.8743 - val_loss: 2.7123 - val_acc: 0.5233\n",
      "Epoch 2964/3000\n",
      "700/700 [==============================] - 0s 337us/step - loss: 0.4012 - acc: 0.8771 - val_loss: 2.7346 - val_acc: 0.5233\n",
      "Epoch 2965/3000\n",
      "700/700 [==============================] - 0s 343us/step - loss: 0.4014 - acc: 0.8714 - val_loss: 2.7131 - val_acc: 0.5200\n",
      "Epoch 2966/3000\n",
      "700/700 [==============================] - 0s 326us/step - loss: 0.4010 - acc: 0.8743 - val_loss: 2.7309 - val_acc: 0.5300\n",
      "Epoch 2967/3000\n",
      "700/700 [==============================] - 0s 316us/step - loss: 0.4007 - acc: 0.8757 - val_loss: 2.7326 - val_acc: 0.5267\n",
      "Epoch 2968/3000\n",
      "700/700 [==============================] - 0s 368us/step - loss: 0.4010 - acc: 0.8771 - val_loss: 2.7318 - val_acc: 0.5267\n",
      "Epoch 2969/3000\n",
      "700/700 [==============================] - 0s 388us/step - loss: 0.4008 - acc: 0.8729 - val_loss: 2.7240 - val_acc: 0.5267\n",
      "Epoch 2970/3000\n",
      "700/700 [==============================] - 0s 327us/step - loss: 0.4013 - acc: 0.8729 - val_loss: 2.7202 - val_acc: 0.5267\n",
      "Epoch 2971/3000\n",
      "700/700 [==============================] - 0s 391us/step - loss: 0.4009 - acc: 0.8757 - val_loss: 2.7247 - val_acc: 0.5200\n",
      "Epoch 2972/3000\n",
      "700/700 [==============================] - 0s 389us/step - loss: 0.4012 - acc: 0.8743 - val_loss: 2.7141 - val_acc: 0.5267\n",
      "Epoch 2973/3000\n",
      "700/700 [==============================] - 0s 373us/step - loss: 0.4005 - acc: 0.8743 - val_loss: 2.7277 - val_acc: 0.5267\n",
      "Epoch 2974/3000\n",
      "700/700 [==============================] - 0s 345us/step - loss: 0.4005 - acc: 0.8757 - val_loss: 2.7374 - val_acc: 0.5267\n",
      "Epoch 2975/3000\n",
      "700/700 [==============================] - 0s 366us/step - loss: 0.4006 - acc: 0.8757 - val_loss: 2.7310 - val_acc: 0.5267\n",
      "Epoch 2976/3000\n",
      "700/700 [==============================] - 0s 395us/step - loss: 0.4011 - acc: 0.8729 - val_loss: 2.7432 - val_acc: 0.5267\n",
      "Epoch 2977/3000\n",
      "700/700 [==============================] - 0s 473us/step - loss: 0.4013 - acc: 0.8714 - val_loss: 2.7369 - val_acc: 0.5200\n",
      "Epoch 2978/3000\n",
      "700/700 [==============================] - 0s 341us/step - loss: 0.4007 - acc: 0.8757 - val_loss: 2.7221 - val_acc: 0.5300\n",
      "Epoch 2979/3000\n",
      "700/700 [==============================] - 0s 329us/step - loss: 0.4008 - acc: 0.8771 - val_loss: 2.7398 - val_acc: 0.5267\n",
      "Epoch 2980/3000\n",
      "700/700 [==============================] - 0s 313us/step - loss: 0.4004 - acc: 0.8743 - val_loss: 2.7448 - val_acc: 0.5200\n",
      "Epoch 2981/3000\n",
      "700/700 [==============================] - 0s 318us/step - loss: 0.4005 - acc: 0.8757 - val_loss: 2.7318 - val_acc: 0.5233\n",
      "Epoch 2982/3000\n",
      "700/700 [==============================] - 0s 323us/step - loss: 0.3999 - acc: 0.8714 - val_loss: 2.7212 - val_acc: 0.5233\n",
      "Epoch 2983/3000\n",
      "700/700 [==============================] - 0s 383us/step - loss: 0.4007 - acc: 0.8743 - val_loss: 2.7429 - val_acc: 0.5233\n",
      "Epoch 2984/3000\n",
      "700/700 [==============================] - 0s 316us/step - loss: 0.3995 - acc: 0.8729 - val_loss: 2.7301 - val_acc: 0.5267\n",
      "Epoch 2985/3000\n",
      "700/700 [==============================] - 0s 352us/step - loss: 0.4004 - acc: 0.8757 - val_loss: 2.7421 - val_acc: 0.5267\n",
      "Epoch 2986/3000\n",
      "700/700 [==============================] - 0s 369us/step - loss: 0.4011 - acc: 0.8743 - val_loss: 2.7495 - val_acc: 0.5267\n",
      "Epoch 2987/3000\n",
      "700/700 [==============================] - 0s 336us/step - loss: 0.4000 - acc: 0.8771 - val_loss: 2.7519 - val_acc: 0.5267\n",
      "Epoch 2988/3000\n",
      "700/700 [==============================] - 0s 345us/step - loss: 0.4000 - acc: 0.8757 - val_loss: 2.7387 - val_acc: 0.5267\n",
      "Epoch 2989/3000\n",
      "700/700 [==============================] - 0s 325us/step - loss: 0.4007 - acc: 0.8700 - val_loss: 2.7374 - val_acc: 0.5300\n",
      "Epoch 2990/3000\n",
      "700/700 [==============================] - 0s 324us/step - loss: 0.4005 - acc: 0.8743 - val_loss: 2.7338 - val_acc: 0.5267\n",
      "Epoch 2991/3000\n",
      "700/700 [==============================] - 0s 335us/step - loss: 0.3999 - acc: 0.8757 - val_loss: 2.7238 - val_acc: 0.5267\n",
      "Epoch 2992/3000\n",
      "700/700 [==============================] - 0s 353us/step - loss: 0.4002 - acc: 0.8743 - val_loss: 2.7307 - val_acc: 0.5233\n",
      "Epoch 2993/3000\n",
      "700/700 [==============================] - 0s 320us/step - loss: 0.4003 - acc: 0.8757 - val_loss: 2.7402 - val_acc: 0.5300\n",
      "Epoch 2994/3000\n",
      "700/700 [==============================] - 0s 310us/step - loss: 0.3997 - acc: 0.8743 - val_loss: 2.7335 - val_acc: 0.5233\n",
      "Epoch 2995/3000\n",
      "700/700 [==============================] - 0s 327us/step - loss: 0.3995 - acc: 0.8757 - val_loss: 2.7505 - val_acc: 0.5200\n",
      "Epoch 2996/3000\n",
      "700/700 [==============================] - 0s 314us/step - loss: 0.4001 - acc: 0.8743 - val_loss: 2.7453 - val_acc: 0.5267\n",
      "Epoch 2997/3000\n",
      "700/700 [==============================] - 0s 379us/step - loss: 0.4004 - acc: 0.8729 - val_loss: 2.7374 - val_acc: 0.5267\n",
      "Epoch 2998/3000\n",
      "700/700 [==============================] - 0s 385us/step - loss: 0.3998 - acc: 0.8771 - val_loss: 2.7493 - val_acc: 0.5267\n",
      "Epoch 2999/3000\n",
      "700/700 [==============================] - 0s 326us/step - loss: 0.4000 - acc: 0.8757 - val_loss: 2.7266 - val_acc: 0.5233\n",
      "Epoch 3000/3000\n",
      "700/700 [==============================] - 0s 325us/step - loss: 0.3996 - acc: 0.8771 - val_loss: 2.7377 - val_acc: 0.5267\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, epochs=3000, batch_size=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAacAAAEKCAYAAAC2bZqoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlMo7WpyUUkrlHS1OSiml8s7/AzItUE3hx65pAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c230cae48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 조기 종료 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/3000\n",
      "700/700 [==============================] - 0s 225us/step - loss: 2.2576 - acc: 0.1643 - val_loss: 2.2272 - val_acc: 0.1633\n",
      "Epoch 2/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 2.2071 - acc: 0.1657 - val_loss: 2.1907 - val_acc: 0.1800\n",
      "Epoch 3/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 2.1729 - acc: 0.1729 - val_loss: 2.1630 - val_acc: 0.1867\n",
      "Epoch 4/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 2.1439 - acc: 0.1786 - val_loss: 2.1369 - val_acc: 0.1867\n",
      "Epoch 5/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 2.1174 - acc: 0.1900 - val_loss: 2.1139 - val_acc: 0.1867\n",
      "Epoch 6/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 2.0937 - acc: 0.2043 - val_loss: 2.0925 - val_acc: 0.2033\n",
      "Epoch 7/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 2.0716 - acc: 0.2086 - val_loss: 2.0723 - val_acc: 0.2067\n",
      "Epoch 8/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 2.0517 - acc: 0.2129 - val_loss: 2.0562 - val_acc: 0.2067\n",
      "Epoch 9/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 2.0338 - acc: 0.2157 - val_loss: 2.0408 - val_acc: 0.2033\n",
      "Epoch 10/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 2.0185 - acc: 0.2129 - val_loss: 2.0269 - val_acc: 0.2067\n",
      "Epoch 11/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 2.0038 - acc: 0.2200 - val_loss: 2.0122 - val_acc: 0.2100\n",
      "Epoch 12/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 1.9908 - acc: 0.2186 - val_loss: 2.0034 - val_acc: 0.2100\n",
      "Epoch 13/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.9785 - acc: 0.2271 - val_loss: 1.9951 - val_acc: 0.2100\n",
      "Epoch 14/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.9682 - acc: 0.2300 - val_loss: 1.9829 - val_acc: 0.2033\n",
      "Epoch 15/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.9579 - acc: 0.2214 - val_loss: 1.9749 - val_acc: 0.2067\n",
      "Epoch 16/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.9480 - acc: 0.2371 - val_loss: 1.9680 - val_acc: 0.2033\n",
      "Epoch 17/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 1.9390 - acc: 0.2343 - val_loss: 1.9609 - val_acc: 0.2033\n",
      "Epoch 18/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.9305 - acc: 0.2314 - val_loss: 1.9533 - val_acc: 0.2100\n",
      "Epoch 19/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.9228 - acc: 0.2271 - val_loss: 1.9448 - val_acc: 0.2100\n",
      "Epoch 20/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.9152 - acc: 0.2386 - val_loss: 1.9388 - val_acc: 0.2100\n",
      "Epoch 21/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 1.9082 - acc: 0.2300 - val_loss: 1.9358 - val_acc: 0.2067\n",
      "Epoch 22/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 1.9008 - acc: 0.2386 - val_loss: 1.9286 - val_acc: 0.2033\n",
      "Epoch 23/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.8950 - acc: 0.2357 - val_loss: 1.9230 - val_acc: 0.2100\n",
      "Epoch 24/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 1.8890 - acc: 0.2314 - val_loss: 1.9203 - val_acc: 0.2100\n",
      "Epoch 25/3000\n",
      "700/700 [==============================] - 0s 133us/step - loss: 1.8826 - acc: 0.2357 - val_loss: 1.9177 - val_acc: 0.2167\n",
      "Epoch 26/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 1.8765 - acc: 0.2300 - val_loss: 1.9104 - val_acc: 0.2167\n",
      "Epoch 27/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 1.8711 - acc: 0.2357 - val_loss: 1.9097 - val_acc: 0.2167\n",
      "Epoch 28/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 1.8658 - acc: 0.2400 - val_loss: 1.9089 - val_acc: 0.2000\n",
      "Epoch 29/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.8611 - acc: 0.2400 - val_loss: 1.9057 - val_acc: 0.1867\n",
      "Epoch 30/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.8561 - acc: 0.2214 - val_loss: 1.8981 - val_acc: 0.2167\n",
      "Epoch 31/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.8510 - acc: 0.2443 - val_loss: 1.8973 - val_acc: 0.1933\n",
      "Epoch 32/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 1.8460 - acc: 0.2357 - val_loss: 1.8929 - val_acc: 0.1900\n",
      "Epoch 33/3000\n",
      "700/700 [==============================] - 0s 127us/step - loss: 1.8420 - acc: 0.2257 - val_loss: 1.8873 - val_acc: 0.2100\n",
      "Epoch 34/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 1.8379 - acc: 0.2314 - val_loss: 1.8813 - val_acc: 0.1967\n",
      "Epoch 35/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 1.8331 - acc: 0.2471 - val_loss: 1.8838 - val_acc: 0.1800\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "# 1. 데이터셋 준비하기\n",
    "\n",
    "# 훈련셋과 시험셋 로딩\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "X_val = X_train[50000:]\n",
    "Y_val = Y_train[50000:]\n",
    "X_train = X_train[:50000]\n",
    "Y_train = Y_train[:50000]\n",
    "\n",
    "X_train = X_train.reshape(50000, 784).astype('float32') / 255.0\n",
    "X_val = X_val.reshape(10000, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋, 검증셋 고르기\n",
    "train_rand_idxs = np.random.choice(50000, 700)\n",
    "val_rand_idxs = np.random.choice(10000, 300)\n",
    "\n",
    "X_train = X_train[train_rand_idxs]\n",
    "Y_train = Y_train[train_rand_idxs]\n",
    "X_val = X_val[val_rand_idxs]\n",
    "Y_val = Y_val[val_rand_idxs]\n",
    "\n",
    "# 라벨링 전환\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_val = np_utils.to_categorical(Y_val)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(units=2, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 엮기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping() # 조기종료 콜백함수 정의\n",
    "hist = model.fit(X_train, Y_train, epochs=3000, batch_size=10, validation_data=(X_val, Y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAEKCAYAAAChTwphAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njUaj0bgM/x8xJ0qTpGIq7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c20d6ada0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- EarlyStopping\n",
    "  - monitor: val_loss, val_acc이 주로 사용 됨\n",
    "  - min_delta: 개선 판단 변화량\n",
    "  - patience: 개선 없이 지속되는 epoch 수\n",
    "  - verbose: 0, 1, 2\n",
    "  - mode\n",
    "    - auto\n",
    "    - min: val_loss일 경우 설정\n",
    "    - max: val_acc일 경우 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/3000\n",
      "700/700 [==============================] - 0s 227us/step - loss: 2.2576 - acc: 0.1643 - val_loss: 2.2272 - val_acc: 0.1633\n",
      "Epoch 2/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 2.2071 - acc: 0.1657 - val_loss: 2.1907 - val_acc: 0.1800\n",
      "Epoch 3/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 2.1728 - acc: 0.1729 - val_loss: 2.1629 - val_acc: 0.1867\n",
      "Epoch 4/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 2.1439 - acc: 0.1786 - val_loss: 2.1369 - val_acc: 0.1867\n",
      "Epoch 5/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 2.1174 - acc: 0.1900 - val_loss: 2.1138 - val_acc: 0.1867\n",
      "Epoch 6/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 2.0937 - acc: 0.2029 - val_loss: 2.0924 - val_acc: 0.2033\n",
      "Epoch 7/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 2.0715 - acc: 0.2086 - val_loss: 2.0723 - val_acc: 0.2067\n",
      "Epoch 8/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 2.0516 - acc: 0.2129 - val_loss: 2.0561 - val_acc: 0.2067\n",
      "Epoch 9/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 2.0337 - acc: 0.2157 - val_loss: 2.0407 - val_acc: 0.2033\n",
      "Epoch 10/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 2.0184 - acc: 0.2143 - val_loss: 2.0269 - val_acc: 0.2067\n",
      "Epoch 11/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 2.0038 - acc: 0.2200 - val_loss: 2.0122 - val_acc: 0.2100\n",
      "Epoch 12/3000\n",
      "700/700 [==============================] - 0s 108us/step - loss: 1.9908 - acc: 0.2186 - val_loss: 2.0034 - val_acc: 0.2100\n",
      "Epoch 13/3000\n",
      "700/700 [==============================] - 0s 104us/step - loss: 1.9784 - acc: 0.2271 - val_loss: 1.9952 - val_acc: 0.2100\n",
      "Epoch 14/3000\n",
      "700/700 [==============================] - 0s 106us/step - loss: 1.9682 - acc: 0.2314 - val_loss: 1.9826 - val_acc: 0.2033\n",
      "Epoch 15/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.9578 - acc: 0.2200 - val_loss: 1.9748 - val_acc: 0.2100\n",
      "Epoch 16/3000\n",
      "700/700 [==============================] - 0s 104us/step - loss: 1.9480 - acc: 0.2357 - val_loss: 1.9682 - val_acc: 0.2033\n",
      "Epoch 17/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.9390 - acc: 0.2343 - val_loss: 1.9608 - val_acc: 0.2033\n",
      "Epoch 18/3000\n",
      "700/700 [==============================] - 0s 110us/step - loss: 1.9305 - acc: 0.2314 - val_loss: 1.9534 - val_acc: 0.2100\n",
      "Epoch 19/3000\n",
      "700/700 [==============================] - 0s 109us/step - loss: 1.9228 - acc: 0.2257 - val_loss: 1.9448 - val_acc: 0.2100\n",
      "Epoch 20/3000\n",
      "700/700 [==============================] - 0s 129us/step - loss: 1.9152 - acc: 0.2386 - val_loss: 1.9389 - val_acc: 0.2100\n",
      "Epoch 21/3000\n",
      "700/700 [==============================] - 0s 137us/step - loss: 1.9082 - acc: 0.2300 - val_loss: 1.9358 - val_acc: 0.2067\n",
      "Epoch 22/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.9008 - acc: 0.2386 - val_loss: 1.9286 - val_acc: 0.2033\n",
      "Epoch 23/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.8950 - acc: 0.2343 - val_loss: 1.9231 - val_acc: 0.2100\n",
      "Epoch 24/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 1.8890 - acc: 0.2329 - val_loss: 1.9203 - val_acc: 0.2100\n",
      "Epoch 25/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.8826 - acc: 0.2357 - val_loss: 1.9177 - val_acc: 0.2167\n",
      "Epoch 26/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.8767 - acc: 0.2286 - val_loss: 1.9101 - val_acc: 0.2167\n",
      "Epoch 27/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.8711 - acc: 0.2357 - val_loss: 1.9102 - val_acc: 0.2167\n",
      "Epoch 28/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.8659 - acc: 0.2400 - val_loss: 1.9090 - val_acc: 0.2000\n",
      "Epoch 29/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 1.8611 - acc: 0.2400 - val_loss: 1.9057 - val_acc: 0.1900\n",
      "Epoch 30/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.8562 - acc: 0.2243 - val_loss: 1.8975 - val_acc: 0.2167\n",
      "Epoch 31/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 1.8510 - acc: 0.2457 - val_loss: 1.8972 - val_acc: 0.1933\n",
      "Epoch 32/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.8460 - acc: 0.2343 - val_loss: 1.8926 - val_acc: 0.1933\n",
      "Epoch 33/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.8420 - acc: 0.2214 - val_loss: 1.8874 - val_acc: 0.2100\n",
      "Epoch 34/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.8379 - acc: 0.2371 - val_loss: 1.8808 - val_acc: 0.1933\n",
      "Epoch 35/3000\n",
      "700/700 [==============================] - 0s 128us/step - loss: 1.8332 - acc: 0.2471 - val_loss: 1.8835 - val_acc: 0.1800\n",
      "Epoch 36/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 1.8296 - acc: 0.2371 - val_loss: 1.8753 - val_acc: 0.1967\n",
      "Epoch 37/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 1.8254 - acc: 0.2486 - val_loss: 1.8744 - val_acc: 0.1800\n",
      "Epoch 38/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.8217 - acc: 0.2371 - val_loss: 1.8700 - val_acc: 0.1933\n",
      "Epoch 39/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.8181 - acc: 0.2443 - val_loss: 1.8708 - val_acc: 0.1800\n",
      "Epoch 40/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 1.8140 - acc: 0.2300 - val_loss: 1.8680 - val_acc: 0.2000\n",
      "Epoch 41/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.8102 - acc: 0.2429 - val_loss: 1.8670 - val_acc: 0.1833\n",
      "Epoch 42/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.8073 - acc: 0.2500 - val_loss: 1.8634 - val_acc: 0.1833\n",
      "Epoch 43/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 1.8042 - acc: 0.2429 - val_loss: 1.8617 - val_acc: 0.1700\n",
      "Epoch 44/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 1.7999 - acc: 0.2386 - val_loss: 1.8577 - val_acc: 0.1933\n",
      "Epoch 45/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.7966 - acc: 0.2443 - val_loss: 1.8567 - val_acc: 0.1733\n",
      "Epoch 46/3000\n",
      "700/700 [==============================] - 0s 134us/step - loss: 1.7935 - acc: 0.2271 - val_loss: 1.8528 - val_acc: 0.1867\n",
      "Epoch 47/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 1.7911 - acc: 0.2643 - val_loss: 1.8546 - val_acc: 0.1800\n",
      "Epoch 48/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 1.7883 - acc: 0.2471 - val_loss: 1.8543 - val_acc: 0.1800\n",
      "Epoch 49/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.7855 - acc: 0.2486 - val_loss: 1.8504 - val_acc: 0.1900\n",
      "Epoch 50/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.7816 - acc: 0.2471 - val_loss: 1.8442 - val_acc: 0.2267\n",
      "Epoch 51/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.7791 - acc: 0.2629 - val_loss: 1.8468 - val_acc: 0.1867\n",
      "Epoch 52/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.7759 - acc: 0.2486 - val_loss: 1.8412 - val_acc: 0.1933\n",
      "Epoch 53/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.7740 - acc: 0.2514 - val_loss: 1.8489 - val_acc: 0.2000\n",
      "Epoch 54/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.7713 - acc: 0.2686 - val_loss: 1.8476 - val_acc: 0.1800\n",
      "Epoch 55/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.7684 - acc: 0.2514 - val_loss: 1.8366 - val_acc: 0.2067\n",
      "Epoch 56/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.7669 - acc: 0.2529 - val_loss: 1.8435 - val_acc: 0.2200\n",
      "Epoch 57/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.7637 - acc: 0.2729 - val_loss: 1.8385 - val_acc: 0.2167\n",
      "Epoch 58/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 1.7613 - acc: 0.2557 - val_loss: 1.8346 - val_acc: 0.2233\n",
      "Epoch 59/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.7587 - acc: 0.2671 - val_loss: 1.8331 - val_acc: 0.2233\n",
      "Epoch 60/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 128us/step - loss: 1.7550 - acc: 0.2614 - val_loss: 1.8263 - val_acc: 0.2467\n",
      "Epoch 61/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.7562 - acc: 0.2843 - val_loss: 1.8338 - val_acc: 0.2400\n",
      "Epoch 62/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 1.7529 - acc: 0.2714 - val_loss: 1.8315 - val_acc: 0.2233\n",
      "Epoch 63/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.7502 - acc: 0.2871 - val_loss: 1.8302 - val_acc: 0.2000\n",
      "Epoch 64/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 1.7480 - acc: 0.2771 - val_loss: 1.8268 - val_acc: 0.2200\n",
      "Epoch 65/3000\n",
      "700/700 [==============================] - 0s 117us/step - loss: 1.7454 - acc: 0.2829 - val_loss: 1.8298 - val_acc: 0.2033\n",
      "Epoch 66/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.7436 - acc: 0.2800 - val_loss: 1.8298 - val_acc: 0.2067\n",
      "Epoch 67/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.7416 - acc: 0.2714 - val_loss: 1.8301 - val_acc: 0.2067\n",
      "Epoch 68/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 1.7402 - acc: 0.2743 - val_loss: 1.8239 - val_acc: 0.2000\n",
      "Epoch 69/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.7373 - acc: 0.2800 - val_loss: 1.8301 - val_acc: 0.2200\n",
      "Epoch 70/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.7354 - acc: 0.2843 - val_loss: 1.8282 - val_acc: 0.2433\n",
      "Epoch 71/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.7342 - acc: 0.2786 - val_loss: 1.8222 - val_acc: 0.2167\n",
      "Epoch 72/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.7326 - acc: 0.2843 - val_loss: 1.8224 - val_acc: 0.2133\n",
      "Epoch 73/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.7295 - acc: 0.2857 - val_loss: 1.8250 - val_acc: 0.2533\n",
      "Epoch 74/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 1.7281 - acc: 0.2857 - val_loss: 1.8255 - val_acc: 0.1967\n",
      "Epoch 75/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.7265 - acc: 0.2800 - val_loss: 1.8190 - val_acc: 0.2267\n",
      "Epoch 76/3000\n",
      "700/700 [==============================] - 0s 113us/step - loss: 1.7252 - acc: 0.2871 - val_loss: 1.8197 - val_acc: 0.2233\n",
      "Epoch 77/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.7225 - acc: 0.3057 - val_loss: 1.8233 - val_acc: 0.2000\n",
      "Epoch 78/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.7209 - acc: 0.2843 - val_loss: 1.8190 - val_acc: 0.1900\n",
      "Epoch 79/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.7197 - acc: 0.2814 - val_loss: 1.8198 - val_acc: 0.2433\n",
      "Epoch 80/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7128 - acc: 0.300 - 0s 110us/step - loss: 1.7185 - acc: 0.2957 - val_loss: 1.8201 - val_acc: 0.2067\n",
      "Epoch 81/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.7164 - acc: 0.2800 - val_loss: 1.8253 - val_acc: 0.2033\n",
      "Epoch 82/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 1.7138 - acc: 0.2771 - val_loss: 1.8157 - val_acc: 0.2700\n",
      "Epoch 83/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.7130 - acc: 0.2814 - val_loss: 1.8199 - val_acc: 0.2400\n",
      "Epoch 84/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.7123 - acc: 0.2957 - val_loss: 1.8221 - val_acc: 0.2133\n",
      "Epoch 85/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.7096 - acc: 0.2971 - val_loss: 1.8161 - val_acc: 0.2300\n",
      "Epoch 86/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.7079 - acc: 0.2800 - val_loss: 1.8160 - val_acc: 0.2600\n",
      "Epoch 87/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.7048 - acc: 0.3071 - val_loss: 1.8202 - val_acc: 0.2667\n",
      "Epoch 88/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.7060 - acc: 0.3057 - val_loss: 1.8120 - val_acc: 0.2433\n",
      "Epoch 89/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.7041 - acc: 0.3029 - val_loss: 1.8178 - val_acc: 0.2200\n",
      "Epoch 90/3000\n",
      "700/700 [==============================] - 0s 114us/step - loss: 1.7025 - acc: 0.2814 - val_loss: 1.8159 - val_acc: 0.2233\n",
      "Epoch 91/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.7015 - acc: 0.3086 - val_loss: 1.8191 - val_acc: 0.2133\n",
      "Epoch 92/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 1.6984 - acc: 0.3129 - val_loss: 1.8227 - val_acc: 0.2067\n",
      "Epoch 93/3000\n",
      "700/700 [==============================] - 0s 125us/step - loss: 1.6978 - acc: 0.3071 - val_loss: 1.8179 - val_acc: 0.2733\n",
      "Epoch 94/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.6967 - acc: 0.3143 - val_loss: 1.8186 - val_acc: 0.2133\n",
      "Epoch 95/3000\n",
      "700/700 [==============================] - 0s 126us/step - loss: 1.6940 - acc: 0.3000 - val_loss: 1.8186 - val_acc: 0.2767\n",
      "Epoch 96/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 1.6948 - acc: 0.3000 - val_loss: 1.8114 - val_acc: 0.2233\n",
      "Epoch 97/3000\n",
      "700/700 [==============================] - 0s 122us/step - loss: 1.6928 - acc: 0.3014 - val_loss: 1.8236 - val_acc: 0.2367\n",
      "Epoch 98/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 1.6920 - acc: 0.3014 - val_loss: 1.8132 - val_acc: 0.2200\n",
      "Epoch 99/3000\n",
      "700/700 [==============================] - 0s 119us/step - loss: 1.6898 - acc: 0.3100 - val_loss: 1.8262 - val_acc: 0.2233\n",
      "Epoch 100/3000\n",
      "700/700 [==============================] - 0s 123us/step - loss: 1.6888 - acc: 0.3100 - val_loss: 1.8224 - val_acc: 0.2267\n",
      "Epoch 101/3000\n",
      "700/700 [==============================] - 0s 108us/step - loss: 1.6876 - acc: 0.3143 - val_loss: 1.8200 - val_acc: 0.2200\n",
      "Epoch 102/3000\n",
      "700/700 [==============================] - 0s 108us/step - loss: 1.6875 - acc: 0.3014 - val_loss: 1.8217 - val_acc: 0.2300\n",
      "Epoch 103/3000\n",
      "700/700 [==============================] - 0s 108us/step - loss: 1.6841 - acc: 0.3000 - val_loss: 1.8108 - val_acc: 0.2500\n",
      "Epoch 104/3000\n",
      "700/700 [==============================] - 0s 111us/step - loss: 1.6840 - acc: 0.3171 - val_loss: 1.8128 - val_acc: 0.2200\n",
      "Epoch 105/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.6819 - acc: 0.3043 - val_loss: 1.8070 - val_acc: 0.2067\n",
      "Epoch 106/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 1.6810 - acc: 0.3143 - val_loss: 1.8178 - val_acc: 0.2233\n",
      "Epoch 107/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.6802 - acc: 0.3086 - val_loss: 1.8250 - val_acc: 0.2367\n",
      "Epoch 108/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.6782 - acc: 0.3043 - val_loss: 1.8182 - val_acc: 0.2767\n",
      "Epoch 109/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.6780 - acc: 0.3157 - val_loss: 1.8192 - val_acc: 0.2233\n",
      "Epoch 110/3000\n",
      "700/700 [==============================] - 0s 121us/step - loss: 1.6775 - acc: 0.3171 - val_loss: 1.8166 - val_acc: 0.2133\n",
      "Epoch 111/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.6774 - acc: 0.3000 - val_loss: 1.8192 - val_acc: 0.2233\n",
      "Epoch 112/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.6744 - acc: 0.3186 - val_loss: 1.8208 - val_acc: 0.2233\n",
      "Epoch 113/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.6736 - acc: 0.3086 - val_loss: 1.8207 - val_acc: 0.2267\n",
      "Epoch 114/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 1.6723 - acc: 0.2986 - val_loss: 1.8231 - val_acc: 0.2267\n",
      "Epoch 115/3000\n",
      "700/700 [==============================] - 0s 124us/step - loss: 1.6710 - acc: 0.3071 - val_loss: 1.8145 - val_acc: 0.2333\n",
      "Epoch 116/3000\n",
      "700/700 [==============================] - 0s 118us/step - loss: 1.6703 - acc: 0.3186 - val_loss: 1.8243 - val_acc: 0.2267\n",
      "Epoch 117/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.6692 - acc: 0.3114 - val_loss: 1.8236 - val_acc: 0.2400\n",
      "Epoch 118/3000\n",
      "700/700 [==============================] - 0s 116us/step - loss: 1.6682 - acc: 0.3100 - val_loss: 1.8274 - val_acc: 0.2367\n",
      "Epoch 119/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 116us/step - loss: 1.6670 - acc: 0.3243 - val_loss: 1.8236 - val_acc: 0.2200\n",
      "Epoch 120/3000\n",
      "700/700 [==============================] - 0s 120us/step - loss: 1.6660 - acc: 0.3157 - val_loss: 1.8208 - val_acc: 0.2167\n",
      "Epoch 121/3000\n",
      "700/700 [==============================] - 0s 109us/step - loss: 1.6646 - acc: 0.3057 - val_loss: 1.8116 - val_acc: 0.2233\n",
      "Epoch 122/3000\n",
      "700/700 [==============================] - 0s 115us/step - loss: 1.6637 - acc: 0.3186 - val_loss: 1.8201 - val_acc: 0.2133\n",
      "Epoch 123/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.6633 - acc: 0.3100 - val_loss: 1.8140 - val_acc: 0.2200\n",
      "Epoch 124/3000\n",
      "700/700 [==============================] - 0s 110us/step - loss: 1.6618 - acc: 0.3129 - val_loss: 1.8212 - val_acc: 0.2167\n",
      "Epoch 125/3000\n",
      "700/700 [==============================] - 0s 112us/step - loss: 1.6607 - acc: 0.3229 - val_loss: 1.8218 - val_acc: 0.2300\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 20)\n",
    "\n",
    "# 1. 데이터셋 준비하기\n",
    "\n",
    "# 훈련셋과 시험셋 로딩\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "X_val = X_train[50000:]\n",
    "Y_val = Y_train[50000:]\n",
    "X_train = X_train[:50000]\n",
    "Y_train = Y_train[:50000]\n",
    "\n",
    "X_train = X_train.reshape(50000, 784).astype('float32') / 255.0\n",
    "X_val = X_val.reshape(10000, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋, 검증셋 고르기\n",
    "train_rand_idxs = np.random.choice(50000, 700)\n",
    "val_rand_idxs = np.random.choice(10000, 300)\n",
    "\n",
    "X_train = X_train[train_rand_idxs]\n",
    "Y_train = Y_train[train_rand_idxs]\n",
    "X_val = X_val[val_rand_idxs]\n",
    "Y_val = Y_val[val_rand_idxs]\n",
    "\n",
    "# 라벨링 전환\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_val = np_utils.to_categorical(Y_val)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(units=2, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 엮기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(patience = 20) # 조기종료 콜백함수 정의\n",
    "hist = model.fit(X_train, Y_train, epochs=3000, batch_size=10, validation_data=(X_val, Y_val), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAEKCAYAAAChTwphAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMysGE5aZmZWjP8DjvDKeCEbeJQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c20d73358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
